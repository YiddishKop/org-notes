* Machine Learning
** a python script to download lec-note automatically
:LOG:
开始设置自动执行，但貌似由于某些网页不遵循排序（找不到）会引发程序终止，
后改为录制宏，手动更改每次i的值，以此观测并及时修正网页地址
:END:

#+NAME: geturl
#+BEGIN_SRC python :results output
import urllib2
url_base = 'https://people.eecs.berkeley.edu/~jrs/189s16/lec/'
urlArr = []
for i in reversed(range(23)):
    if i < 9:
        urlArr.append(url_base + '0' + str(i+1))
    else: urlArr.append(url_base + str(i+1))

for i in range(23):
    response = urllib2.urlopen(urlArr[i])
    webcontent = response.read()
    output = '** lec ' + str(i + 1) + '\n' + webcontent
    print output
#+END_SRC

#+RESULTS: geturl#+begin_example
** lec 1 Introduction.
:Reference:
Introduction.
Classification, training, and testing.
Validation and overfitting.
Read ESL, Chapter 1
:END:

*** Class information
    CS 189 / 289A
    Machine Learning
    Jonathan Shewchuk

    Class website:  http://www.cs.berkeley.edu/~jrs/189

    TAs:  Tuomas Haarnoja, Aldo Pacchiano, Rohan Chitnis, Shaun Singh,
      Marvin Zhang, Brian Hou

    Discussion sections:
      Go to schedule.berkeley.edu and look up CS 189 (NOT 289A).
      You choose your section.  If the room is too full, please go to another one.
      Sections 102 and 103 are cancelled.
      No sections this week.

    Questions:  Please use Piazza, not email.
      [Piazza has an option for private questions, but please use public for most
       questions so other people can benefit.]
      For personal matters only, jrs@cory.eecs.berkeley.edu

    [Enrollment:  More than 100 of you were admitted around noon today.
     Most non-CS grad students won't be admitted.]
    [For those of you taking CS 162, I have good news:  CS 162 has been moved to
     exam group 3.  There is no longer a final exam conflict between 189 and 162.]

    Prerequisites
    -------------
      Math 53 (vector calculus)
      Math 54 (linear algebra)
      CS 70 (discrete math; probability)
      CS 188 (AI; probability; decision theory)

    Grading: 189
    ------------
    40%  Homework:  7 assignments.  Late policy:  5 slip days total.
    20%  Midterm:  Wednesday, March 16, in class (6:30-8 pm)
    20%  Final Exam:  Friday, May 13, 3-6 PM  (Exam group 19)

    Grading:  289A
    --------------
    40%  HW
    20%  Midterm
    20%  Final Exam
    20%  Project

    Cheating
    --------
    - Discussion of HW problems is encouraged.
    - All homeworks, including programming, must be written individually.
    - We will actively check for plagiarism.
    - Typical penalty is a large NEGATIVE score, but I reserve right to give
      an instant F for even one violation, and will always give an F for two.

    [Last time I taught CS 61B, we had to punish roughly 100 people for cheating.
    It was very painful.  Please don't put me through that again.]

*** CORE MATERIAL
    =============
    - Finding patterns in data; using them to make predictions.
    - Models and statistics helps us understand patterns.
    - Optimization algorithms "learn" the patterns.

    [The most important part of this is the data.  Data drives everything else.
    You cannot learn much if you don't have enough data.
    You cannot learn much if your data sucks.
    But it's amazing what you can do if you have lots of good data.
    Machine learning has changed a lot in the last decade because the internet
      has made truly vast quantities of data available.  For instance, with
      a little patience you can download tens of millions of photographs.
      Then you can build a 3D model of Paris.
    Some techniques that had fallen out of favor, like neural nets, have come back
      big in the last few years because researchers found that they work so much
      better when you have vast quantities of data.]

*** EXAMPLE:  CLASSIFYING DIGITS
    ============================
    [Slide:  Classifiers.]
    [Slide:  Example:  Classifying Digits.]

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 21:53:23
    [[file:Machine Learning/screenshot_2017-04-30_21-53-23.png]]

    - Collect training images:  e.g. 7's and digits that are not 7's
    - Express these images as vectors

                   |-3-|
                   | 3 |
                   | 3 |
                   | 3 |
    ---------      | 0 |                               As a general rule in machine learning,
    |3|3|3|3|      | 0 |                    /---->     you get some data, you image each data   -------+
    ---------      | 2 |                   /           as _a point in high dimension space_            |
    |0|0|2|3|      | 3 |                  /                                                            |
    ---------  ->  | 0 |   -> 16 dimension space       Now you have bunch of points in 16 dimension  <-+
    |0|0|1|3|      | 0 |                               you want to know is there some structure, like
    ---------      | 1 |                               can we buid a 'wall' that all the '7' in onside
    |3|3|3|3|      | 3 |                               all the '1' in another.
    ---------      | 3 |
                   | 3 |
                   | 3 |
                   |-3-|


    #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 21:50:39
    [[file:Machine Learning/screenshot_2017-04-30_21-50-39.png]]
    The problem of classification. We are given data points,
    each belonging to one of two classes. Then we are given additional points whose class is
    unknown, and we are asked to predict what class each new point is in. Given the credit card
    balance and annual income of a cardholder, predict whether they will default on their debt

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 21:53:44
    [[file:Machine Learning/screenshot_2017-04-30_21-53-44.png]]
    [Draw 2 colors of dots, almost but not quite linearly separable.]
    ["How do we classify a new point?"  Draw a third color point.]
    [One possibility:  look at nearest neighbor.]
    [Another possibility:  draw linear "decision boundary"; label it.]
    [Those are two different models for the data]

    [Slide:  nearest neighbor classifier and linear classifier.]
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 21:54:16
    [[file:Machine Learning/screenshot_2017-04-30_21-54-16.png]]


    [Slide:  nearest neighbor and k-nearest neighbor.
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 21:55:11
    [[file:Machine Learning/screenshot_2017-04-30_21-55-11.png]]
     The left figure has a big advantage:  it classifies all the test data
     correctly, whereas the right figure does not.  But the right figure has an
     advantage too.  Somebody please tell me what.

     The left figure is an example of what's called _"overfitting."_
     In the left figure, observe how intricate the decision boundary is that
     separates the positive examples from the negative examples.  It's a bit too
     intricate to reflect reality.  In the right figure, the decision boundary is
     smoother.  Intuitively, that smoothness is probably more likely to correspond
     to reality.]


**** Validation
----------
***** 2 sets
      - _Train_ a _classifier_:  it _learns_ to distinguish 7 from not 7
      - _Test_ the classifier on NEW images

***** 2 error
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 21:59:43
      [[file:Machine Learning/screenshot_2017-04-30_21-59-43.png]]
      2 kinds of error:
      - Training set error:
        The linear classifier doesn't classify all 7's / not 7's correctly
      - Test set error:
        Try out new images, not used during training.
        Some of them might be classified wrong.

***** 2 kinds bad sample
      _outliers_:  samples whose labels are atypical
        (e.g. solvent borrower who defaulted anywA Simple Classifieray).
      _overfitting_:  when the test error deteriorates because
        the classifier becomes too sensitive to outliers or other spurious patterns.

      [In machine learning, the goal is to create a classifier that *generalizes* to
       new examples we haven't seen yet.  Overfitting is counterproductive to that
       goal.  So we're always seeking a compromise between decision boundaries that
       make fine distinctions and decision boundaries that are downright
       superstitious.]

      [When I underline a word or phrase, that usually means it's a definition.
       If you want to do well in this course, my advice to you is to memorize the
       definitions I cover in class.]
***** hyperparameters
      Most ML algorithms have a few _hyperparameters_ that control over/underfitting,
        e.g. k in k-nearest neighbors.  We select them by

***** 2+1 data sets, validation
      _validation_:
      - Hold back a subset of training data, called the _validation_set_.
      - Train the classifier multiple times with different hyperparameter settings.
      - Choose the settings that work best on validation set.

      Now we have 3 sets:
      _training_set_ used to learn model weights
      _validation_set_ used to tune hyperparamters, choose among different models
      _test_set_ used as FINAL evaluation of model.  Keep in a vault.
        Run ONCE, at the very end.
        [It's very bad when researchers in medicine or pharmaceuticals peek into
         the test set prematurely!]

***** kaggle.com
      Kaggle.com:
        runs ML competitions, including our HWs
        we use 2 test sets:  "public" set results available during competition
                             "private" set revealed only after due date
        [If your public results are a lot better than your private results, we will
         know that you overfitted.]

*** Techniques [taught in this class, NOT a complete list]
    ----------
    Supervised learning:
    - Classification:  is this email spam?
    - Regression:  how likely does this patient have cancer?
    Unsupervised learning:
    - Density estimation:  what is probability density function of rainfall?
    - Clustering:  which DNA sequences are similar to each other?
    - Dimensionality reduction:  what are common features of faces?
                                 common differences?


    [Show slides with examples of:
    - Classification
    - Regression
    - Density estimation
    - Clustering
    - Dimensionality reduction
    ]

    #+end_example
** lec 2 Classifiers
:Reference:
Linear classifiers.
Predictor functions and decision boundaries.
The centroid method.
Perceptrons.
Read parts of the Wikipedia [[https://www.wikiwand.com/en/Perceptron][Perceptron]] page.
Read ESL, Section 4.5–4.5.1
:END:
*** Classifiers
===========
**** Sample, Feature(dimension), Class                                  :def:
     You are given a data set of n _samples_, each with d _features_.
     d features = d dimensional space.
     we're going to represent each sample as a point in _d dimensional space_.
     Some samples belong to _class_ O; some do not.

**** Representation of an example
     Example:  Samples are bank loans
               Features are income & age (d = 2)
               Some are in class "defaulted", some are not

               Goal:  Predict whether future borrowers will default,
                      based on their income & age.
**** Sample, Point, Feature_vector, Predictor, Independent_var     :def:
     Represent each _sample_ as a _point_ in d-dimensional space,
     called a _feature_vector_ (aka _predictors_, _independent_variables_).

**** an example of 2 feature classifier
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 22:09:06
     [[file:Machine Learning/screenshot_2017-04-30_22-09-06.png]]

             ^                        ^                        ^    _overfitting_
             |  X     X X             |  X     X O             |  X  O  O X
             |    X   X               |    X   X               |    X   X
             | O       X              | O       O              | X       O
     income  |O     X         income  |O     X         income  |O     X
             |    O   X               |    O X O               |    O   X
             | O    O   X             | O    O   O             | X    O   X
             | O       O              | O       O              | O       O
             +----------->            +----------->            +----------->
             age                      age                      age

             [Draw lines/curves separating O's from X's.  Use those curves to predict
             which future borrowers will default.]

**** Decision Boundary                                             :def:
     _decision boundary_:  the boundary chosen by our classifier to separate
       items in the class from those not.

     [By the way, when I underline a word or a short phrase, usually that is a
      *definition*.  If you want to do well in this course, you should *memorize*
      all the definitions I write down.]

**** Predictor/Decision/Discriminant Function       :def:
     Some (not all) classifiers work by computing a
     _predictor function_:  A function f(x) that maps a sample point x to
     a scalar such that
     f(x) >  0     if x is in class O;
     f(x) <= 0     if x not in class O.
     Aka _decision_function_ or _discriminant_function_.

     For these classifiers, the _decision boundary_ is {x ∈ R^d : f(x) = 0}
     [That is, the set of all points where the prediction function is zero.]
     Usually, this set is _a (d - 1) dimensional surface_ in R^d space.

**** Isosurface,Isovalue,Isocontour                                     :def:
     {x : f(x) = 0} is also called an _isosurface_ of f for the _isovalue_ 0.

     f has other isosurfaces for other isovalues, e.g. {x : f(x) = 1}.

     [Show plot & *isocontours* of sqrt(x^2 + y^2) - 3.
     Imagine a function in R^d, and imagine its (d - 1)-dimensional isosurface.]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 22:55:32
     [[file:Machine Learning/screenshot_2017-04-30_22-55-32.png]]
     left is sample space of 3-dimensional space
     right picture is a set of circular 'cake' which means 2-dimension isosurface with different radius
     但是更多时候，我们会有很多feature，也就是很多维度的向量，each feature 1 dimension.
     比如处理图像，一张 4pixel × 4pixel 图像就是 16-dimension feature-vector(有一个15-dimensional surface),
     each pixel may have 6 bytes space to represent different color. it is huge dimensional space.

     #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 23:07:59
     [[file:Machine Learning/screenshot_2017-04-30_23-07-59.png]]
     this is another 3-dimensional sample space,
     each isosurface would look like an ellipsoid(but,still is a 2-dimensional surface) in 3-dimensional space

**** Linear Classifier                                                  :def:
     _linear classifier_:  The decision boundary is a hyperplane.
     Usually uses a linear predictor function.  [Sometimes no predictor fn.]
**** Overfitting                                                        :def:
     _overfitting_:  When sinuous decision boundary fits sample data so well that it
     doesn't classify future items well.

*** Math Review
-----------

**** Vectors:
     [I will write vectors in matrix notation.]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-04-30 23:16:31
     [[file:Machine Learning/screenshot_2017-04-30_23-16-31.png]]
         -   -
         |x_1|
         |x_2|                        T
     x = |x_3| = [x_1 x_2 x_3 x_4 x_5]
         |x_4|
         |x_5|
         -   -
     Think of x as a point in 5-dimensional space.

**** Conventions of symbol
     Conventions (often, but not always):
     uppercase roman = matrix or random variable   X
     lowercase roman = vector                      x
     Greek = scalar                                alpha
     Other scalars:                                n = # of samples
                                                   d = # of features (per sample)
                                                     = dimension of sample points
                                                   i j k = indices
     function (often scalar)                       f( ), s( ), ...
**** Inner product: Linear fn
     _inner_product_ (aka _dot_product_):  x . y = x_1 y_1 + x_2 y_2 + ... + x_d y_d

                                     T
     _Matrix notation_ also written x  y

     Clearly,  f(x) = w . x + alpha  is a _linear_function_ in x.

**** Inner product: Euclidean norm, Normalize
     _Euclidean_norm_:  |x| = sqrt(x . x) = sqrt(x_1^2 + x_2^2 + ... + x_d^2)

       |x| is the length (aka Euclidean length) of a vector x.

                          x
       Given a vector x, --- is a _unit_ vector (length 1).
                         |x|
                                                  x
       "_Normalize_ a vector x":  replace x with ---.
                                                 |x|

**** Inner product: Compute angle
     Use dot products to compute angles:

           x
          /                       x . y     x     y
         /           cos theta = ------- = --- . ---
        / theta                  |x| |y|   |x|   |y|
       ---------->                        \___/ \___/
                                       length 1 length 1

           x   acute                ^     right       x        obtuse
          /     +ve                 |       0          \         -ve
         /                          |_                  \
        /                           | |                  \
       ---------->                  +----------->         --------------->
       cos theta > 0                cos theta = 0         cos theta < 0
**** Predictor fn: Hyperplane,
     Given a linear predictor function f(x) = w . x + alpha,
     the decision boundary is like

     #+NAME: how to get a hyperplane
     #+BEGIN_QUOTE
     Predictor fn = 0;
     f(x) = 0;
     w . x + alpha = 0;
     H = {x : w . x = - alpha};
     #+END_QUOTE

     The set H is called a _hyperplane_.    (a line in 2D, a plane in 3D.)

     [I want you to understand what a hyperplane is.  In 2D, it's a line.  In 3D,
     it's a plane.  Now take that concept and generalize it to higher dimensions.
     In d dimensions, a hyperplane is a flat, infinite thing with dimension d - 1.
     A hyperplane divides the d-dimensional space into two halves.]

**** Hyperplane: Normal vector
     #+NAME: how to get normal vector
     #+BEGIN_QUOTE
     Theorem:
     Let xy be a vector that lies on H.  Then w . (y - x) = 0.
     Proof:
     x and y lie on H.  Thus w . (x - y) = - alpha - (- alpha) = 0.

     w is called the _normal_vector_ of H,
     because (as the theorem shows) w is normal (perpendicular) to H.
     (I.e. w is normal to every pair of points in H.)
     #+END_QUOTE

     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 13:38:48
     [[file:Machine Learning/screenshot_2017-05-01_13-38-48.png]]

**** Normal vector: Signed Distance
     If w is a unit vector, then w . x + alpha is the _signed_distance_ from x to H.
     I.e. it's the distance, but positive on one side of H; negative on other side.
     这个很好理解，对于一个平面和一个点，我如何求这个点到这个平面的距离呢？
     我首先求这个平面的垂直向量，然后连线这个点与原点，这条线会穿过这个平面形成一个交点。
     这个点与交点之间的线段在这个垂直向量上的映射就是这个点到这个平面的距离。

     或者这样理解，函数 w.x = y
     当 w.x = y 在2维时， w.x=常量 是一条线;
     当 w.x = y 在3维时， w.x=常量 是一个面;
     按照'上加下减，左加右减'的原则
     w.x=0形成的平面 向上平移1 变成 w.x=1
     w.x=0形成的平面 向下平移1 变成 w.x=-1

     所以 signed distance of x' to H（因为通过[[*升维法：What if separating hyperplane doesn't pass through origin?][升维法]],H始终可以表示为 w.x=0）, 就是H经过平移
     alpha 距离得到的平面 H' 恰好包含 x'

     Moreover, the distance from H to the origin is alpha.  [How do we know that?]

     Hence alpha = 0 if and only if H passes through origin.

     [w does not have to be a unit vector for the classifier to work.
     If w not unit vector, w . x + alpha is a multiple of signed distance.
     If you want to fix that, you can _rescale_ the equation
     by computing |w| and dividing both w and alpha by 1 / |w|.]

**** Signed Distance: Weight
     The coefficients in w, plus alpha, are called _weights_ or sometimes
     _regression_coefficients_.

     [That's why I named the vector w; "w" stands for "weights".]

     The input data is _linearly_separable_ if there exists a hyperplane that
     separates all the samples in class O from all the samples NOT in class O.

     [At the beginning of this lecture, I showed you one plot that's linearly
     separable and two that are not.]

     [We will investigate some linear classifiers that only work for linearly
     separable data, then we'll move on to more sophisticated linear classifiers
     that do a decent job with non-separable data.  Obviously, if your data is not
     linearly separable, a linear classifier cannot do a *perfect* job.  But we're
     still happy if we can find a classifier that usually predicts correctly.]

*** A Stupid Classifier
**** Centroid method
     compute mean mu_C of all vectors in class O and
             mean mu_X of all vectors NOT in O.

     We use the predictor function

     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 14:48:06
       [[file:Machine Learning/screenshot_2017-05-01_14-48-06.png]]

                                                   mu_C + mu_X
       f(x) = (mu_C - mu_X) . x - (mu_C - mu_X) . (-----------)
                                                        2

              \___________/                       \___________/
              normal vector                 midpoint between mu_C, mu_X

     so that decision boundary is the hyperplane that bisects line segment
     w/endpoints mu_C, mu_X.

     [Better yet, we can adjust the right hand side to minimize the number of
      misclassified points.  Same normal vector, but different position.]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 16:23:46
     [[file:Machine Learning/screenshot_2017-05-01_16-23-46.png]]

     [In this example, there's clearly a better linear classifier that classifies
      every sample correctly.
      Note that this is hardly the worst example I could have given.
      If you're in the mood for an easy puzzle, pull out a sheet of paper and think
      of an example, with lots of samples, where the centroid method misclassifies
      every sample but one.]
***** Only better for Gaussian distribution
      [Nevertheless, there are cases where this method works well, like when all your
      positive examples come from one Gaussian distribution, and all your negative
      examples come from another.]

**** Perceptron Algorithm (Frank Rosenblatt, 1957)
     --------------------
     Slow, but correct for linearly separable samples.
     Uses a _numerical_optimization_ algorithm, namely, _gradient_descent_.
***** Some key questions about Numerical Optimization
       How many of you know what numerical optimization is?
       How many of you know what gradient descent is?
       How many of you know what Lagrange multipliers are?
       How many of you know what linear programming is?
       How many of you know what the simplex algorithm for linear programming is?
       How many of you know what convex programming is?

       We're going to learn what all these things are.  As machine learning people,
       we will be heavy users of all the optimization methods.  Unfortunately,
       I won't have time to teach you *algorithms* for all these optimization
       problems, but we'll learn a few, and I'll give you some hints how the other
       algorithms work.
***** Represent a sample: a row of features

      一行就是一个sample，一列就是一种feature。

              feature1  feature2 feature3 feature4 feature5
              +--------+--------+--------+--------+--------+
      sample1 |        |        |        |        |        |
        X_1   |        |        |        |        |        |
              +--------+--------+--------+--------+--------+
      sample2 |        |        |        |        |        |
        X_2   |        |        |        |        |        |
              +--------+--------+--------+--------+--------+
      sample3 |        |        |        |        |        |
        X_3   |        |        |        |        |        |
              +--------+--------+--------+--------+--------+
      sample4 |        |        |        |        |        |
        X_4   |        |        |        |        |        |
              +--------+--------+--------+--------+--------+
      sample5 |        |        |        |        |        |
        X_5   |        |        |        |        |        |
              +--------+--------+--------+--------+--------+


      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 17:39:37
      [[file:Machine Learning/screenshot_2017-05-01_17-39-37.png]]

      [The reason I'm using capital X here is because we typically store these
       vectors as _rows of a matrix X_.  So the subscript picks out a _row_ of X,
       representing a _specific sample_.  When I want to pick out _one feature_ from
       a sample, I'll add _a second subscript_ after the first one.]

***** Represent Label
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 17:40:23
      [[file:Machine Learning/screenshot_2017-05-01_17-40-23.png]]

      For simplicity, consider only decision boundaries that pass through the origin.
      (We'll fix this later.)                                ~~~~~~~~~~~~~~~~~~~~~~~

***** Goal:  find weights w
      That is, if sample 'i' is in class 'O', then we want a positive signed
      distantce to that point from the hyperplane; else, we want a negative one.

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 17:06:24
      [[file:Machine Learning/screenshot_2017-05-01_17-06-24.png]]

***** Get <<constraint>> for optimization: A trick make 2 inequation to 1 inequation.
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 17:41:45
      [[file:Machine Learning/screenshot_2017-05-01_17-41-45.png]]

      constraint 是把一个 machine learning problem 转化为 optimization problem 问题的前提

***** Define Loss function
      Define the _loss_function_
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 17:42:11
      [[file:Machine Learning/screenshot_2017-05-01_17-42-11.png]]

      - yi is our prediction;
      - z  is correct classification.

      注意 if y_i z < 0 那么 -y_i z > 0 的， 所以这个loss fn 是越接近于0
      越好，如果是正值且很大说明不好，分的不是很理想。

      [Here, z is the classifier's prediction, and y_i is the correct answer.]

      Idea:  if z has the same sign as y_i, the loss function is zero (happiness).
      But if z has the wrong sign, the loss function is positive.

      [For each sample, you want to get the loss function down to zero, or as close
      to zero as possible.  It's called the "loss function" because the bigger it
      is, the bigger a loser you are.]

***** Define risk function
      risk 是标准浮动的空间, 简单说 R(w) = sum(L(测，真))
      loss 是测试结果和现实情况之间的误差度量，
      如果结果都在hyperplane同一边，L(测，真) = 0;
      如果结果分在hyperplane的两边，L(测，真) = -yi*z. = -(X_i.w)*z

      Idea:
      We define a risk_function R that is _positive_ if some constraints are _violated_.

      Then we use _optimization_ to choose w that _minimizes R_.

      Define _risk_function_ (aka _objective_function_ or _cost_function_)

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 17:42:41
      [[file:Machine Learning/screenshot_2017-05-01_17-42-41.png]]
      可以看到，constraint fn 与 risk fn 在这里会师了。
      R(w) = SUM([[constraint]] violated)
      Min( R(w) ) = Min( SUM([[constraint]] violated))  <- this is our target

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-01 18:03:54
      [[file:Machine Learning/screenshot_2017-05-01_18-03-54.png]]
      Plot of risk R(w). Every point in the dark green flat spot is a minimum.
      We’ll look at this more next lecture

      If w classifies all the points X_1, ..., X_n correctly, then R(w) = 0.
      Otherwise, R(w) is positive, and we want to find a better value of w.

      --------------------------------------------------------------------------
      | Goal:  Solve this _optimization_problem_:  find w that minimizes R(w). |
      --------------------------------------------------------------------------

***** Summarize
      Through the semester what you're often going to do is take some complex
      machine learning problem and reduce it to an optimazation problem which
      you can throw a well known mathmatical tech.
      [Show plot of R.]

** lec 3 Perceptron Algorithm and Hard-SVM
:Reference:
Gradient descent,
stochastic gradient descent,
and the perceptron learning algorithm.
Feature space versus weight space.
The maximum margin classifier,
aka hard-margin support vector machine (SVM).
Read ISL, Section 9–9.1
:END:

*** Perceptron Algorithm (cont'd)
**** Recall:
     - linear predictor fn f(x) = w . x   (for simplicity, no alpha)
     - decision boundary {x : f(x) = 0}   (a hyperplane through the origin)

     - samples X_1, X_2, ..., X_n (vectors); classifications y_1, ..., y_n = +- 1
     - goal:  find weights w such that y_i X_i . w >= 0
     - goal, rewritten:  find w that minimizes R(w) = sum -y_i X_i . w     [risk fn]
                                              i in V
                         where V is the set of indicies i for which y_i X_i . w < 0.

     [Our original problem was to find a separating hyperplane in one space, which
      I'll call x-space.  But we've transformed this into a problem of finding an
      optimal point in a different space, which I'll call w-space. It's important
      to understand transformations like this, where a structure in one space
      becomes a point in another space.  In this *particular* problem, there is
      a duality between hyperplanes and points.]

**** Duality between x-space and w-space:

                x-space (primal)                w-space (dual)
        |---------------------------------------------------------------|
        |  hyperplane:  {z : w . z = 0} |  point:       w               |
        |---------------------------------------------------------------|
        |  point:       x               |  hyperplane:  {z : x . z = 0} |
        |---------------------------------------------------------------|

     If a point x lies on a hyperplane H, then
     its dual hyperplane x^* contains the dual point H^*.
     我的面是你的点，你的点是我的面。
     通过这个规律，我们要求自己这边的面，就先约束自己的点，然后映射到你的面，在通过
     约束你的面得到你的约束点，再把你的约束点映射回我的约束面。

     #+NAME: convert x-space 2 w-space
     #+BEGIN_SRC ditaa
                  x-space        .         w-space
                                 .
                **        vector ⊥ plane         /------------
                ** --------------.------------>  /           /
                 |               .              /           /
                 |               .             /           /
               \ | /             .            ------+-----/
                \|/              .                  |
                 X               .                  |
                /|\              .                  |
               / | \             .              vector ∈ plane
                 |               .                  |
                 v               .                  |
              /---------/        .                  |
             /         /         .                  |
            /         /          .                  v
           /         /  <--------.----------------- **
          /----------      plane ⊥ vector           **
                                 .
     #+END_SRC

     Duality 到底是什么？
     说白了就是 *坐标系* 变换:
     - 原来坐标系(x-space)的坐标轴是：feature_1,feature_2,feature_3...
     - 新的坐标系(w-space)的坐标轴是：w_1, w_2, w_3...
     - 而且根据constraint，w.x=0

     同一个函数 yi(w.xi)>=0 用不同的坐标系就会绘出不一样的图形。巧的是如果使用
     x-space坐标系，其中的点在 w-space 恰好是面;其中的面在 w-space 恰好是点。

     *现在已知 x-space 的点，求 x-space 的面*
     我就把问题转化为:
     *现在已知 w-space 的面，求 w-space 的点*
     而且，因为w.x=0的限制条件，所以
     *x-space的点(向量)与w-space的面垂直;反之亦然*

     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 18:44:41
     [[file:Machine Learning/screenshot_2017-05-02_18-44-41.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 10:27:51
     [[file:Machine Learning/screenshot_2017-05-02_10-27-51.png]]

     Observe that the x-space sample points are the normal vectors for the
     w-space lines. We can choose w to be anywhere in the shaded region

     原问题空间中求hyperplane{x:x.w=0}范围的问题，通过 dual 转换为上图中求w向量范围的问题(灰色区域是w向量的取值范围)。
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 11:31:36
     [[file:Machine Learning/screenshot_2017-05-02_11-31-36.png]]
     这个plot是关于 R(w) 在 w-space 的图像。可以看到，深绿色区域是 R(w) = 0 的点。
     这里假设 w 是二维向量。

     If we want to enforce inequality x . w >= 0, that means
       - x should be on the correct side of {z : z . w = 0} in primal x-space
       - w   "    "  "   "     "     "   "  {z : x . z = 0} in dual w-space

                      ^                    \    ^    /
         primal       |        X            \   |   /   dual
                      |                      \  |  /
                 X    |      ----             \ | /            [Observe that the
                      |  ----                  \|/              primal points are
            <---------+--------->     <=========+=========>     the normal vectors
                ----  |                        /|\              for the dual lines.]
            ----      |                       / | \
                      0                      /  |  \
                      |                     /   | w \
                      v                    /    v    \

     [For a sample x in class O, w and x must be on the *same* side of the dual
      hyperplane x^*.  For a sample x not in class O (X), w and x must be on
      *opposite* sides of the dual hyperplane x^*.]

     [Show plots of R.  Note how R's creases match the dual chart.]

     [In this example, we can choose w to be any point in the bottom pizza slice;
      all those points satisfy the inequalities.]

     [We have an optimization problem; we need an optimization algorithm to solve
      it.]

**** An optimization algorithm:  gradient descent on R.

     问题演变为如何最小化 R(w), 可以用求导法来求极值，也可以用这里的梯度下降法来求极值。
***** How to minimize a function: -grad(fn)
      Given a starting point w, find gradient of R with respect to w; this is the
      direction of steepest ascent.  Take a step in the opposite direction.  Recall
      [from your vector calculus class]
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 18:52:45
      [[file:Machine Learning/screenshot_2017-05-02_18-52-45.png]]

                    -         -                           -     -
                    | dR/dw_1 |                           | z_1 |
                    |         |                           |     |
        grad R(w) = | dR/dw_2 |     and    grad (z . w) = | z_2 | = z.
                    |    .    |                           |  .  |
                    |    .    |                           |  .  |
                    | dR/dw_d |                           | z_d |
                    -         -                           -     -

        grad R(w) = sum  grad -y_i X_i . w = - sum  y_i X_i
                   i in V                     i in V


        注意：反梯度法的 loss-fn 是要计算所有样本的误差之和

        这里梯度 grad R(w) 得到的是一个 R(w) 以最快速度变大的方向， 所以如果我希望 R(w) 以最快速度变小，那么
        就应该朝梯度的反方向移动点 w(或叫向量w), 梯度的反向就是负梯度：- Grad[R(w)]， 这就是梯度下降法或叫 _负梯度法_ 。

        At any point w, we walk downhill in direction of steepest descent, - grad R(w).

***** Algo: gradient descent
      #+NAME: algo: gradient descent
      #+BEGIN_QUOTE
        w = arbitrary nonzero starting point (good choice is any y_i X_i)
        while R(w) > 0
          V <- set of indicies i for which y_i X_i . w < 0   // 这里从公式 - y_i X_i . w > 0 转化得到
          w <- w + epsilon sum  y_i X_i                      // 这里更普适的写法是: w = w + 负梯度增量
                          i in V
        return w
      #+END_QUOTE
      上述算法中，每一次更新向量 w，都会让 w 朝 R(w)=0 的区域前进一点，所以要注意调整 learning rate。
      下图中每一次转折，都代表一次 w 的更新， 可以看到这个点是从 R(w)>0 浅绿色 ====> R(w)=0 深绿色移动的。
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 12:20:14
      [[file:Machine Learning/screenshot_2017-05-02_12-20-14.png]]

***** Learning rate, step size
      epsilon is the _step_size_ aka _learning_rate_, chosen empirically.
      [Best choice depends on input problem!]

      [Show plot of R again.  Show typical steps.]

***** Running time of Gradient descent algo
      Problem:  Slow!  _Each step_ takes _O(nd)_ time.
      因为 所有 sample 都要检测 y_i X_i . w <? 0 ,而 w 向量又是 d 维的。
      所以单次循环的时间复杂度是： O(nd)

      - n 表示有 n 个样本;
      - d 表示有 d 个feature; 一个feature表示一维度。

      注意：反梯度法的 loss-fn 是要计算所有样本的误差之和

**** Improvement: Perceptron algo (SGD)
     Optimization algorithm 2:  _Stochastic Gradient Descent_

     Idea:  each step, pick *one* misclassified X_i;
            do gradient descent on loss fn L(X_i . w, y_i).

     Called the _perceptron_algorithm_.  Each step takes O(d) time.
       [Not counting the time to search for a misclassified X_i.]
***** algo: perceptron
      注意：反梯度法的 loss-fn 是要计算所有样本的误差之和
       GD    反梯度法 单步循环 是 all-sample one-modification2w
      SGD 随机反梯度法 单步循环 是 one-sample one-modification2w

      因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速
      度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也
      不能在线更新模型，即在运行的过程中，不能增加新的样本。

      #+NAME: algo: SGD
      #+BEGIN_QUOTE
        while some y_i X_i . w < 0
          w <- w + epsilon y_i X_i
        return w
      #+END_QUOTE

      #+NAME: algo: gradient descent
      #+BEGIN_QUOTE
        w = arbitrary nonzero starting point (good choice is any y_i X_i)
        while R(w) > 0
          V <- set of indicies i for which y_i X_i . w < 0   // 这里从公式 - y_i X_i . w > 0 转化得到
          w <- w + epsilon sum  y_i X_i                      // 这里更普适的写法是: w = w + 负梯度增量
                          i in V
        return w
      #+END_QUOTE
***** when SGD work well: loss-fn must be convex
      SGD虽然更快更灵活能在线添加样本，但是没有GD的适用范围广。GD能处理的问题，SGD未必能处理。

      In general, you CAN NOT assume that if you _optimize a sum of fn_ that you can
      optimize each fn _separately in turn_ and find a minimum of the sum.

      The reason SGD works in this particular case is because the _loss fn_ has a
      very nice property called _convexity_. The sum of a bunch of convex fn is
      convex and that's what makes SGD work

      这里的意思是说，如果 loss-fn 是凸函数，那么就是可以使用SGD，如果不是，就不能用SGD。

      [By the way, stochastic gradient descent does not work for every problem that
       gradient descent works for.  The perceptron risk function happens to have
       special properties that allow stochastic gradient descent to always succeed.]

***** Advantage of SGD: online algo
      [One interesting aspect of the perceptron algorithm is that it's an
       _"online algorithm"_, which means that if new data points come in while the
       algorithm is already running, you can just throw them into the mix and
       keep looping.]
***** Perceptron Convergence Theorem
      Perceptron Convergence Theorem:  If data is linearly separable, perceptron
      algorithm will find a linear classifier that classifies all data correctly in
      at most O(R^2 / gamma^2) iterations, where R = max |X_i| is "radius of data"
      and gamma is the "maximum margin".  [I'll define "maximum margin" shortly.]

      We're not going to prove this, because it's obsolete.]

      Although the step size/learning rate doesn't appear in that big-O expression,
       it does have an effect on the running time, but the effect is hard to
       characterize.
***** Problem of step size(learning rate)
****** SGD is also get slower
        The algorithm gets slower if:
        - epsilon is too small
          because it has to take lots of steps to get down the hill
        - epsilon is too big for a different reason:
          it jumps right over the region with zero risk and oscillates back and
          forth for a long time.
****** hard to choose a good step size
       Although stochastic gradient descent is faster for this problem than gradient
       descent, the perceptron algorithm is still slow.

       There's no reliable way to choose a good step size epsilon.

       Fortunately, optimization algorithms have improved a lot since 1957. You
       can get rid of the step size by using any decent modern "line search"
       algorithm. Better yet, you can find a better decision boundary much more
       quickly by _quadratic programming_, which is what we'll talk about next.

**** 升维法：What if separating hyperplane doesn't pass through origin?

:Reference:
升维法，也是 TsingHua-Datamining chap4 ppt page6 中为什么会有 x_0 w_0 的原因
:END:

      如果 [超平面] 不过原点，那么就通过 [升维] 把问题转化为 [经过原点] 的问题

      if separating hyperplane doesn't pass through orighin, what we do is there
      are many tricks in optimization where you _add a dimension or two_ to
      problem, so your space gets one or two dimensions bigger in order to
      accommodate some trick.

      Add a fictitious dimension.
      #+BEGIN_SRC ditaa
        Hyperplane:  w . x + alpha = 0

                              -     -
                              | x_1 |
          [ w_1 w_2 alpha ] . | x_2 | = 0
                              |  1  |
                              -     -
                 |               |
                 |               |
                 v               v
              new 'w'         new 'x'
                    \            /
                     v          v
                    +--------------+
                    |  w' . x' = 0 |
                    +--------------+
      #+END_SRC

                              d + 1
      Now we have samples in R     , all lying on plane x      = 1.
                                                         d + 1

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 16:29:19
      [[file:Machine Learning/screenshot_2017-05-02_16-29-19.png]]

      样本原本有2个特征：x,y。所有样本点都分在一个平面上。通过增一个维度变成三维
      空间，这样所有的点仍然都处在原来的那个平面上，只不过这个平面在新空间表示为：
      X_d+1 = 1我要做的是在这个空间中使用 SGD or GD 算法得到 w', 然后去掉最后一
      位得到 w。这个w，相当于原来的 w' 平面与 X_d+1 = 1 平面的交线.

      Run perceptron algorithm in (d + 1)-dimensional space.

      [The perceptron algorithm was invented in 1957 by Frank Rosenblatt at the
       Cornell Aeronautical Laboratory.  It was originally designed not to be a
       program, but to be implemented in hardware for image recognition on a
       20 x 20 pixel image.  Rosenblatt built a Mark I Perceptron Machine
       that ran the algorithm, complete with electric motors to do weight updates.]

      [Show Mark I photo.  This is what it took to process a 20 x 20 image in 1957.]

      [Then he had a press conference where he predicted that perceptrons would be
       "the embryo of an electronic computer that [the Navy] expects will be able to
       walk, talk, see, write, reproduce itself and be conscious of its existence."
       We're still waiting on that.]

**** 梯度下降法的变形形式
     :Reference:
     这里请先参考 Tsinghua-Datamining 课程课件：chap4 Neural Networks-ppt@10
     :END:

     梯度下降法有3中变形形式，它们之间的区别为我们在计算目标函数的梯度时使用到多少数
     据。根据数据量的不同，我们在参数更新的精度和更新过程中所需要的时间两个方面做出权
     衡。

***** 2.1 批梯度下降法
      Vanilla梯度下降法，又称为批梯度下降法（batch gradient descent），在整个训练数据
      集上计算损失函数关于参数θ的梯度：

      θ=θ − η ⋅ ∇θJ(θ)

      因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速
      度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也
      不能在线更新模型，即在运行的过程中，不能增加新的样本。

      批梯度下降法的代码如下所示：
      #+BEGIN_SRC python
      for i in range(nb_epochs):
          params_grad = evaluate_gradient(loss_function, data, params)
          params = params - learning_rate * params_grad
      #+END_SRC

      然后，我们利用梯度的方向和学习率更新参数，学习率决定我们将以多大的步长更新参数。
      对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个
      局部最小值。

***** 2.2 随机梯度下降法
      :Reference:
      [[http://scikit-learn.org/stable/modules/sgd.html][参考 scikit-learn python包]]
      :END:

      相反，随机梯度下降法（stochastic gradient descent, SGD）根据每一条训练样本x(i)和标签y(i)更新参数：

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 13:58:26
      [[file:Machine Learning/screenshot_2017-05-02_13-58-26.png]]

      对于大数据集，因为批梯度下降法在每一个参数更新之前，会对相似的样本计算梯度，
      所以在计算过程中会有冗余。而SGD在每一次更新中只执行一次，从而消除了冗余。
      因而，通常SGD的运行速度更快，同时，可以用于在线学习。SGD以高方差频繁地更新，
      导致目标函数出现如图1所示的剧烈波动。

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 13:59:04
      [[file:Machine Learning/screenshot_2017-05-02_13-59-04.png]]
      图1：SGD波动（来源：Wikipedia）

      与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于SGD的波动性，一方
      面，波动性使得SGD可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收
      敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们
      缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，
      可以分别收敛到局部最小值和全局最小值。与批梯度下降的代码相比，SGD的代码片
      段仅仅是在对训练样本的遍历和利用每一条样本计算梯度的过程中增加一层循环。注
      意，如6.1节中的解释，在每一次循环中，我们打乱训练样本。

      #+BEGIN_SRC python
      for i in range(nb_epochs):
          np.random.shuffle(data)
          for example in data:
              params_grad = evaluate_gradient(loss_function, example, params)
              params = params - learning_rate * params_grad
      #+END_SRC

***** 2.3 小批量梯度下降法
      小批量梯度下降法最终结合了上述两种方法的优点，在每次更新时使用n个小批量训练样本：

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 13:59:59
      [[file:Machine Learning/screenshot_2017-05-02_13-59-59.png]]

      这种方法，a)减少参数更新的方差，这样可以得到更加稳定的收敛结果；b)可以利用
      最新的深度学习库中高度优化的矩阵优化方法，高效地求解每个小批量数据的梯度。
      通常，小批量数据的大小在50到256之间，也可以根据不同的应用有所变化。当训练
      神经网络模型时，小批量梯度下降法是典型的选择算法，当使用小批量梯度下降法时，
      也将其称为SGD。注意：在下文的改进的SGD中，为了简单，我们省略了参数
      x(i:i+n);y(i:i+n)。

      在代码中，不是在所有样本上做迭代，我们现在只是在大小为50的小批量数据上做迭代：
      #+BEGIN_SRC python
      for i in range(nb_epochs):
          np.random.shuffle(data)
          for batch in get_batches(data, batch_size=50):
              params_grad = evaluate_gradient(loss_function, batch, params)
              params = params - learning_rate * params_grad
      #+END_SRC
***** 3 挑战

      虽然Vanilla小批量梯度下降法并不能保证较好的收敛性，但是需要强调的是，这也给我们留下了如下的一些挑战：

      1. 选择一个合适的学习率可能是困难的。学习率太小会导致收敛的速度很慢，学习
         率太大会妨碍收敛，导致损失函数在最小值附近波动甚至偏离最小值。
      2. 学习率调整[17]试图在训练的过程中通过例如退火的方法调整学习率，即根据预
         定义的策略或者当相邻两代之间的下降值小于某个阈值时减小学习率。然而，策
         略和阈值需要预先设定好，因此无法适应数据集的特点[4]。
      3. 此外，对所有的参数更新使用同样的学习率。如果数据是稀疏的，同时，特征的
         频率差异很大时，我们也许不想以同样的学习率更新所有的参数，对于出现次数
         较少的特征，我们对其执行更大的学习率。
      4. 高度非凸误差函数普遍出现在神经网络中，在优化这类函数时，另一个关键的挑
         战是使函数避免陷入无数次优的局部最小值。Dauphin等人[5]指出出现这种困难
         实际上并不是来自局部最小值，而是来自鞍点，即那些在一个维度上是递增的，
         而在另一个维度上是递减的。这些鞍点通常被具有相同误差的点包围，因为在任
         意维度上的梯度都近似为0，所以SGD很难从这些鞍点中逃开。
**** 梯度下降优化算法
     这里请先参考 Tsinghua-Datamining 课程课件：chap4 Neural Networks-ppt@28

     下面，我们将列举一些算法，这些算法被深度学习社区广泛用来处理前面提到的挑战。我们
     不会讨论在实际中不适合在高维数据集中计算的算法，例如诸如牛顿法的二阶方法。

***** 4.1 动量法

      SGD很难通过陡谷，即在一个维度上的表面弯曲程度远大于其他维度的区域[19]，这
      种情况通常出现在局部最优点附近。在这种情况下，SGD摇摆地通过陡谷的斜坡，同
      时，沿着底部到局部最优点的路径上只是缓慢地前进，这个过程如图2a所示。


      这里写图片描述
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:19:29
      [[file:Machine Learning/screenshot_2017-05-02_14-19-29.png]]
      图2：来源：Genevieve B. Orr

      如图2b所示，动量法[16]是一种帮助SGD在相关方向上加速并抑制摇摆的一种方法。
      动量法将历史步长的更新向量的一个分量γ增加到当前的更新向量中（部分实现中交
      换了公式中的符号）

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:19:50
      [[file:Machine Learning/screenshot_2017-05-02_14-19-50.png]]
      vt=γvt−1+η∇θJ(θ)

      θ=θ−vt

      动量项γ通常设置为0.9或者类似的值。

      从本质上说，动量法，就像我们从山上推下一个球，球在滚下来的过程中累积动量，
      变得越来越快（直到达到终极速度，如果有空气阻力的存在，则γ<1）。同样的事情
      也发生在参数的更新过程中：对于在梯度点处具有相同的方向的维度，其动量项增大，
      对于在梯度点处改变方向的维度，其动量项减小。因此，我们可以得到更快的收敛速
      度，同时可以减少摇摆。

***** 4.2 Nesterov加速梯度下降法

      然而，球从山上滚下的时候，盲目地沿着斜率方向，往往并不能令人满意。我们希望
      有一个智能的球，这个球能够知道它将要去哪，以至于在重新遇到斜率上升时能够知
      道减速。

      Nesterov加速梯度下降法（Nesterov accelerated gradient，NAG）[13]是一种能够
      给动量项这样的预知能力的方法。我们知道，我们利用动量项γvt−1来更新参数θ。
      通过计算θ−γvt−1能够告诉我们参数未来位置的一个近似值（梯度并不是完全更新），
      这也就是告诉我们参数大致将变为多少。通过计算关于参数未来的近似位置的梯度，
      而不是关于当前的参数θ的梯度，我们可以高效的求解 ：

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:20:05
      [[file:Machine Learning/screenshot_2017-05-02_14-20-05.png]]
      vt=γvt−1+η∇θJ(θ−γvt−1)

      θ=θ−vt

      同时，我们设置动量项γ大约为0.9。动量法首先计算当前的梯度值（图3中的小的蓝
      色向量），然后在更新的累积梯度（大的蓝色向量）方向上前进一大步，Nesterov加
      速梯度下降法NAG首先在先前累积梯度（棕色的向量）方向上前进一大步，计算梯度
      值，然后做一个修正（绿色的向量）。这个具有预见性的更新防止我们前进得太快，
      同时增强了算法的响应能力，这一点在很多的任务中对于RNN的性能提升有着重要的
      意义[2]。


      这里写图片描述
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:20:58
      [[file:Machine Learning/screenshot_2017-05-02_14-20-58.png]]
      图3：Nesterov更新（来源：[[http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf][G. Hinton的课程6c]]）

      对于NAG的直观理解的另一种解释可以参见
      http://cs231n.github.io/neural-networks-3/，同时Ilya Sutskever在其博士论文
      [18]中给出更详细的综述。

      既然我们能够使得我们的更新适应误差函数的斜率以相应地加速SGD，我们同样也想
      要使得我们的更新能够适应每一个单独参数，以根据每个参数的重要性决定大的或者
      小的更新。

***** 4.3 Adagrad

      Adagrad[7]是这样的一种基于梯度的优化算法：让学习率适应参数，对于出现次数较
      少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较
      小的学习率。因此，Adagrad非常适合处理稀疏数据。Dean等人[6]发现Adagrad能够
      极大提高了SGD的鲁棒性并将其应用于Google的大规模神经网络的训练，其中包含了
      YouTube视频中的猫的识别。此外，Pennington等人[15]利用Adagrad训练Glove词向
      量，因为低频词比高频词需要更大的步长。

      前面，我们每次更新所有的参数θ时，每一个参数θi都使用的是相同的学习率η。
      由于Adagrad在t时刻对每一个参数θi使用了不同的学习率，我们首先介绍Adagrad对
      每一个参数的更新，然后我们对其向量化。为了简洁，令gt,i为在t时刻目标函数关
      于参数θi的梯度：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:21:58
      [[file:Machine Learning/screenshot_2017-05-02_14-21-58.png]]

      gt,i=∇θJ(θi)

      在t时刻，对每个参数θi的更新过程变为：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:22:12
      [[file:Machine Learning/screenshot_2017-05-02_14-22-12.png]]

      θt+1,i=θt,i−η⋅gt,i

      对于上述的更新规则，在t时刻，基于对θi计算过的历史梯度，Adagrad修正了对每一个参数θi的学习率：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:22:27
      [[file:Machine Learning/screenshot_2017-05-02_14-22-27.png]]

      θt+1,i=θt,i−ηGt,ii+ϵ−−−−−−−√⋅gt,i

      其中，Gt∈Rd×d是一个对角矩阵，对角线上的元素i,i是直到t时刻为止，所有关于
      θi的梯度的平方和（Duchi等人[7]将该矩阵作为包含所有先前梯度的外积的完整矩
      阵的替代，因为即使是对于中等数量的参数d，矩阵的均方根的计算都是不切实际
      的。），ϵ是平滑项，用于防止除数为0（通常大约设置为1e−8）。比较有意思的是，
      如果没有平方根的操作，算法的效果会变得很差。

      由于Gt的对角线上包含了关于所有参数θ的历史梯度的平方和，现在，我们可以通过
      Gt和gt之间的元素向量乘法⊙向量化上述的操作：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:23:03
      [[file:Machine Learning/screenshot_2017-05-02_14-23-03.png]]

      θt+1=θt−ηGt+ϵ−−−−−√⊙gt

      Adagrad算法的一个主要优点是无需手动调整学习率。在大多数的应用场景中，通常
      采用常数0.01。

      Adagrad的一个主要缺点是它在分母中累加梯度的平方：由于没增加一个正项，在整
      个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，
      在学习率无限小时，Adagrad算法将无法取得额外的信息。接下来的算法旨在解决这
      个不足。

***** 4.4 Adadelta

      Adadelta[21]是Adagrad的一种扩展算法，以处理Adagrad学习速率单调递减的问题。
      不是计算所有的梯度平方，Adadelta将计算计算历史梯度的窗口大小限制为一个固定
      值w。

      在Adadelta中，无需存储先前的w个平方梯度，而是将梯度的平方递归地表示成所有
      历史梯度平方的均值。在t时刻的均值E[g2]t只取决于先前的均值和当前的梯度（分
      量γ类似于动量项）：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:23:27
      [[file:Machine Learning/screenshot_2017-05-02_14-23-27.png]]

      E[g2]t=γE[g2]t−1+(1−γ)g2t

      我们将γ设置成与动量项相似的值，即0.9左右。为了简单起见，我们利用参数更新
      向量Δθt重新表示SGD的更新过程：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:23:41
      [[file:Machine Learning/screenshot_2017-05-02_14-23-41.png]]

      Δθt=−η⋅gt,i

      θt+1=θt+Δθt

      我们先前得到的Adagrad参数更新向量变为：

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:23:54
      [[file:Machine Learning/screenshot_2017-05-02_14-23-54.png]]

      Δθt=−ηGt+ϵ−−−−−√⊙gt

      现在，我们简单将对角矩阵Gt替换成历史梯度的均值E[g2]t：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:24:32
      [[file:Machine Learning/screenshot_2017-05-02_14-24-32.png]]

      Δθt=−ηE[g2]t+ϵ−−−−−−−−√gt

      由于分母仅仅是梯度的均方根（root mean squared，RMS）误差，我们可以简写为：

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:24:47
      [[file:Machine Learning/screenshot_2017-05-02_14-24-47.png]]
      Δθt=−ηRMS[g]tgt

      作者指出上述更新公式中的每个部分（与SGD，动量法或者Adagrad）并不一致，即更
      新规则中必须与参数具有相同的假设单位。为了实现这个要求，作者首次定义了另一
      个指数衰减均值，这次不是梯度平方，而是参数的平方的更新：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:25:03
      [[file:Machine Learning/screenshot_2017-05-02_14-25-03.png]]

      E[Δθ2]t=γE[Δθ2]t−1+(1−γ)Δθ2t

      因此，参数更新的均方根误差为：

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:25:16
      [[file:Machine Learning/screenshot_2017-05-02_14-25-16.png]]
      RMS[Δθ]t=E[Δθ2]t+ϵ−−−−−−−−−√

      由于RMS[Δθ]t是未知的，我们利用参数的均方根误差来近似更新。利用
      RMS[Δθ]t−1替换先前的更新规则中的学习率η，最终得到Adadelta的更新规则：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:25:36
      [[file:Machine Learning/screenshot_2017-05-02_14-25-36.png]]

      Δθt=−RMS[Δθ]t−1RMS[g]tgt

      θt+1=θt+Δθt

      使用Adadelta算法，我们甚至都无需设置默认的学习率，因为更新规则中已经移除了
      学习率。

***** 4.5 RMSprop

      RMSprop是一个未被发表的自适应学习率的算法，该算法由Geoff Hinton在其
      Coursera课堂的课程6e中提出。

      RMSprop和Adadelta在相同的时间里被独立的提出，都起源于对Adagrad的极速递减的
      学习率问题的求解。实际上，RMSprop是先前我们得到的Adadelta的第一个更新向量
      的特例：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:25:51
      [[file:Machine Learning/screenshot_2017-05-02_14-25-51.png]]

      E[g2]t=0.9E[g2]t−1+0.1g2t

      θt+1=θt−ηE[g2]t+ϵ−−−−−−−−√gt

      同样，RMSprop将学习率分解成一个平方梯度的指数衰减的平均。Hinton建议将γ设
      置为0.9，对于学习率η，一个好的固定值为0.001。

***** 4.6 Adam

      自适应矩估计（Adaptive Moment Estimation，Adam）[9]是另一种自适应学习率的
      算法，Adam对每一个参数都计算自适应的学习率。除了像Adadelta和RMSprop一样存
      储一个指数衰减的历史平方梯度的平均vt，Adam同时还保存一个历史梯度的指数衰减
      均值mt，类似于动量：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:26:49
      [[file:Machine Learning/screenshot_2017-05-02_14-26-49.png]]
      mt=β1mt−1+(1−β1)gt

      vt=β2vt−1+(1−β2)g2t

      mt和vt分别是对梯度的一阶矩（均值）和二阶矩（非确定的方差）的估计，正如该算
      法的名称。当mt和vt初始化为0向量时，Adam的作者发现它们都偏向于0，尤其是在初
      始化的步骤和当衰减率很小的时候（例如β1和β2趋向于1）。

      通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:26:02
      [[file:Machine Learning/screenshot_2017-05-02_14-26-02.png]]
      m^t=mt1−βt1

      v^t=vt1−βt2

      正如我们在Adadelta和RMSprop中看到的那样，他们利用上述的公式更新参数，由此
      生成了Adam的更新规则：
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:27:02
      [[file:Machine Learning/screenshot_2017-05-02_14-27-02.png]]

      θt+1=θt−ηv^t−−√+ϵm^t

      作者建议β1取默认值为0.9，β2为0.999，ϵ为10−8。他们从经验上表明Adam在实际
      中表现很好，同时，与其他的自适应学习算法相比，其更有优势。

***** 4.7 算法可视化

      下面两张图给出了上述优化算法的优化行为的直观理解。（还可以看看这里关于
      Karpathy对相同的图片的描述以及另一个简明关于算法讨论的概述）。

      在图4a中，我们看到不同算法在损失曲面的等高线上走的不同路线。所有的算法都是
      从同一个点出发并选择不同路径到达最优点。注意：Adagrad，Adadelta和RMSprop能
      够立即转移到正确的移动方向上并以类似的速度收敛，而动量法和NAG会导致偏离，
      想像一下球从山上滚下的画面。然而，NAG能够在偏离之后快速修正其路线，因为NAG
      通过对最优点的预见增强其响应能力。

      图4b中展示了不同算法在鞍点出的行为，鞍点即为一个点在一个维度上的斜率为正，
      而在其他维度上的斜率为负，正如我们前面提及的，鞍点对SGD的训练造成很大困难。
      这里注意，SGD，动量法和NAG在鞍点处很难打破对称性，尽管后面两个算法最终设法
      逃离了鞍点。而Adagrad，RMSprop和Adadelta能够快速想着梯度为负的方向移动，其
      中Adadelta走在最前面。

      SGD without momentum
      (a)损失去面的等高线上SGD优化
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:28:57
      [[file:Machine Learning/screenshot_2017-05-02_14-28-57.png]]


      SGD with momentum
      (b)在鞍点处的SGD优化
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 14:29:32
      [[file:Machine Learning/screenshot_2017-05-02_14-29-32.png]]
      图4：来源和全部动画：Alec Radford

      正如我们所看到的，自适应学习速率的方法，即 Adagrad、 Adadelta、 RMSprop 和
      Adam，最适合这些场景下最合适，并在这些场景下得到最好的收敛性。

***** 4.8 选择使用哪种优化算法？

      那么，我们应该选择使用哪种优化算法呢？如果输入数据是稀疏的，选择任一自适应
      学习率算法可能会得到最好的结果。选用这类算法的另一个好处是无需调整学习率，
      选用默认值就可能达到最好的结果。

      总的来说，RMSprop是Adagrad的扩展形式，用于处理在Adagrad中急速递减的学习率。
      RMSprop与Adadelta相同，所不同的是Adadelta在更新规则中使用参数的均方根进行
      更新。最后，Adam是将偏差校正和动量加入到RMSprop中。在这样的情况下，RMSprop、
      Adadelta和Adam是很相似的算法并且在相似的环境中性能都不错。Kingma等人[9]指
      出在优化后期由于梯度变得越来越稀疏，偏差校正能够帮助Adam微弱地胜过RMSprop。
      综合看来，Adam可能是最佳的选择。

      有趣的是，最近许多论文中采用不带动量的SGD和一种简单的学习率的退火策略。已
      表明，通常SGD能够找到最小值点，但是比其他优化的SGD花费更多的时间，与其他算
      法相比，SGD更加依赖鲁棒的初始化和退火策略，同时，SGD可能会陷入鞍点，而不是
      局部极小值点。因此，如果你关心的是快速收敛和训练一个深层的或者复杂的神经网
      络，你应该选择一个自适应学习率的方法。
*** MAXIMUM MARGIN CLASSIFIERS
    ==========================
    The _margin_ of a linear classifier is the distance from the decision boundary
    to the nearest sample point.  What if we make the margin as big as possible?
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 16:49:22
    [[file:Machine Learning/screenshot_2017-05-02_16-49-22.png]]

**** new constraint
     Margin 是分界面hyperplane 到最近点的垂直距离，所以有两个margin，一边一个。
     由上图   -1 <= Margin <= 1 可以用同样的方法推导出新的 constraint 公式：
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 16:49:54
     [[file:Machine Learning/screenshot_2017-05-02_16-49-54.png]]

     [Notice that the right-hand side is a 1, rather than a 0 as it was for the
      perceptron risk function.  It's not obvious, but this a much better way to
      formulate the problem, partly because it makes it impossible for the weight
      vector w to get set to zero.]
**** from constraint to margin
     If w is a unit vector, |w| = 1, the constraints imply the margin is at least 1;
       [because w . X_i + alpha is the signed distance] signed distance 的意思是说

                                                                         1
     BUT we allow w to have arbitrary length, so the margin is at least ---.
                                                                        |w|
                                                          不等式两边同除以|w|得到

                                 2
     There is a _slab_ of width --- containing no samples
                                |w|
       [with the hyperplane running along its middle].

**** from constraint to new optimization problem
     To maximize the margin, minimize |w|.  Optimization problem:

       ----------------------------------------------------------------
       | Find w and alpha that minimize |w|^2                         |
       | subject to y_i (X_i . w + alpha) >= 1    for all i in [1, n] |
       ----------------------------------------------------------------

     Called a _quadratic_program_ in d + 1 dimensions and n constraints.
     It has one unique solution!

     Why use |w|^2 instead of |w|?
     [The reason we use |w|^2 as an objective function, instead of |w|, is that
      the length function |w| is not smooth at zero, whereas |w|^2 is smooth
      everywhere.]

     The solution gives us a _maximum_margin_classifier_, aka
     a _hard_margin_ _support_vector_machine_ (SVM).

     [Technically, this isn't really a support vector machine yet; it doesn't
      fully deserve that name until we add features and kernelization, which we'll
      do in later lectures.]

     [Show 3D example in (w, alpha) weight space + 2D cross-section w1 = 1/17.
      Show optimal point on both graphs.]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 18:03:06
     [[file:Machine Learning/screenshot_2017-05-02_18-03-06.png]]

     This is an example of what the linear constraints look like in the 3D
     weight space (w1,w2,α) for an SVM with three training points.

     The SVM is looking for the point nearest the origin that lies _above the_
     _blue plane_ (representing an _inclass_ training point) but _below the red and_
     _pink planes_ (representing _out-of-class_ training points).

     In this example, that optimal point lies where the three planes intersect.
     At right we see a 2D cross-section w1 = 1/17 of the 3D space, because the
     optimal solution lies in this cross-section.

     The constraints say that the solution must lie in the leftmost pizza slice,
     while being as close to the origin as possible, so the optimal solution is where
     the three lines meet.

     用之前的图（如下）对比理解，上图仅仅展示了 w-space 空间的样子;
     而下图左边是 x-space, 右边是 w-space 。
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-02 10:27:51
     [[file:Machine Learning/screenshot_2017-05-02_10-27-51.png]]

** lec 4 Soft-SVM and Features

:Reference:
参考 Tsinghua-Datamining chap5 SVM page 15
:END:

*** SOFT-MARGIN SUPPORT VECTOR MACHINES (SVMs)
    ===================================
**** SVMs solves 2 problems:
     - Hard-margin SVMs fail if data not linearly separable.
     -   "    "     "   sensitive to outliers.
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:06:55
     [[file:Machine Learning/screenshot_2017-05-03_15-06-55.png]]
     [Show example where one outlier moves the decision boundary a lot.]

     Idea:  Allow some samples to violate the margin, with _slack_variables_.
            Modified constraint for sample i:
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:07:13
     [[file:Machine Learning/screenshot_2017-05-03_15-07-13.png]]
       y_i (X_i . w + alpha) >= 1 - xi_i

     [Observe that the only difference between these constraints and the
      hard margin constaints we saw last lecture is the extra slack term xi_i.]
     [We also impose new constraints, that the slack variables are never negative.]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:07:28
     [[file:Machine Learning/screenshot_2017-05-03_15-07-28.png]]
       xi_i >= 0

     [This inequality ensures that all samples that *don't* violate the
      margin are treated the same; they all have xi_i = 0.  Sample i has nonzero
      xi_i if and only if it violates the margin.]

     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:07:53
     [[file:Machine Learning/screenshot_2017-05-03_15-07-53.png]]
     [Show figure of margin where some samples have slack.  For each violating
      point, the slack distance is xi*_i = xi_i / |w|.]

     To prevent abuse of slack, we add a _loss_term_ to objective fn.

     Optimization problem:

     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:08:25
     [[file:Machine Learning/screenshot_2017-05-03_15-08-25.png]]
       ----------------------------------------------------------------------
       |                                                  n                 |
       | Find w, alpha, and xi_i that minimize |w|^2 + C sum xi_i           |
       |                                                 i=1                |
       | subject to y_i (X_i . w + alpha) >= 1 - xi_i   for all i in [1, n] |
       |            xi_i >= 0                           for all i in [1, n] |
       ----------------------------------------------------------------------
     ...a quadratic program in d + n + 1 dimensions and 2n constraints.
     [It's a quadratic program because its objective function is quadratic and its
      constraints are linear inequalities.]

     C > 0 is a scalar _regularization_hyperparameter_ that trades off:
               |------------------------|------------------------------------------|
               | small C                | big C                                    |
     ----------|------------------------|------------------------------------------|
      desire   | maximize margin 1/|w|  | keep most slack variables zero or small  |
     ----------|------------------------|------------------------------------------|
      danger   | underfitting           | overfitting                              |
               | (misclassifies much    | (awesome training, awful test)           |
               |  training data)        |                                          |
     ----------|------------------------|------------------------------------------|
      outliers | less sensitive         | very sensitive                           |
     ----------|------------------------|------------------------------------------|
      boundary | more "flat"            | more sinuous                             |
     -------------------------------------------------------------------------------

     [The last row only applies to nonlinear decision boundaries, which we'll
      discuss next.  Obviously, a linear decision boundary can't be sinuous.]

     Use validation to choose C.
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:09:00
     [[file:Machine Learning/screenshot_2017-05-03_15-09-00.png]]
     [Show examples of how slab varies with C.  Smallest C upper left; largest C
      lower right.]
     
     [One way to think about slack is to pretend that slack is money we can spend
      to buy permission for a sample to violate the margin.  The further a sample
      penetrates the margin, the bigger the fine you have to pay.  We want to
      make the margin as big as possible, but we also want to spend as little money
      as possible.  If the regularization parameter C is small, it means we're
      willing to spend lots of money on violations so we can get a bigger margin.
      If C is big, it means we're cheap and we want to prevent violations, even
      though we'll get a narrower margin .  If C is infinite, we're back to
      a hard-margin SVM.]


**** FEATURES
     ========
***** Q:  How to do nonlinear decision boundaries?

      A:  Make nonlinear _features_ that _lift_ samples into a higher-dimensional
          space.  High-d linear classifier -> low-d nonlinear classifier.

      [Features work with all classifiers, including perceptrons, hard-margin SVMs,
       and soft-margin SVMs.]

***** Example 1:  The _parabolic_lifting_map_
      ---------------------------------------
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:10:37
      [[file:Machine Learning/screenshot_2017-05-03_15-10-37.png]]f
      Phi(x): R^d -> R^{d+1}
               -       -
               |   x   |                                          2
      Phi(x) = |       |   <--- lifts x onto paraboloid x    = |x|
               | |x|^2 |                                 d+1
               -       -

      [We've added a new feature, |x|^2.  Even though the new feature is just a
       function of other input features, it gives our linear classifier more power.]

      Find a linear classifier in Phi-space.
      It induces a sphere classifier in x-space.
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:10:59
      [[file:Machine Learning/screenshot_2017-05-03_15-10-59.png]]
                ^    X  X
                |         X
                |  X  O     X
                | X   O O  X                [Draw paraboloid, lifted samples, and
                |X   O  OO                   plane decision boundary in 3D here.]
                |  X O O  X X
                |XX X    X
                |     X   X
                O----------->
        [Draw circle decision boundary]

        单词： ellipsoid, 椭圆体;
              sphere   , 球体
              hyperboloid, 双曲面
              paraboloid, 抛物面

      Theorem:  Phi(X_1), ..., Phi(X_n) are linearly separable iff X_1, ..., X_n are
                separable by a hypersphere.
                (Possibly a _degenerate_ hypersphere = hyperplane.)

      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 11:29:29
      [[file:Machine Learning/screenshot_2017-05-03_11-29-29.png]]

      根据这个图可以看出，朝平面是一个特殊的球体; 同样的，也可以把一条线理解为一个特殊的圆形。

      Proof:  Consider hypersphere in R^d w/center c & radius rho.
             #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:11:45
             [[file:Machine Learning/screenshot_2017-05-03_15-11-45.png]]
              Points inside:  |x - c|^2 < rho^2
                              |x|^2 - 2c . x + |c|^2 < rho^2
                                         -       -
                                         |   x   |
                              [-2c^T  1] |       | < rho^2 - |c|^2
                                         | |x|^2 |
                                         -       -
                      normal vector ^        ^ Phi(x)

              Hence points inside sphere -> same side of hyperplane in Phi-space.
              (Reverse implication works too.)

      [Although the math above doesn't expose it, hyperplane separators are a special
       case of hypersphere separators, so hypersphere classifiers can do everything
       linear classifiers can do and more.  If you take a sphere and increase its
       radius to infinity while making it pass through some point, in the limit you
       get a plane; so you can think of a plane as a degenerate sphere.  With the
       parabolic lifting map, a hyperplane in x-space corresponds to a hyperplane in
       Phi-space that is parallel to the x_{d+1}-axis.]

***** Example 2:  Axis-aligned ellipsoid/hyperboloid decision boundaries
      ------------------------------------------------------------------
      [Draw examples of axis-aligned ellipses & hyperbola.]

      In 3D, these have the formula

        #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:12:25
        [[file:Machine Learning/screenshot_2017-05-03_15-12-25.png]]
        A x_1^2 + B x_2^2 + C x_3^2 + D x_1 + E x_2 + F x_3 = -G

      [Here, the capital letters are scalars, not matrices.]
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:12:54
      [[file:Machine Learning/screenshot_2017-05-03_15-12-54.png]]
      Phi(x): R^d -> R^{2d}
      Phi(x) = [ x_1^2  ...  x_d^2  x_1  ...  x_d ]^T

      [We've turned d input features into 2d features for our linear classifier.
       If the samples are separable by an axis-aligned ellipsoid or hyperboloid, per
       the formula above, then the samples lifted to Phi-space are separable by
       a hyperplane whose normal vector is (A, B, C, D, E, F).]

***** Example 3:  Ellipsoid/hyperboloid
      ---------------------------------
      [Draw example of non-axis-aligned ellipse.]

      General formula:  [for an ellipsoid or hyperboloid]
       #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:13:24
       [[file:Machine Learning/screenshot_2017-05-03_15-13-24.png]]
        A x_1^2 + B x_2^2 + C x_3^2 + D x_1 x_2 + E x_2 x_3 + F x_3 x_1 +
        G x_1 + H x_2 + I x_3 = -J

      Phi(x): R^d -> R^{(d^2+3d)/2}

      [The isosurface defined by this equation is called a _quadric_.  In the special
       case of 2D, it's also known as a _conic_section_.]

      [You'll notice that there is a quadratic blowup in the number of features,
       because every *pair* of input features creates a new feature in Phi-space.
       If the dimension is large, these feature vectors are getting huge, and that's
       going to impose a serious computational cost.  But it might be worth it to
       find good classifiers for data that aren't linearly separable.]

***** Example 4:  Predictor fn is degree-p polynomial
      -----------------------------------------------
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:14:01
      [[file:Machine Learning/screenshot_2017-05-03_15-14-01.png]]
      E.g. a cubic in R^2:
                                                                                    T
      Phi(x) = [ x_1^3  x_1^2 x_2  x_1 x_2^2  x_2^3  x_1^2  x_1 x_2  x_2^2  x_1  x_2]
      Phi(x): R^d -> R^{O(d^p)}

      [Now we're really blowing up the number of features!  If you have, say, 100
       features per sample and you want to use degree-4 predictor functions, then
       each lifted feature vector has a length on the order of 100 million,
       and your learning algorithm will take approximately forever to run.]
      [However, later in the semester we will learn an extremely clever trick that
       allows us to work with these huge feature vectors very quickly, without ever
       computing them.  It's called "kernelization" or "the kernel trick".  So even
       though it appears now that working with degree-4 polynomials is
       computationally infeasible, it can actually be done very quickly.]
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:14:30
      [[file:Machine Learning/screenshot_2017-05-03_15-14-30.png]]
      [Show SVMs with degree 1/2/5 predictor functions.  Observe that the
       margin tends to get wider as the degree increases.]

      [Increasing the degree like this accomplishes two things.
       - First, the data might become linearly separable when you lift them to a high
         enough degree, even if the original data are not linearly separable.
       - Second, raising the degree can increase the margin, so you might get a more
         robust separator.
       However, if you raise the degree too high, you will overfit the data.]
       #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:15:19
       [[file:Machine Learning/screenshot_2017-05-03_15-15-19.png]]
      [Show training vs. test error for degree 1/2/5 predictor functions.  In this
       example, a degree-2 predictor gives the smallest test error.]

      [Sometimes you should search for the ideal degree--not too small, not too big.
       It's a balancing act between underfitting and overfitting.  The degree is an
       example of a *hyperparameter* that can be optimized by validation.]

      [If you're using both polynomial features and a soft-margin SVM, now
       you have two hyperparameters:  the degree and C.  Generally, the optimal C
       will be different for ever polynomial degree, so when you change degree, you
       have to run validation again to find the best C for that degree.]


      [So far I've talked only about polynomial features.  But features can get much
       more interesting than polynomials, and they can be tailored to fit a specific
       problem.  Let's consider a type of feature you might use if you wanted to
       implement, say, a handwriting recognition algorithm.]

***** Example 5:  Edge detection
      --------------------------
      _Edge_detector_:  algorithm for approximating grayscale/color gradients in
        image, e.g.
        - tap filter
        - Sobel filter
        - oriented Gaussian derivative filter
        [images are discrete, not continuous fields, so approximation is necessary.]

      [See "Image Derivatives" on Wikipedia.]
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 15:16:19
      [[file:Machine Learning/screenshot_2017-05-03_15-16-19.png]]
      Collect line orientations in local histograms (each having 12 orientation bins
        per region); use histograms as features (*instead* of raw pixels).

      [Show picture of image histograms.]

      Paper:  Maji & Malik, 2009.

      [If you want to, optionally, use these features in Homework 1 and try to win
       the Kaggle competition, this paper is a good online resource.]

      [When they use a linear SVM on the raw pixels, Maji & Malik get an error rate
       of 15.38% on the test set.  When they use a linear SVM on the histogram
       features, the error rate goes down to 2.64%.]

      [Many applications can be improved by designing application-specific features.
       There's no limit but your own creativity and ability to discern the structure
       hidden in your application.]
*** Summarize of lec-4
     首先，通过 *线性方程* 得到一个 hyperplane;
     其次，通过 *升维* 把所有问题都转化为 *过原点* 问题;
     其次，通过 *对偶* 把所有问题都转化为 *优化w空间* 问题;
     其次，通过 *yi技巧* 得到线性可分问题的 constraint-fn;
     其次，通过 *所有样本误差之和* 可以实现 GD;
     其次，通过 *单个样本误差 + convex-loss-fn* 可以实现 SGD;
     其次，通过 *SVMh* 解决 GD/SGD 的 biased;
     其次，通过 *hyperparameter: C of SVMs, k of k-nearst, p of polynomial* 控制 under/over fitting;
     其次，通过 *+-1 和 yi技巧* 得到 SVMh 问题的 constraint-fn;
     其次，通过 *引入ξ(called 'zai')* 得到 SVMs 解决 SVMh 的 outliers;
     其次，通过 *ξ and C* 得到 SVMs 问题的 constraint-fn;
     其次，通过 *升维* 把线性不可分问题转为高阶线性可分问题;
     其次，通过 *概率分类* 解决 same point at coordinate, with different class;
**** Definitions of Lec4
**** New Terms of Lec4
** lec 5 Hierarchical of ML and Optimization Problem
*** ML ABSTRACTIONS    [some meta comments on machine learning]
    ===============
    [When you write a large computer program, you break it down into subroutines
     and modules.  Many of you know from experience that you need to have the
     discipline to impose strong abstraction barriers between different modules, or
     your program will become so complex you can no longer manage nor maintain it.]

    [When you learn a new subject, it helps to have mental abstraction barriers,
     too, so you know when you can replace one approach with a different approach.
     I want to give you four levels of abstraction that can help you think about
     machine learning.  It's important to make mental distinctions between these
     four things, and the code you write should have modules that reflect these
     distinctions as well.]

    -----------------------------------------------------------------------------
    | APPLICATION/DATA                                                          |
    |                                                                           |
    | data labeled (classified) or not?                                         |
    | yes: labels categorical (classification) or quantitative (regression)?    |
    | no:  _similarity_ (clustering) or positioning (dimensionality reduction)? |
    |---------------------------------------------------------------------------|
    | MODEL                           [what kinds of hypotheses are permitted?] |
    |                                                                           |
    | e.g.:                                                                     |
    | - predictor fns:  linear, polynomial, logistic, neural net, ...           |
    | - nearest neighbors, decision trees(have no predictor fn)                 |
    | - features                                                                |
    | - low vs. high capacity (affects overfitting, underfitting, inference)    |
    |---------------------------------------------------------------------------|
    | OPTIMIZATION PROBLEM                                                      | a molde can be optimized by many OPTIMIZATION PROBLEM
    |                                                                           | a OPTIMIZATION PROBLEM always can be expressed with
    | - variables, objective fn, constraints                                    | 3 key-terms: var, obj, contr
    | e.g., unconstrained, convex program, least squares, PCA                   |
    |---------------------------------------------------------------------------|
    | OPTIMIZATION ALGORITHM                                                    | an OPTIMIZATION PROBLEM can be solved by many
    |                                                                           | OPTIMIZATION ALGORITHMS
    | e.g., gradient descent, simplex, SVD                                      | (简单理解为求 极大/小值)
    -----------------------------------------------------------------------------

    [In this course, we focus primarily on the middle two levels. As a data
     scientist, you might be given an _application_, and your challenge is to
     _turn it into an optimization problem_ that we know how to solve. We'll talk
     a bit about optimization algorithms, but usually you'll use an optimization
     code that's faster and more robust than what you would write.

    [The second level, the model, has a huge effect on the success of your
     learning algorithm.  Sometimes you can get a big improvement by tailoring the
     model or its features to fit the structure of your specific data.  The model
     also has a big effect on whether you overfit or underfit.  And if you want
     a model that you can interpret so you can do _inference_, the model has to
     be regular, not too complex.  Lastly, you have to pick a model that leads to
     an optimization problem that can be solved.  Some optimization problems are
     just too hard.]

    [It's important to understand that when you change something in one level of
     this diagram, you probably have to change all the levels underneath it.
     If you switch from a linear classifier to a neural net, your optimization
     problem changes, and your optimization algorithm probably changes too.]

    [Not all machine learning methods fit this four-level decomposition.
     Nevertheless, for everything you learn in this class, think about where it
     fits in this hierarchy.  If you don't distinguish which math is part of the
     model and which math is part of the optimization algorithm, this course will
     be very confusing for you.]

*** OPTIMIZATION PROBLEMS
    =====================
    [I want to familiarize you with some types of optimization problems that can be
     solved reliably and efficiently, and the names of some of the optimization
     algorithms used to solve them.  An important skill for you to develop is to be
     able to go from an application to a well-defined optimization problem.]

**** Unconstrained Optimization
     -------------
     根据， AMATH301 课程提示：
     unconstrained problem include:
     - Derivative based method
       - GD
       - fminsearch
     - Derivative-free based method
       - golden section
       - successive parabolic interpolation
     2 kinds of methods to solve it.



     Goal:  Find w that minimizes (or maximizes) a continuous fn f(w).

     f is _smooth_ if its gradient is continuous too.

     A _global_minimum_ of f is a value w such that f(w) <= f(v) for every v.
     A _local_minimum_   " "  " "   "   "   "    "    "      "
                        for every v in a tiny ball centered at w.
                        [In other words, you cannot walk downhill from w.]

                        这样的算法有 GD，SGD，simulated anealing, genetic algo
                        但是，GD/SGD 用于continuous-fn,  后两者用于 不连续函数

               ^                  ---
               |--              --   --         ----          -
               |  --           -       --     --    --       -
               |    -         -          -   -        --   --
               |     -       -            - -           ---
               |      --   --              -             x
               |        ---                ^            /
               |         ^x---------- local minima ----/
              -O---------+-------------------------------------->
               |         |
                  global minimum

     Usually, finding a local minimum is easy;
              finding the global minimum is hard. [or impossible]

     Exception:  A function is _convex_ if for every x, y in R^d,
     the line connecting (x, f(x)) to (y, f(y)) does not go below f(x).

               ^                                                  -
             f |-                                               --
               | o============================================o-
               |  --                                      ---
               |    ----                              ----
               |        -----                     ----
               |             ------          -----
               |                   ----------
              -O-o--------------------------------------------o----->
               | x                                            y

     E.g. perceptron risk fn is convex and nonsmooth.
     不是 smooth 的，因为 perceptron 的 loss-fn 大概长这样子，因为是条件函数:

     \                                 / 0        ; if yi 与 w.Xi 符号相同
      \                        L(x) = +
       \                               \ -yi w.Xi ; if yi 与 w.Xi 符号相反
        +-------------------

     [When you sum together convex functions, you always get a convex function.
      The perceptron risk function is a sum of convex loss functions.]

     A [continuous] convex function [on a closed, convex domain] has either
     - no minimum (goes to -infinity), or
     - just one local minimum, or
     - a connected set of local minima that are all global minima with equal f.
     [The perceptron risk function has the latter.]
     [In the last two cases, if you walk downhill, you eventually converge to
      a global minimum.]
     
     [However, there are many applications where you don't have a convex objective
      function, and your machine learning algorithm has to settle for finding a
      local minimum.  For example, neural nets try to optimize an objective function
      that has *lots* of local minima; they almost never find a global minimum.]

***** Algs for smooth f:
      - Steepest descent:
        = blind [with learning rate]              repeat:  w <- w - epsilon grad f(w)
        = with line search:
          x Secant method
          x Newton-Raphson (may need Hessian matrix of f)
        = stochastic (blind)   [trains on one sample per iteration, or a small batch]
      - Nonlinear conjugate gradient              [uses the same line search methods]
        - 不是朝着grad反方向移动，通常认为这个方向未必是最好的方向。
      - Newton's method (needs Hessian matrix)

***** Algs for nonsmooth f:
      - Steepest descent
        = blind
        = with direct line search (e.g. golden section search)
          direct line search 也很适用于那些你只有数据，但是没有函数的情况。这时候用direct line
          search 也同样可以得到一个minimum。
          direct line search 并不是用 grad，因为 grad 在 nonsmooth-fn 中的结果并不十分可信。

      These algs find a local minimum.

      计算 global-minimum 耗费时间太多了。当然，如果函数是凸函数，那么
      local-minimum is global-minimum

      _line_search_:  finds a local minimum along the search direction by solving
                      an optimization problem in 1D.
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 17:20:09
      [[file:Machine Learning/screenshot_2017-05-03_17-20-09.png]]
      [...instead of using a blind step size like the perceptron algorithm does.
       Solving a 1D problem is much easier than solving a higher-dimensional one.]

      Why line search fail, when fn is non-continuous?

      Why GD can not used in saddle?
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 17:32:28
      [[file:Machine Learning/screenshot_2017-05-03_17-32-28.png]]
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 17:32:50
      [[file:Machine Learning/screenshot_2017-05-03_17-32-50.png]]
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 17:35:42
      [[file:Machine Learning/screenshot_2017-05-03_17-35-42.png]]

      [Neural nets are _unconstrained optimization problems_ with many, many local
       minima.  They sometimes benefit from the more sophisticated optimization
       algorithms, but when the input data set is very large, researchers often
       favor the dumb, blind, stochastic versions of gradient descent.]

      [If you're optimizing over a d-dimensional space, the Hessian matrix is
       a d-by-d matrix and it's usually dense, so most methods that use the Hessian
       are computationally infeasible when d is large.]

**** Constrained Optimization (smooth equality constraints)
     ------------------------
     Goal:  Find w that minimizes (maximizes) f(w)
            subject to g(w) = 0              [<- observe that this is an isosurface]

            where g is a smooth fn
            (may be vector, encoding multiple constraints)

     Alg:  Use _Lagrange_multipliers_.

***** Linear Program
      --------------
      Linear objective fn + linear *inequality* constraints.

      Goal:  Find w that maximizes (or minimizes) c . w
             subject to A w <= b

             where A is n-by-d matrix, b in R^n, expressing n _linear_constraints_:
             A_i w <= b_i,        i in [1, n]

                         |    /        \            /
           ^           --+---O----------\----------/-------  <-- active constraint
            \            |  /.optimum....\        /
             \ c         | /..............\      /
              \          |/................\    /
               \         /.....feasible.....\  /
                \       /|......region.......\/
                       / |.................../\
                      /  |................../  \
                    -/---+-----------------/----\----
                    /    |                /      \
                   /
                  /  <-- active constraint


      The set of points that satisfy all constraints is a convex _polytope_ called
      the _feasible_region_ F.  The _optimum_ is the point in F that is furthest in
      the direction c.  [What does convex mean?]  A point set P is _convex_ if
      for every p, q in P, the line segment with endpoints p, q lies entirely in P.

      [A polytope is just a polyhedron, generalized to higher dimensions.]

      We can always find a global optimum that is a vertex of the polytope.

      The optimum achieves equality for some constraints (but not most), called the
      _active_constraints_ of the optimum.

      [In the figure above, there are two active constraints.  In an SVM, active
       constraints correspond to the samples that touch or violate the slab.]

      [Sometimes, there is more than one optimal point.  For example, in the figure
       above, if c pointed straight up, every point on the top horizontal edge would
       be optimal.  The set of optimal points is always convex.]

      Example:  EVERY _feasible_point_ (w, alpha) gives a linear classifier:

        ----------------------------------------------------------------
        | Find w, alpha that maximizes 0                               |
        | subject to y_i (w . X_i + alpha) >= 1    for all i in [1, n] |
        ----------------------------------------------------------------

      IMPORTANT:
      The data are linearly separable iff the feasible region is not the empty set.
              ^
              |----- Also true for maximum margin classifier (quadratic program)

      Algs for linear programming:
      - Simplex (George Dantzig, 1947)
        [Indisputibly one of the most important algorithms of the 20th century.]
        [Walks along edges of polytope from vertex to vertex until it finds optimum.]
      - Interior point methods

      [Linear programming is very different from unconstrained optimization; it has
       a much more combinatorial flavor.  If you knew which constraints would be the
       active constraints once you found the solution, it would be easy; the hard
       part is figuring out which constraints should be the active ones.  There are
       exponentially many possibilities, so you can't afford to try them all.
       So linear programming algorithms tend to have a very discrete, computer
       science feeling to them, like graph algorithms, whereas unconstrained
       optimization algorithms tend to have a continuous, numerical optimization
       feeling.]

      [Linear programs crop up everywhere in engineering and science, but they're
       usually in disguise.  An extremely useful talent you should develop is to
       recognize when a problem is a linear program.]

      [A linear program solver can find a linear classifier, but it can't find the
       maximum margin classifier.  We need something more powerful.]

***** Quadratic program
      -----------------
      Quadratic, convex objective fn + linear inequality constraints.

      Goal:  Find w that minimizes f(w) = w^T Q w + c^T w
             subject to A w <= b

             where Q is a symmetric, positive definite matrix.

      A matrix is _positive_definite_ if w^T Q w > 0 for all w != 0.

      Only one local minimum!

      [If Q is indefinite, so f is not convex, then the minimum is not unique and
       quadratic programming is NP-hard.]

      Example:  Find maximum margin classifier.
      #+DOWNLOADED: /tmp/screenshot.png @ 2017-05-03 19:06:46
      [[file:Machine Learning/screenshot_2017-05-03_19-06-46.png]]
      [Show circular contour; draw polygon on top; show constrained minimum.
       In an SVM, we are looking for the point in this polygon that's closest to
       the origin.  Example with one active constraint; example with two.]

      Algs for quadratic programming:
      - Simplex-like
      - Sequential minimal optimization (SMO, used in LIBSVM)
      - Coordinate descent (used in LIBLINEAR)

      [One clever idea used in SMO is that they do a line search that uses the
       Hessian, but it's cheap to compute because they don't walk in the direction of
       steepest descent; instead they walk along just one or a few coordinate axes at
       a time.]

***** Convex Program (EE 127/227A/227B)
      --------------
      Convex objective fn + convex inequality constraints.

      [What I've given you here is, roughly, a sliding scale of optimization problems
       of increasing complexity, difficulty, and computation time.  But even convex
       programs are relatively easy to solve.  When you're trying to address the
       needs of real-world applications, it's not uncommon to devise an optimization
       problem with crazy inequalities and an objective function that's nowhere near
       convex.  These are sometimes very, very hard to solve.]
***** [extend] Genetic algorithm
** lec 6 Decision theory
*** DECISION THEORY
    ===============
    [Today I'm going to talk about a style of classifier very different from SVMs.
     The classifiers we'll cover in the next few weeks are based on probability,
     because sometimes a point in feature space doesn't have just one class.]

    [Suppose one borrower with income $30,000 and debt $15,000 defaults.
         another    "      "     "       "     "   "      "    doesn't default.
     So in your feature space, you have two samples at the _same point_ with
     _different classes_.  Obviously, in that case, you can't draw a decision
     boundary that classifies all points with 100% accuracy.]

    Multiple samples with different classes could lie at same point:
    we want a *probabilistic classifier*.

    Suppose 10% of population has cancer, 90% doesn't.              [caps here
    Probability distributions for calorie intake, P(X | Y):          mean random
                                                                     variables,
       calories  (X)       |  < 1,200  | 1,200--1,600 |  > 1,600     not matrices.]
       --------------------+-----------+--------------+----------
       cancer    (Y =  1)  |    20%    |      50%     |    30%
       no cancer (Y = -1)  |     1%    |      10%     |    89%

    [I made these numbers up.  Please don't take them as medical advice.]

    Recall:  P(X) = P(X | Y = 1) P(Y = 1) + P(X | Y = -1) P(Y = -1)
             P(1,200 <= X <= 1,600) = 0.5 * 0.1 + 0.1 * 0.9 = 0.14

    You meet guy eating x = 1,400 calories/day.  Guess whether he has cancer?

    [If you're in a hurry, you might see that 50% of people with cancer eat
     1,400 calories, but only 10% of people with no cancer do, and conclude that
     someone who eats 1,400 calories probably has cancer.  But that would be wrong,
     because that reasoning fails to take the prior probabilities into account.]

**** Bayes' Theorem:

     | posterior probability      | prior prob.   | for 1,200 <= X <= 1,600
     |                            v               v
     v                P(X | Y = 1) P(Y = 1)    0.05        \
     P(Y = 1 | X)  =  ---------------------  = ----         |
                               P(X)            0.14         |
                                                             > sum is 1
                     P(X | Y = -1) P(Y = -1)   0.09         |
     P(Y = -1 | X) = ----------------------- = ----         |
                               P(X)            0.14        /

     P(cancer | X = 1,400 cals) = 5/14 ~ 36%.

     [So we probably shouldn't diagnose cancer.]

     [However, we've been assuming that we want to maximize the chance of a correct
      prediction.  But that's not always the right assumption.  If you're developing
      a cheap screening test for cancer, you'd rather have more false positives and
      fewer false negatives.  A false negative might mean somebody misses an early
      diagnosis and dies of a cancer that could have been treated if caught early.
      A false positive just means that you spend more money on more accurate tests.]

     
     A _loss_function_ L(z, y) specifies badness if true class is y,
     classifier predicts z.

                     /  1      if z = 1, y = -1         false positive is bad
     E.g., L(z, y) = |  5      if z = -1, y = 1         false negative is BAAAAAD
                     \  0      if z = y

     A 36% probability of loss 5 is worse than a 64% prob. of loss 1,
     so we recommend further cancer screening.

     Defs:  loss fn above is _asymmetrical_.
            The _0-1_loss_function_ is 1 for incorrect predictions,  [symmetrical]
                                       0 for correct.

     [Another example where you want a very asymmetrical loss function is for spam
      detection.  Putting a good email in the spam folder is much worse than putting
      spam in your inbox.]

     Let r : R^d -> +-1 be a _decision_rule_, aka _classifier_:  a fn that maps
     a feature vector x to 1 ("in class") or -1 ("not in class").

     The _risk_ for r is the expected loss over all values of x, y:

       R(r) = E[L(r(X), Y)]
            = sum (L(r(x), 1) P(Y = 1 | X = x) + L(r(x), -1) P(Y = -1 | X = x)) P(x)
               x
            = P(Y = 1) sum L(r(x), 1) P(X = x | Y = 1) +
                        x
              P(Y = -1) sum L(r(x), -1) P(X = x | Y = -1)
                         x

     The _Bayes_decision_rule_ aka _Bayes_classifier_ is the r that minimizes R(r);
     call it r*.  Assuming L(z, y) = 0 for z = y:

               /   1      if L(-1, 1) P(Y = 1 | X = x) > L(1, -1) P(Y = -1 | X = x),
       r*(x) = |
               \  -1      otherwise

     In cancer example, r* = 1 for intakes <= 1,600; r* = -1 for intakes > 1,600.

     The _Bayes_risk_, aka _optimal_risk_, is the risk of the Bayes classifier.
     [In our cancer example, the last expression for risk gives:]

       R(r*) = 0.1 (5 * 0.3) + 0.9 (1 * 0.01 + 1 * 0.1) = 0.249

     [It is interesting that, if we really know all these probabilities, we really
      can construct an ideal probabilistic classifier.  But in real applications,
      we rarely know these probabilities; the best we can do is use statistical
      methods to estimate them.]

     
     Suppose X has a continuous probability density fn (PDF).

     Review:  [Go back to your CS 70 or stats notes if you don't remember this.]

                                                                        / x_2
       ^ P(x)    ====        prob. that random variable X in [x , x ] = |    P(x) dx
       |       ==  |.==      [shaded area]                     1   2    / x_1
       |      =    |..|=
       |     =     |..| ==                                              / inf
       |   ==      |..|   =====            area under whole curve = 1 = |    P(x) dx
       | ==        |..|        =========                                / -inf
       +-----------+--+----------------------> x
                   x  x                                            / inf
                    1  2      _expected_ value of f(X):  E[f(X)] = |    f(x) P(x) dx
                                                                   / -inf
                          / inf
       _mean_ mu = E[X] = |    x P(x) dx
                          / -inf

                       2             2       2      2
       _variance_ sigma  = E[(X - mu) ] = E[X ] - mu

     [Perhaps our cancer statistics look like this:]
       ^             ====
       |           ==    == P(X | Y = 1)                [area under each curve is 1]
       |          =        =
       |         =          =
       |        =            =
       |        =            =
       |       =             +=++++++++++++++
       |       =         ++++  =             ++++  P(X | Y = -1)
       |      =        ++       =                ++
       |     =       ++          ==                ++++
       |   ==    ++++              ======              ++++++
       | ==++++++                        ===============     +++++
       +-----------------------------------------------------------> x

     [Let's go back to the 0-1 loss function for a moment.  In other words, you want
      a classifier that maximizes the chance of a correct prediction.  The wrong
      answer would be to look where these two curves cross and make that be the
      decision boundary.  As before, it's wrong because it doesn't take into account
      the prior probabilities.]

     Suppose P(Y = 1) = 1/3, P(Y = -1) = 2/3, 0-1 loss:

       ^  P(X | Y = 1) P(Y = 1)
       |             ====        ++++++++
       |           ==    ==  ++++        ++++
       |          =        =+                ++  P(X | Y = -1) P(Y = -1)
       |         =       ++|=                  ++
       |        =       +..|.=                   +
       |        =      +...|.=                    +
       |       =      +....|..=                    ++
       |       =     +.....|...=                     ++
       |      =    ++......|....=                      ++
       |     =   ++........|.....==                      ++++
       |   ==++++..........|.......======                    +++++
       | ==++..............|.............===============
       +-------------------+----------------------------==========> x
                           |
            Bayes optimal decision boundary
     
     [To maximize the chance you'll predict correctly whether somebody has cancer,
      the Bayes decision rule looks up x on this chart and picks the curve with the
      highest probability.  In this example, that means you pick cancer when x is
      left of the optimal decision boundary, and no cancer when x is to the right.]

     Define _risk_ as before, replacing summations with integrals.

       R(r) = E[L(r(X), Y)]
                       /
            = P(Y = 1) | L(r(x), 1) P(X = x | Y = 1) dx +
                       /
                        /
              P(Y = -1) | L(r(x), -1) P(X = x | Y = -1) dx
                        /

     If L is 0-1 loss,          [the risk has a particularly nice interpretation]
       R(r) = P(r(x) is wrong)  [which makes sense, because R is the expected loss.]

     For Bayes decision rule, Bayes Risk is the area under minimum of functions
     above (shaded).  Assuming L(z, y) = 0 for z = y:

               /
       R(r*) = |  min  L(-y, y) P(X = x | Y = y) P(Y = y) dx
               / y=+-1

     [If you want to use an asymmetrical loss function, just scale the vertical
      reach of each curve accordingly in the figure above.]

     _Bayes_optimal_decision_boundary_:  {x : P(Y = 1 | X = x) = 0.5}
                                              \______________/  \___/
                                                predictor fn   isovalue

     [Show figure of 2D Gaussians with decision boundary.]

     [Obviously, the accuracy of the probabilities is most important near the
      decision boundary.  Far away from the decision boundary, a bit of error in
      the probabilities probably wouldn't change the classification.]

     [You can also have multi-class classifiers, where each sample is in one class
      among many.  The Bayesian approach is a particularly convenient way to
      generate multi-class classifiers, because you can simply choose whichever
      class has the greatest posterior probability.  Then the decision boundary lies
      wherever two or more classes are tied for the highest probability.]

     
*** 3 WAYS TO BUILD CLASSIFIERS
    ===========================
    (1)  Generative models (e.g. LDA)

         - Assume samples come from probability distributions,
           different for each class.
         - Guess form of distributions
         - For each class C, fit distribution parameters to class C samples,
           giving P(X | Y = C)
         - For each C, estimate P(Y = C)
         - Bayes' Theorem gives P(Y | X)
         - If 0-1 loss, pick class C that maximizes P(Y = C | X = x)    [posterior]
                            equivalently, maximizes P(X = x | Y = C) P(Y = C)

    (2)  Discriminative models (e.g. logistic regression)

         - Model P(Y | X) directly

    (3)  Find decision boundary (e.g. SVM)

         - Model r(x) directly (no posterior)

    Advantage of (1 & 2):  P(Y | X) tells you probability your guess is wrong
                           [This is something SVMs don't do.]
    Advantage of (1):  you can diagnose outliers:  P(X) is very small
    Disadvantages of (1):  often hard to estimate distributions accurately;
                           real distributions rarely match standard ones.

    [What I've written here doesn't actually define the phrases "generative model"
     or "discriminative model".  The proper definitions accord with the way
     statisticians think about models.  A _generative_model_ is a full
     probabilistic model of all variables, whereas a _discriminative_model_
     provides a model only for the target variables.]

    [It's important to remember that we rarely know precisely the value of any of
     these probabilities.  There is usually error in all of these P's, and in
     a generative model those errors can get compounded when we apply Bayes'
     Theorem to estimate P(Y | X).  In practice, generative models are most
     popular when you have phenomena that are really well fitted by the normal
     distribution.]

** lec 7
GAUSSIAN DISCRIMINANT ANALYSIS
==============================
Fundamental assumption:  each class comes from normal distribution (Gaussian).

                                                               2
                 2                   1                 |x - mu|
  X ~ N(mu, sigma ) :  P(x) = ---------------- exp ( - --------- )
                              sqrt(2 pi) sigma         2 sigma^2

For each class C, suppose we estimate mean mu_C, variance sigma_C^2, and
prior pi_C = P(Y = C).

Given x, Bayes rule r*(x) returns class C that maximizes P(X = x | Y = C) pi_C.

ln z is monotonically increasing for z > 0, so it is equivalent to maximize

                                                  2
                                        |x - mu_C|
  Q (x) = ln ( sqrt(2 pi) P(x) pi ) = - ----------- - ln sigma  + ln pi
   C                             C      2 sigma_C^2           C        C
  ^                       ^
  | quadratic in x.       | normal distribution, replaces P(X = x | Y = C)

[In a 2-class problem, you can also incorporate an asymmetrical loss function
 the same way we incorporated the prior pi_C.  In a multi-class problem, it
 gets a bit more complicated, because the penalty for guessing wrong might
 depend not just on the true class, but also on the wrong guess.]

QUADRATIC DISCRIMINANT ANALYSIS (QDA)
===============================
Suppose only 2 classes C, D.  Then

          /   C      if Q (x) - Q (x) > 0,
  r*(x) = |              C       D
          \   D      otherwise

Prediction fn is quadratic in x.  Bayes decision boundary is Q (x) - Q (x) = 0.
                                                              C       D
  - In 1D, B.d.b. may have 1 or 2 points.    [Solution to a quadratic equation]
  - In d-D, B.d.b. is a quadric.                [In 2D, that's a conic section]

[Show 2D Gaussian diagrams (from last lecture).]

[The equations I wrote down above can apply to a one-dimensional feature space,
 or they could apply equally well to a multi-dimensional feature space with
 isotropic, spherical Gaussians.  In the latter case, x and mu are vectors, but
 the variance is a scalar.  Next lecture we'll look at anisotropic Gaussians
 where the variance is different along different directions.]

[QDA works very nicely with more than 2 classes.  You typically wind up with
 multiple decision boundaries that adjoin each other at joints.  The feature
 space gets partitioned into regions.  In two or more dimensions, it looks like
 a sort of Voronoi diagram.]

[Show multiplicatively weighted Voronoi diagram.]

[You might not be satisfied with just knowing how each point is classified.
 One of the great things about QDA is that you can also determine the
 probability that your classification is correct.  Let's work that out.]


To recover posterior probabilities in 2-class case, use Bayes:

                           P(X | Y = C) pi_C
  P(Y = C | X) = -------------------------------------
                 P(X | Y = C) pi_C + P(X | Y = D) pi_D

                                                   Q_C(x)
                                           recall e       = sqrt(2 pi) P(x) pi
                                                                              C
                           Q_C(x)
                          e                       1
  P(Y = C | X = x) = ----------------- = --------------------
                      Q_C(x)    Q_D(x)        Q_D(x) - Q_C(x)
                     e       + e         1 + e

                   = s(Q (x) - Q (x)), where
                        C       D
                  1
  s(gamma) = -----------    <===  _logistic_fn_ aka _sigmoid_fn_
                  -gamma
             1 + e

[Show logistic fn.  s(0) = 1/2, s(inf) -> 1, s(-inf) -> 0, monoton increasing.]


LINEAR DISCRIMINANT ANALYSIS (LDA)            [less likely to overfit than QDA]
============================
Fundamental assumption:  all the Gaussians have same variance sigma.

[The equations simplify nicely in this case.]

                                        2       2
                  (mu_C - mu_D) . x   mu_C  - mu_D
  Q (x) - Q (x) = ----------------- - ------------- + ln pi  - ln pi
   C       D           sigma^2          2 sigma^2          C        D
                  \_______________/ \_______________________________/
                        w . x      +             alpha

[The quadratic terms in Q_C and Q_D cancelled each other out!]

Now it's a linear classifier!  Choose C that maximizes _linear_discriminant_fn_

                  2
  mu_C . x      mu_C
  -------- - --------- + ln pi                     [works for any # of classes]
  sigma^2    2 sigma^2        C

In 2-class case:  decision boundary is w . x + alpha = 0
                  Bayes posterior is P(Y = C | X = x) = s(w . x + alpha)

[The effect of "w . x + alpha" is to scale and translate the logistic fn
 in x-space.  It's a linear transformation.]
[Show two 1D Gaussians + logistic fn.]
[Show Voronoi diagram.]
                                                             mu_C + mu_D
If pi  = pi  = 1/2   ===>   (mu  - mu ) . x - (mu  - mu ) . (-----------) = 0
     C     D                   C     D           C     D          2

This is the centroid method!


MAXIMUM LIKELIHOOD ESTIMATION OF PARAMETERS (Ronald Fisher, circa 1912)
===========================================
[Before I talk about fitting Gaussians, I want to start with a simpler example.
 It's easier to understand maximum likelihood if we consider a discrete
 distribution first; then a continuous distribution later.]

Let's flip biased coins!  Heads with probability p; tails w/prob. 1 - p.

10 flips, 8 heads, 2 tails.  What is most likely value of p?

Binomial distribution:  X ~ B(n, p)

             /n\  x        n - x
  P[X = x] = | | p  (1 - p)
             \x/

Our example:  n = 10,

                 8        2      def
  P[X = 8] = 45 p  (1 - p)        =  L(p)

Probability of 8 heads in 10 flips:
written as a fn L(p) of distribution parameter(s), this is the _likelihood_fn_.

_Maximum_likelihood_estimation_ (MLE):  A method of estimating the parameters
of a statistical model by picking the params that maximize the likelihood fn.

[Let's phrase it as an optimization problem.]

  --------------------------------
  |  Find p that minimizes L(p)  |
  --------------------------------

[Show graph of L(p).]

Solve this example by setting derivative = 0:

  dL        7        2       8
  -- = 360 p  (1 - p)  - 90 p  (1 - p) = 0
  dp

  ===>    4 (1 - p) - p = 0    ===>    p = 0.8

[It shouldn't seem surprising that a coin that comes up heads 80% of the time
 is the coin most likely to produce 8 heads in 10 flips.]

       d^2L
Note:  ---- = -18.8744 < 0   at p = 0.8, confirming it's a maximum.
       dp^2


LIKELIHOOD OF A GAUSSIAN
========================
Given samples x_1, x_2, ..., x_n (scalars or vectors), find best-fit Gaussian.

[Let's do this with a normal distribution instead of a binomial distribution.
 If you generate a random point from a normal distribution, what is the
 probability that it will be exactly at the mean of the Gaussian?]

[Zero.  So it might seem like we have a bit of a problem here.  With a
 continuous distribution, the probability of generating any particular point
 is zero.  But we're just going to ignore that and do "likelihood" anyway.]

Likelihood of generating these samples is

  L(mu, sigma; x_1, ..., x_n) = P(x_1) P(x_2) ... P(x_n)

The _log_likelihood_ l(.) is the ln of the likelihood L(.).
Maximizing likelihood  <==>  maximizing log-likelihood.

  l(mu, sigma; x_1, ..., x_n) = ln P(x_1) + ln P(x_2) + ... + ln P(x_n)

                               dl
  Want to set grad   l = 0,  ------- = 0
                  mu         d sigma
                      2
              |x - mu|
  ln P(x) = - --------- - ln sqrt(2 pi) - ln sigma  [ln of normal distribution]
              2 sigma^2

              n  x_i - mu                 ^    1  n            [The ^hats^ mean
  grad   l = sum -------- = 0    ====>    mu = - sum x_i        "estimated"]
      mu     i=1 sigma^2                       n i=1
                          2        2
    dl       n  |x_i - mu|  - sigma                    ^  2   1  n           2
  ------- = sum ------------------ = 0      ====>    sigma  = - sum |x  - mu|
  d sigma   i=1      sigma^3                                  n i=1   i

                                        ^                      ^
We don't know mu exactly, so substitute mu for mu to compute sigma.

In short, we use mean & variance of samples in class C to estimate
mean & variance of Gaussian for class C.

For QDA:  estimate mean & variance of each class as above
        & estimate the priors (for each class C):

   ^      n_C
  pi  = -------                              [this is the coin flip parameter!]
    C   sum n_D  <==  sum of samples in all classes
         D

For LDA:  same mean & priors; one variance for all classes:

    ^  2   1                          2
  sigma  = - sum    sum     |x  - mu |
           n  C  {i:y  = C}   i     C
                     i

** lec 8
EIGENVECTORS
============
[I don't know if you were properly taught about eigenvectors here at Berkeley,
 but I sure don't like the way they're taught in most linear algebra books.
 So I'll start with a review.  You all know the definition of an eigenvector:]

Given matrix A, if Av = lambda v for some vector v != 0, scalar lambda, then
v is an _eigenvector_ of A and lambda is the associated _eigenvalue_ of A.

[But what does that mean?  It means that v is a magical vector that, after
 being multiplied by A, still points in the *same*direction*, or in exactly
 the *opposite*direction*.]

Eigenvalue 2:                                                             3   /
                                                                         A v /
          ^                   ^                   ^                   ^     /
          |                   |                   |                   |    /
          |                   |                   |   / 2             |   /
          |                   |                   |  / A v            |  /
          |  v                | / Av              | /                 | /
          |/                  |/                  |/                  |/
  <-------+------->   <-------+------->   <-------+------->   <-------+------->
          |                   |                   |                   |
          v                   v                   v                   v

Eigenvalue -1/2:

          ^                   ^                   ^                   ^
          |                   |                   |                   |
          |               \   |                   |                   |
          |             Aw \  |                   |               3   |
          |                 \ |                   |              A w  |
          |                  \|                   |                  \|
  <-------+------->   <-------+------->   <-------+------->   <-------+------->
          |\                  |                   |\                  |
          | \                 |                   | \  2              |
          |  \                |                   |   A w             |
          |   \               |                   |                   |
          |    \              |                   |                   |
          v     \ w           v                   v                   v
                 \
                  \

[For most matrices, most vectors don't have this property.  So the ones that do
 are special, and we call them eigenvectors.]
[Clearly, when you scale an eigenvector, it's still an eigenvector.  Only the
 direction matters, not the length.  Let's look at a few consequences.]

Theorem:  if v is eigenvector of A w/eigenvalue lambda,
        then v is eigenvector of A^k w/eigenvalue lambda^k     [will use later]

         2                          2
Proof:  A  v = A (lambda v) = lambda  v, etc.

Theorem:  moreover, if A is invertible,
          then v is eigenvector of A^-1 w/eigenvalue 1 / lambda^k

         -1       1     -1        1                [Look at the figures above,
Proof:  A   v = ------ A   Av = ------ v            but go from right to left.]
                lambda          lambda

[Stated simply:  When you invert a matrix, the eigenvectors don't change, but
 the eigenvalues get inverted.  When you square a matrix, the eigenvectors
 don't change, but the eigenvalues get squared.]

[Those theorems are pretty obvious.  The next theorem is not obvious at all.
 But it's going to be very useful for understanding the effect of a symmetric
 matrix on a vector that is *not* an eigenvector.]

_Spectral_Theorem_:  every symmetric n-by-n matrix has n eigenvectors that are
                                                 T
                     mutually orthogonal, i.e., v  v  = 0   for all i != j
                                                 i  j

[This takes about a page of math to prove.
 One minor detail is that a matrix can have more than n eigenvector directions.
 If two eigenvectors happen to have the same eigenvalue, then every linear
 combination of those eigenvectors is also an eigenvector.  Then you have
 infinitely many eigenvector directions, but they all span the same plane.
 So you just arbitrarily pick two vectors in that plane that are orthogonal to
 each other.  By contrast, the set of eigenvalues is always uniquely determined
 by a matrix, including the multiplicity of eigenvalues.]

We can use them as a basis for R^n.

[Now we can ask, what happens to a vector that *isn't* an eigenvector when
 you apply a symmetric matrix to it?  Express that vector as a linear
 combination of eigenvectors, and look at each eigenvector separately.]      #
                                                                         3  # /
                                                                        A x# /
          ^              Ax # ^                   ^                   ^    #/
          |                 # |                   |                   |   #/
          |               \  #|                   |   /               |  #/
          |                \ #|                   |  /     2          | #/
          |                 \ # /                 | /  ## A x         | #
          |/                 \#/                  |/ ##              \|#
  <-------+------->   <-------+------->   <-------+##----->   <-------+------->
          |#                  |                   |\                  |
          | ##                |                   | \                 |
          |  \#  x = v + w    |                   |                   |
          |   \#              |                   |                   |
          |    \##            |                   |                   |
          v     \ #           v                   v                   v
                 \ #
                  \

[Every time we apply A to this vector, it changes direction.  We can understand
 it by writing it as a sum of components that don't change direction.]

Write x as linear combo of eigenvectors:

  x = alpha v + beta w

   k                k                k
  A x = alpha lambda  v + beta lambda  w
                    v                w


Ellipsoids
----------
[Now, let's look what happens to a quadratic function when we apply a symmetric
 matrix to the space, with these two eigenvectors and eigenvalues.]

          T
  f(x) = x x                  <== quadratic; isotropic; isosurfaces are spheres
  g(x) = f(Ax)                <== A symmetric

          T  2                                                    2
       = x  A  x              <== _quadratic_form_ of the matrix A
                                  anisotropic; isosurfaces are ellipsoids

[Show isocontours for f(x) = |x|^2 and g(x) = |Ax|^2, where A = [ 3/4   5/4 ]
                                                                [ 5/4   3/4 ]
 Draw the stretch direction (1, 1) & the shrink direction (1, -1).]

[Here's how to think of this:  we stretched the plane on the right along the
 direction with eigenvalue 2, and shrunk the plane along the direction with
 eigenvalue -1/2; then we drew the circular isocontours, like on the left; then
 we undid the stretching and let the plane spring back to its original shape.
 So the circle turned into an ellipse when the plane sprang back.]

[Looking at the quadratic form is one of the best ways to visually understand
 symmetric matrices and their eigenvectors and eigenvalues.]

g(x) = 1 is an ellipsoid with axes v_1, v_2, ..., v_n and
                              radii 1/lambda_1, 1/lambda_2, ... 1/lambda_n

because if v  has length 1/lambda , g(v ) = f(Av ) = f(lambda  v ) = 1
            i                    i     i        i            i  i

        ==>  v_i  lies on the ellipsoid

[The reason the radii are the reciprocals of the eigenvalues is that we're
 stretching the plane by the eigenvalues, then drawing the spheres, then
 letting the plane spring back to its original shape.  When the plane springs
 back, each axis of the spheres gets scaled by 1/eigenvalue.]

bigger eigenvalue   <==>   steeper slope   <==>   shorter ellipsoid radius
                         [ ^ really bigger curvature; slope varies along axis]

Alternate interpretation:  ellipsoids are spheres in _distance_metric_ A^2

Call M = A^2 a _metric_tensor_ because
the distance between samples x & z in stretched space is

                                    T
  d(x, z) = |Ax - Az| = sqrt{(x - z)  M (x - z)}

[This is the Euclidean distance in the stretched space, but let's think of it
 as an alternative metric for measuring distances in the original space.  It's
 a kind of distance from x to z that's different from the Euclidean distance.]

[I'm calling M a "tensor" because that's standard usage in Riemannian geometry,
 but don't worry about what "tensor" means.  For our purposes, it's a matrix.]

Ellipsoids are "spheres" in this metric:  {x : d(x, center) = isovalue}


A square matrix B is _positive_definite_     if w^T B w > 0  for all w != 0.
                                             <==>   all eigenvalues positive
                     _positive_semidefinite_ if w^T B w >= 0 for all w.
                                             <==>   all eigenvalues nonnegative
                     _indefinite_            if +ve eigenvalue & -ve eigenvalue
                     invertible              if no zero eigenvalue

[Show figures of ellipses for +ve definite, +ve semidefinite, indefinite
 matrices, and inverse of +ve definite matrix; separate "whiteboard".
 Positive eigenvalues correspond to axes where the curvature goes up; negative
 eigenvalues correspond to axes where the curvature goes down.]

Metric tensors must be symmetric +ve definite (SPD).

[Remember that M = A^2, so M's eigenvalues are the squares of the eigenvalues
 of A, so the eigenvalues must be nonnegative and M is positive semidefinite.
 But if M has a zero eigenvalue, its distance function is not a "metric".
 To have a metric, you must have a strictly positive definite M.  If you have
 eigenvalues of zero, the isosurfaces are cylinders instead of ellipsoids.]

Special case:  M & A are diagonal   <==>  eigenvectors are coordinate axes
                                    <==>  ellipsoids are _axis-aligned_

[Draw axis-aligned isocontours for a diagonal metric.]

Building a Quadratic w/Specified Eigenvectors/values
----------------------------------------------------
[I, personally, find the process of going from eigenvectors and eigenvalues to
 a matrix and some ellipsoids to be more intuitive than the reverse.  So let's
 do that.  Suppose you know which ellipsoid axes you want to use, and you know
 what ellipsoid radius or stretch factor you want to use along each axis.]

Choose n mutually orthogonal *unit* n-vectors v_1, ..., v_n
  [so they specify an orthonormal coordinate system]
Let V = [v_1   v_2   ...   v_n]                               <== n-by-n matrix
Observe:  V^T V = I           [off-diagonal 0's because vectors are orthogonal]
                              [diagonal 1's because unit vectors]
==>   V^T = V^-1   ==>   V V^T = I
V is _orthonormal_matrix_:  acts like rotation (or reflection)

Choose some inverse radii lambda_i:
             -                                      -
             | lambda_1      0       ...      0     |
Let Lambda = |    0       lambda_2            0     |          [diagonal matrix
             |   ...                                |           of eigenvalues]
             |    0          0       ...   lambda_n |
             -                                      -

                        T    n               T
Theorem:  A = V Lambda V  = sum lambda_i v  v   has chosen eigenvectors/values
                            i=1           i  i       [Clearly, A is symmetric]
               ^       ^                 \___/
          equivalent   |             outer product:  n-by-n matrix, rank 1
               v       |
Proof:  AV = V Lambda  |       <== definition of eigenvectors! (in matrix form)
                       |
This is a _matrix_factorization_ called the _eigendecomposition_.
Lambda is the _diagonalized_ version of A.
V^T rotates ellipsoid to be axis-aligned.

This is also a recipe for building quadratics with axes v_i, radii 1 / lambda_i

Observe:  M = A^2 = V Lambda V^T V Lambda V^T = V Lambda^2 V^T

Given SPD metric tensor M, we can find a symmetric _square_root_ A = M^{1/2}:
- compute eigenvectors/values of M
- take square roots of M's eigenvalues
- reassemble matrix A

[The first step--breaking a matrix down to its eigenvectors and eigenvalues--is
 much harder than the last step--building up a new matrix from its eigenvectors
 and eigenvalues.  But I think that the latter process helps take a lot of the
 mystery out of eigenvectors.]


ANISOTROPIC GAUSSIANS
=====================
  X ~ N(mu, Sigma)

                     1                      1         T      -1
  P(x) = -------------------------- exp ( - - (x - mu)  Sigma   (x - mu) )
         sqrt(2 pi)^d sqrt(|Sigma|)         2
                           ^ determinant of Sigma

Sigma is the SPD _covariance_matrix_.
Sigma^-1 is the SPD _precision_matrix_; serves as metric tensor.

[Show example of paraboloid q(x) and bivariate Gaussian n(q(x)), w/univariate
 Gaussian n(.) between them.]

** lec 9
ANISOTROPIC GAUSSIANS
=====================
  X ~ N(mu, Sigma)                     <==  X is a random d-vector with mean mu

                     1                      1         T      -1
  P(x) = -------------------------- exp ( - - (x - mu)  Sigma   (x - mu) )
         sqrt(2 pi)^d sqrt(|Sigma|)         2
                           ^ determinant of Sigma

Sigma is the d-by-d SPD _covariance_matrix_.
Sigma^-1 is the d-by-d SPD _precision_matrix_; serves as metric tensor.

                                           T      -1
Write P(x) = n(q(x)), where q(x) = (x - mu)  Sigma   (x - mu)
             ^              ^
     R->R, exponential   R^d -> R, quadratic

[Now q(x) is a function we understand--it's just a quadratic bowl centered at
 mu whose curvature is represented by the metric tensor Sigma^-1.  q(x) is the
 squared distance from mu to x under this metric.  The other function n(.) is
 like a 1D Gaussian with a different normalization factor.  It is helpful to
 understand that this mapping n(.) does not change the isosurfaces.]

Principle:  given f:R -> R, isosurfaces of f(q(x)) are same as q(x)
            (different isovalues), except that some might be "combined"
            [if f maps them to the same value]

[Show example of paraboloid q(x) and bivariate Gaussian n(q(x)), w/univariate
 Gaussian n(.) between them.]
                                                  T        T          T
_covariance_:  Cov(X, Y) = E[(X - E[X]) (Y - E[Y]) ] = E[XY ] - mu  mu
               Var(X) = Cov(X, X)                                 X   Y
               [These two definitions hold for both vectors and scalars.]

For a Gaussian, one can show Var(X) = Sigma.
[...by integrating the expectation in anisotropic spherical coordinates.]
Hence
          -                                                        -
          |   Var(X_1)       Cov(X_1, X_2)    ...    Cov(X_1, X_d) |
  Sigma = | Cov(X_2, X_1)      Var(X_2)              Cov(X_2, X_d) |
          |      ...                                      ...      |
          | Cov(X_d, X_1)    Cov(X_d, X_2)    ...      Var(X_d)    |
          -                                                        -

[An important point is that statisticians didn't just arbitrarily decide to
 call this thing a covariance matrix.  Rather, statisticians discovered that
 if you find the covariance of the normal distribution by integration, it turns
 out that each covariance happens to be an entry in the matrix inverse to the
 metric tensor.  This is a happy discovery; it's rather elegant.]

[Observe that Sigma is symmetric, as Cov(X_i, X_j) = Cov(X_j, X_i).]

X_i, X_j independent   ==>   Cov(X_i, X_j) = 0
Cov(X_i, X_j) = 0  AND  joint normal distribution   ==>   X_i, X_j independent
all features pairwise independent   ==>   Sigma is diagonal
Sigma is diagonal   <==>   axis-aligned Gaussian; squared radii on the diagonal
                    <==>   P(X) = P(X_1) P(X_2) ... P(X_d)
                           \__/   \______________________/
                     multivariate     each univariate

[So when the features are independent, you can write the multivariate Gaussian
 as a product of univariate Gaussians.  When they aren't, you can do a change
 of coordinates to the eigenvector coordinate system, and write it as a product
 of univariate Gaussians in those coordinates.]

[It's tricky to keep track of the relationships between the matrices, so here's
 a handy chart.]

            covariance                 precision
              matrix                    matrix
                                       (metric)
                           inverse              -1
     ----->   Sigma     <----------->  M = Sigma
    /          ^                          ^
    |          | |                        | |
    |   square | |                 square | |
    |          | | root                   | | root
    |          | |                        | |
    |            v                          v
    |             1/2      inverse              -1/2
    |        Sigma      <----------->  A = Sigma
    |   (maps spheres to             (maps ellipsoids
    |      ellipsoids)                  to spheres)
    |
    |          ^                     1/2
    |          | eigenvalues of Sigma    are ellipsoid radii
    |            (standard deviations along the eigenvectors) ---\
    |                                                            |
    eigenvalues of Sigma are variances along the eigenvectors    |
                                |                                v
                                v     T                1/2           1/2  T
    Diagonalizing:  Sigma = V Lambda V            Sigma    = V Lambda    V

[Remember that all four of these matrices have the same eigenvectors V.
 Remember that when you take the inverse or square or square root of an SPD
 matrix, you do the same to its eigenvalues.  So the ellipsoid radii, being the
 eigenvalues of Sigma^1/2, are the square roots of the eigenvalues of Sigma;
 moreover, they are the inverse square roots of the precision matrix (metric).]

[Keep this chart handy when you do Homework 3.]


Maximum likelihood estimation for anisotropic Gaussians
-------------------------------------------------------
Given samples x_1, ..., x_n and classes y_1, ..., y_n, find best-fit Gaussians.

[Once again, we want to fit the Gaussian that maximizes the likelihood of
 generating the samples in a specified class.  This time I won't derive the
 maximum-likelihood Gaussian; I'll just tell you the answer.]

For QDA:

    ^      1                                  T
  Sigma  = -     sum     (x  - mu ) (x  - mu )     <== _conditional_covariance_
       C   n  {i:y  = C}   i     C    i     C          for samples in class C
            C     i      \____________________/
                          outer product matrix       [Show example maxlike.jpg]

Priors pi_C, means mu_C:  same as before
[Priors are class samples / total samples; means are per-class sample means]
  ^
Sigma_C is positive semidefinite, but not always definite!
If some zero eigenvalues, must eliminate the zero-variance dimensions
(eigenvectors).

[I'm not going to discuss how to do that today, but it involves projecting the
 samples onto a subspace along the eigenvectors with eigenvalue zero.]

For LDA:

    ^      1                                     T
  Sigma  = - sum    sum     (x  - mu ) (x  - mu )     <== _pooled_within-class_
           n  C  {i:y  = C}   i     C    i     C          _covariance_matrix_
                     i

[Notice that although we're computing one covariance matrix for all the data,
 each sample contributes with respect to *its*own*class's*mean*.  This gives
 a very different result than if you simply compute one covariance matrix for
 all the samples using the global mean!  In the former case, the variances are
 typically smaller.]


[Let's revisit QDA and LDA and see what has changed now that we know
 anisotropic Gaussians.  The short answer is "not much has changed".
 By the way, capital X is once again a random variable.]

QDA
---
pi , mu , Sigma  may be different for each class C.
  C    C       C

Choosing C that maximizes P(X = x | Y = C) pi  is equivalent to maximizing
the _quadratic_discriminant_fn_              C
                        d               1         1
  Q (x) = ln (sqrt(2 pi)  P(x) pi ) = - - q (x) - - ln |Sigma | + ln pi
   C                      ^      C      2  C      2          C         C
                          |
                    Gaussian for C                 [works for any # of classes]

2 classes:  Prediction fn Q (x) - Q (x) is quadratic, but may be indefinite
                           C       D
            ==>  Bayes decision boundary is a quadric.

            Posterior is P(Y = C | X = x) = s(Q (x) - Q (x))
                                               C       D
            where s(.) is logistic fn

[Show example where decision boundary is a hyperbola:  two anisotropic
 Gaussians, Q_C - Q_D, logistic fn, posterior.  Show anisotropic Voronoi
 diagram.  Separate "whiteboard".]

LDA
---
One Sigma for all classes.
[Once again, the quadratic terms cancel each other out so the predictor
 function is linear and the decision boundary is a hyperplane.]

Q (x) - Q (x) =
 C       D
                           T       -1          T       -1
           T      -1     mu_C Sigma   mu_C - mu_D Sigma   mu_D
(mu  - mu )  Sigma   x - ------------------------------------- + ln pi  - ln pi
   C     D                                 2                          C        D
\____________________/ \_______________________________________________________/
          T
         w x          +                          alpha

Choose class C that maximizes the _linear_discriminant_fn_

    T      -1     1   T      -1
  mu  Sigma   x - - mu  Sigma   mu  + ln pi        [works for any # of classes]
    C             2   C           C        C

                                  T
2 classes:  Decision boundary is w x + alpha = 0
                                               T
            Posterior is P(Y = C | X = x) = s(w x + alpha)

[Show example where decision boundary is a line:  two anisotropic Gaussians,
 Q_C - Q_D, logistic fn, posterior.  Show ESL, Figure 4.11 (LDAdata.pdf):
 example of LDA with messy data.  Separate "whiteboard".]


Notes:
- Changing prior pi_C (or loss) is easy:  adjust alpha.
- LDA often interpreted as projecting samples onto normal vector w;
  cutting the line in half.
  [Show projection onto normal vector (project.png)]
- For 2 classes, LDA has d+1 parameters (w, alpha);
                 QDA has d(d+3)/2 + 1 parameters;
                 QDA more likely to overfit.
                 [Show comparison (ldaqda.pdf)]
- With features, LDA can give nonlinear boundaries; QDA can give nonquadratic.
- We don't get *true* optimum Bayes classifier
  -- estimate distributions from finite data
  -- real-world data not perfectly Gaussian
- Posterior gives decision boundaries for 10% confidence, 50%, 90%, etc.
  -- Choosing isovalue = probability p is same as choosing asymmetrical loss
     p for false positive, (1 - p) for false negative.


[LDA & QDA are best in practice for many applications.  In STATLOG project, LDA
 or QDA were in the top three classifiers for 10 out of 22 datasets.  But it's
 not because all those datasets are Gaussian.  LDA & QDA work well when the
 data can only support simple decision boundaries such as linear or quadratic,
 because Gaussian models provide stable estimates.]  [ESL, Section 4.3]

** lec 10
REGRESSION aka Fitting Curves to Data
==========
Classification:  given sample x, predict class (often binary)
Regression:      given sample x, predict a numerical value

[Classification gives a discrete prediction, whereas regression gives us
 a quantitative prediction, usually on a continuous scale.]
[We've already seen an example of regression in Gaussian discriminant analysis.
 QDA and LDA don't just give us a classifier; they also give us the probability
 that a particular class label is correct.  So QDA and LDA implicitly do
 regression on probability values.]

- Choose form of regression fn h(x; p) with parameters p     (h = _hypothesis_)
  - like predictor fn in classification [e.g. linear, quadratic, logistic in x]
- Choose a cost fn (objective fn) to optimize
  - usually based on a loss fn; e.g. risk fn = expected loss

Some regression fns:
(1)  linear:     h(x; w, alpha) = w^T x + alpha
(2)  polynomial
     [equivalent to linear regression with added polynomial features]
(3)  logistic:   h(x; w, alpha) = s(w^T x + alpha)
                                             1
     recall:     logistic fn s(gamma) = -----------
                                             -gamma
                                        1 + e
     [This choice is interesting.  You'll recall that LDA produces a posterior
      probability fn with this equation.  So this equation seems to be
      a natural form for modeling certain probabilities.  If we want to model
      probabilities, sometimes we use LDA; but alternatively, we could skip
      fitting Gaussians to samples, and instead just directly try to fit
      a logistic function to a set of probabilities.]

Some loss fns:  let z be prediction h(x); y be true value
(A)  L(z, y) = (z - y)^2                        _squared_error_
(B)  L(z, y) = |z - y|                          _absolute_error_
(C)  L(z, y) = - y ln z - (1 - y) ln (1 - z)    _logistic_loss_:  y in [0, 1],
                                                                  z in (0, 1)
Some cost fns to minimize:
            1  n
(a)  J(h) = - sum L(h(X ), y )                  _mean_loss_
            n i=1      i    i                   [you can leave out the "1/n"]
             n
(b)  J(h) = max L(h(X ), y )                    _maximum_loss_
            i=1      i    i
             n
(c)  J(h) = sum omega_i L(h(X ), y )            _weighted_sum_ [some samples
            i=1              i    i             more important than others]
            1  n                           2
(d)  J(h) = - sum L(h(X ), y ) + lambda |w|     l  _penalized_/_regularized_
            n i=1      i    i                    2
            1  n
(e)  J(h) = - sum L(h(X ), y ) + lambda |w|     l  _penalized_/_regularized_
            n i=1      i    i              l_1   1

Least-squares linear regr.:  (1) + (A) + (a) \
Weighted least-squ. linear:  (1) + (A) + (c) | quadratic; minimize w/calculus
Ridge regression:            (1) + (A) + (d) /
Lasso:                       (1) + (A) + (e) \ minimize w/gradient descent
Logistic reg.:               (3) + (C) + (a) /
Least absolute deviations:   (1) + (B) + (a) \ minimize w/linear programming
Chebyshev criterion:         (1) + (B) + (b) /

[I have given you several choices of regression fn form, several choices of
 loss fn, and several choices of objective fn.  These are interchangeable parts
 where you can snap one part out and replace it with a different one.  But the
 optimization algorithm and its speed depend crucially on which parts you pick.
 Let's see some examples.]

LEAST-SQUARES LINEAR REGRESSION
===============================
linear regression fn (1) + squared loss fn (A) + cost fn (a).

  -----------------------------------------------------------
  |                               n                       2 |
  | Find w, alpha that minimizes sum (X  . w + alpha - y )  |
  |                              i=1   i                i   |
  -----------------------------------------------------------
[Show linear regression example (linregress.pdf)]

Convention:  X is n-by-d _design_matrix_ of samples
             y is d-vector of dependent scalars.
  -                                             -                 -     -
  | X_11   X_12   ...   |  X_1j  |   ...   X_1d |                 | y_1 |
  | X_21   X_22         |  X_2j  |         X_2d |                 | y_2 |
  |  ...                |        |              |                 |     |
  | --------------------+--------+------------- |           T     | ... |
  | X_i1   X_i2         |  X_ij  |         X_id | < sample X      |     |
  | -----------------------------+------------- |           i     |     |
  |  ...                |        |              |                 |     |
  | X_n1   X_n2   ...   |  X_nj  |         X_nd |                 | y_n |
  -                                             -                 -     -
                           ^                                        ^
                           feature column X                         y
                                           *j
Usually n > d.

Recall fictitious dimension trick [from Lecture 3]:  replace x . w + alpha with

                    -       -
                    |  w_1  |
  [ x_1  x_2  1 ] . |  w_2  |
                    | alpha |
                    -       -

Now X is an n-by-(d+1) matrix;   w is a (d+1)-vector.

  -----------------------------------
  |                               2 |
  | Find w that minimizes |Xw - y|  | = RSS(w), for _residual_sum_of_squares_
  -----------------------------------

[We know X and y, but w is unknown; we want to solve for w.]
Optimize by calculus:

                   T T       T      T
minimize RSS(w) = w X Xw - 2y Xw + y y

                    T       T
    grad RSS    = 2X Xw - 2X y = 0

                    T      T
              ==>  X Xw = X y                   <== the _normal_equations_
                   \_/\_____/                       [w unknown; X & y known]
        (d+1)-by-(d+1)  (d+1)-vectors
    T
If X X is singular, problem is underconstrained [because the samples all lie on
a common hyperplane.  Notice that X^T X is always positive semidefinite.]

                                     T  -1  T
We use a linear solver to find w = (X X)   X y         [never actually invert!]
                                   \________/
                                   X^+, the _pseudoinverse_ of X, (d+1)-by-n

[X is usually not square, so it can't have an inverse.  However, every X has
 a pseudoinverse X^+, and if X has rank d+1, then X^+ is a "left inverse".]

           +      T  -1  T
Observe:  X X = (X X)   X X = I   <= (d+1)-by-(d+1)   [which explains its name]

                                        ^                  ^          +
Observe:  the predicted values of y are y  = h(x )   ==>   y = Xw = XX y = Hy
                                         i      i
                      +
          where H = XX  is called the _hat_matrix_ because it puts the hat on y
                        n-by-n [and if n > d+1, it is not the identity matrix!]

Interpretation as a projection:
  ^
- y = Xw in R^n is a linear combination of columns of X (one per feature)
- For fixed X, varying w, Xw is subspace of R^n spanned by columns

               O y
       ________|___________   Figure in n-dimensional space (1 dim/sample)
      /       _|          /   NOT d-dimensional feature space (row space of X).
     /       | | ^       /
    /        ' O y = Xw /  <== subspace spanned by X's columns
   /                   /       (at most d+1 dimensions)
  ---------------------

              ^                  ^
- Minimizing |y - y| finds point y nearest y on subspace
  ==>  projects y onto subspace
       [the vertical line is the direction of projection and the error vector]
                                                              T
- Error is smallest when line is perpendicular to subspace:  X (Xw - y) = 0
                                                      ==> the normal equations!
- Hat matrix H does the projecting.  [Also sometimes called projection matrix.]

Advantages:
- Easy to compute; just solve a linear system.
- Unique, stable solution.  [Except when the problem is underconstrained.]
Disadvantages:
- Very sensitive to outliers, because error is squared!
- Fails if X^T X is singular.

[Least-squares linear regression was apparently first posed and solved in 1801
 by the great mathematician Carl Friedrich Gauss, who used least squares to
 predict the trajectory of the planetoid Ceres.  A paper he wrote on the topic
 is regarded as the birth of modern linear algebra.]


LOGISTIC REGRESSION (David Cox, 1958)
===================
logistic regression fn (3) + logistic loss fn (C) + cost fn (a).
Fits "probabilities" in range (0, 1).

Usually used for classification.  The input y_i's *can* be probabilities,
but in most applications they're all 0 or 1.

QDA, LDA:  generative models
logistic regression:  discriminative model

With X and w including the fictitious dimension; alpha is w's last component...
  -------------------------------------------------------------
  | Find w that maximizes                                     |
  |                                                           |
  |      n  /                                               \ |
  | J = sum | y  ln s(X  . w) + (1 - y ) ln (1 - s(X  . w)) | |
  |     i=1 \  i       i              i             i       / |
  -------------------------------------------------------------
[Note that we are maximizing, not minimizing, this function because
 I've flipped the sign of the logistic loss (C).]
[Show log loss plots (logloss0.pdf, logloss.7.pdf)]

-J(w) is convex!  [J is "concave".]  Solve by gradient ascent.

[To do gradient ascent, we'll need to compute some derivatives.]

                                              -gamma
                   d          1              e
  s'(gamma)  =  -------  -----------  =  --------------
                d gamma       -gamma           -gamma 2
                         1 + e           (1 + e      )

             =  s(gamma) (1 - s(gamma))          [Show s' plot (dlogistic.pdf)]

Let s  = s(X  . w)
     i      i
                    / y_i            1 - y_i          \
    grad  J  =  sum | --- grad s_i - ------- grad s_i |
        w           \ s_i            1 - s_i          /

                    / y_i   1 - y_i \
             =  sum | --- - ------- | s  (1 - s ) X
                    \ s_i   1 - s_i /  i       i   i

             =  sum (y  - s ) X
                      i    i   i
                                         n
Gradient ascent rule:  w <- w + epsilon sum (y  - s(X  . w)) X
                                        i=1   i      i        i

Stochastic gradient ascent:  w <- w + epsilon (y  - s(X  . w)) X
                                                i      i        i
Works best if we shuffle samples in random order, process one by one.
For very large n, sometimes converges before we visit all samples!

[This looks a lot like the perceptron learning rule.  The only difference is
 that the "- s_i" part is new.]

Starting from w = 0 works well in practice.

[Show mwascom's logistic regression plot (problogistic.png) from
http://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression]

** lec 11
WEIGHTED LEAST-SQUARES REGRESSION
=================================
linear regression fn (1) + squared loss fn (A) + cost fn (c).

[The idea of weighted least-squares is that some samples might be more trusted
 than others, or there might be certain samples you want to fit particularly
 well.  So you assign those more trusted samples a higher weight.  If you
 suspect some samples of being outliers, you can assign them a lower weight.]

Assign each sample a weight omega_i; collect them in nxn diagonal matrix Omega.
                                            ^                          ^
Greater omega_i -> work harder to minimize |y_i - y_i|        recall:  y = Xw

  --------------------------------------------------
  |                               T                |    n                     2
  | Find w that minimizes (Xw - y)  Omega (Xw - y) | = sum omega  ((Xw)  - y )
  --------------------------------------------------   i=1      i      i    i

                                   T             T
Solve for w in normal equations:  X  Omega Xw = X  Omega y

[Once again, you can interpret it as a projection:]
            1/2 ^                                  1/2
Note:  Omega    y is orthogonal projection of Omega    y onto
                                           1/2
       subspace spanned by columns of Omega    X.

[If you stretch the n-dimensional space by applying the linear transformation
 Omega^1/2, it's an orthogonal projection in that stretched space.]


NEWTON'S METHOD
===============
Iterative optimization method for smooth fn J(w).
Often much faster than gradient descent.  [We'll use for logistic regression.]

Idea:  You're at point v.  Approximate J(w) near v by quadratic fn.
       Jump to its unique critical pt.  Repeat until bored.

[Show method in 1D (newton1.pdf--newton3.pdf); in 2D (newton2D.png).]

Taylor series about v:
                               2                          2
  grad J(w) = grad J(v) + (grad  J(v)) (w - v) + O(|w - v| )

  where grad^2 J(v) is the _Hessian_matrix_ of J(w) at v.

Find critical pt w by setting grad J(w) = 0:

               2      -1
  w = v - (grad  J(v))   grad J(v)

[This is an iterative update rule you can repeat until it converges to
 a solution.  As usual, we don't really want to compute a matrix inverse.]

Newton's method:

  pick starting point w
  repeat until convergence              2
    e <- solution to linear system (grad  J(w)) e = - grad J(w)
    w <- w + e

Warning:  Doesn't know difference between minima, maxima, saddle points.
          Starting point must be "close enough" to desired solution.

[If the objective function J is actually quadratic, Newton's method needs only
 one step to find the correct answer.  The closer J is to quadratic, the faster
 Newton's method tends to converge.]
[Newton's method is superior to blind gradient descent for some optimization
 problems for several reasons.  First, it tries to find the right step length
 to reach the minimum, rather than just walking an arbitrary distance downhill.
 Second, rather than follow the direction of steepest descent, it tries to
 optimize all directions at once.]
[Nevertheless, it has some major disadvantages.  The biggest one is that
 computing the Hessian can be quite expensive, and it has to be recomputed
 every iteration.  It can work well for low-dimensional feature spaces, but
 you would never use it for a neural net, because there are too many weights.
 Newton's method also doesn't work for most nonsmooth functions.  It
 particularly fails for the perceptron risk function, whose Hessian is zero,
 except where it's not even defined.]


LOGISTIC REGRESSSION (continued)
====================
Recall:  s'(gamma) = s(gamma) (1 - s(gamma))          s  = s(X  . w)
                                                       i      i
                       n                  T
         grad  J(w) = sum (y  - s ) X  = X  (y - s)
             w        i=1   i    i   i
where s is n-vector with components s_i
[Now we work out the Hessian too, so we can use Newton's method.]

      2           n                  T      T
  grad  J(w) = - sum s  (1 - s ) X  X  = - X  Omega X
      w          i=1  i       i   i  i

where Omega is diagonal matrix with components s  (1 - s )
                                                i       i
                                     T
Omega is +ve definite for all w, so X  Omega X is +ve semidefinite,
so -J(w) is convex.
[Therefore, Newton's method finds an optimal point if it converges at all.]

Newton's method:
                                    T               T
Solve for e in normal equations:  (X  Omega X) e = X  (y - s)
w <- w + e                              ^                  ^
repeat until "convergence"         Remember:  Omega, s are fns of w

[Notice that this looks a lot like weighted least squares, but the weight
 matrix Omega and the right-hand-side vector y-s change every iteration.]
An example of _iteratively_reweighted_least_squares_.
Omega prioritizes samples with s_i near 0.5; tunes out samples near 0/1.
[In other words, samples near the decision boundary have the biggest effect on
 the iterations.  Meanwhile, the iterations move the decision boundary; in
 turn, that movement may change which samples have the most influence.  In the
 end, only the samples near the decision boundary make a big contribution to
 the logistic fit.]

Idea:  If n very large, save time by using a random subsample of the samples
per iteration.  Increase sample size as you go.

[The principle is that the first iteration isn't going to take you all the way
 to the optimal point, so why waste time looking at all the samples?  Whereas
 the last iteration should be the most accurate one.]

LDA vs. logistic regression
---------------------------
Advantages of LDA:
- For well-separated classes, LDA stable; log. reg. surprisingly unstable
- > 2 classes easy & elegant, log. reg. needs modifying ("softmax regression")
- Slightly more accurate when classes nearly normal, especially if n is small
Advantages of log. reg.:
- More emphasis on decision boundary [though not as much as SVMs]
  [Show figure of log.reg. vs. LDA for tight boundary (logregvsLDAauni.pdf)]
- Hence less sensitive to outliers
- Easy & elegant treatment of "partial" class membership; LDA is all-or-nothing
- More robust on some non-Gaussian distributions (e.g. large skew)


ROC CURVES (for test sets)
==========
[Show ROC curve (ROC.pdf).]
[This is a _ROC_curve_.  That stands for _receiver_operating_characteristics_,
 which is an awful name but we're stuck with it for historical reasons.
 It is made by running a classifier on the test set or validation set.
 It shows the rate of false positives vs. true positives for a range of
 settings.
 We assume there is a knob we can turn to trade off these two types of error;
 in this case, that knob is the probability threshold for Gaussian discriminant
 analysis or linear regression.
 However, neither axis is the probability threshold.]
x-axis:  "false positive rate = % of -ve classified as +ve"
y-axis:  "true positive rate = % of +ve classified as +ve  aka _sensitivity_"
"false negative rate":  vertical distance from curve to top
"_specificity_":  horizontal distance from curve to right
[You generate this curve by trying *every* probability threshold; for each
 threshold, measure the false positive & true positive rates and plot a point.]
upper right corner:  "always classify +ve (Pr >= 0)"
lower left corner:  "always classify -ve (Pr > 1)"
diagonal:  "random classifiers"
[A rough measure of a classifier's effectiveness is the area under the curve.
 For a classifier that is always correct and never assigns a probability
 between 0 and 1, the area under the curve is one.

[IMPORTANT:  In practice, the trade-off between false negatives and false
positives is usually negotiated by choosing a point on this plot, based on real
test data, and NOT by taking the choice of threshold that's best in theory.]


LEAST-SQUARES POLYNOMIAL REGRESSION
===================================
Replace each X  with feature vector Phi(X ) with all terms of degree 1...p
              i                          i
                  2               2                   T
e.g. Phi(X ) = [ X     X   X     X     X     X     1 ]
          i       i1    i1  i2    i2    i1    i2

[Notice that we've added the fictitious dimension "1" here, so we don't need
 to add it again.  This basis covers all polynomials quadratic in X_i1, X_i2.]

Can also use non-polynomial features (e.g. edge detectors).
Otherwise just like linear or logistic regression.

Log. reg. + quadratic features = same logistic posteriors as QDA.

Very easy to overfit!      [Show overfits (overunder.png; UScensusquartic.png)]

** lec 12
STATISTICAL JUSTIFICATIONS FOR REGRESSION
=========================================
[So far, I've talked about regression as a way to fit curves to points.
 Recall how early in the semester I divided machine learning into 4 levels:
 the application, the model, the optimization problem, and the optimization
 algorithm.  My last two lectures about regression were at the bottom two
 levels:  optimization.  The cost functions that we optimize are somewhat
 arbitrary.  Today let's take a step back to the second level, the model.
 I will describe some models, how they lead to those optimization problems, and
 how they contribute to underfitting or overfitting.]

Typical model of reality:
- samples come from unknown prob. distribution:  X_i ~ D
- y-values are sum of unknown, non-random surface + random noise:  for all X_i,

  y  = f(X ) + epsilon ,                    epsilon  ~ D', D' has mean zero
   i      i           i                            i

[We are positing that reality is described by a function f.  We don't know f,
 but f is not a random variable; it represents a real relationship between
 x and y that we can estimate.  We add to that a random variable epsilon, which
 represents measurement errors and all the other sources of statistical error
 when we measure real-world phenomena.  Notice that the noise is independent of
 x.  That's a pretty big assumption, and often it does not apply in practice,
 but that's all we'll have time to deal with this semester.  Also notice that
 this model leaves out systematic errors, like when your measuring device adds
 one to every measurement, because we usually can't diagnose systematic errors
 from data alone.]

Goal of regression:  find h that estimates f.

Ideal approach:  choose h(x) = E [Y | X = x] = f(x) + E[epsilon] = f(x)
                                Y

[If this expectation exists at all, it partly justifies our model of reality.
 We can retroactively define f to be this expectation.]
[Draw figure showing example f, distribution for a fixed x.]

Least-Squares Regression from Maximum Likelihood
------------------------------------------------
Suppose epsilon  ~ N(0, sigma^2);  then y  ~ N(f(X ), sigma^2)
               i                         i        i
Recall that log likelihood for normal dist. is
                         2
               |y_i - mu|                     <=  mu = f(X )
  ln P(y ) = - ----------- - constant,                    i
        i       2 sigma^2

  ln (P(y ) P(y ) ... P(y )) = ln P(y ) + ln P(y ) + ... ln P(y )
         1     2         n           1          2              n

Takeaway:  Max likelihood => find f by least-squares

[So if the noise is normally distributed, maximum likelihood justifies using
 the least-squares cost function.]
[However, I've told you in previous lectures that least-squares is very
 sensitive to outliers.  If the error is truly normally distributed, that's not
 a big deal, especially when you have a lot of samples.  But in the real world,
 the real distribution of outliers often isn't normal.  Outliers might come
 from wrongly measured measurements, data entry errors, anomalous events, or
 just not having a normal distribution.  When you have a heavy-tailed
 distribution, for example, least-squares isn't a good choice.]


Empirical Risk
--------------
The _risk_ for hypothesis h is expected loss R(h) = E[L] over all x, y.
Discriminative model:  we don't know X's dist. D.  How can we minimize risk?
[If we have a generative model, we can estimate the probability distributions
 for X and Y and derive the expected loss.  That's what we did for Gaussian
 discriminant analysis.  But today I'm assuming we don't have a generative
 model, so we don't know those probabilities.  Instead, we're going to
 approximate the distribution in a very crude way:  we pretend that the samples
 *are* the distribution.]

_Empirical_distribution_:  a discrete probability distribution that IS the
                           sample set, with each sample equally likely
_Empirical_risk_:  expected loss under empirical distribution
                   ^      1  n
                   R(h) = - sum L(h(X_i), y_i)
                          n i=1
[The hat on the R indicates it's only a cheap approximation of the true,
 unknown statistical risk we really want to optimize.  Often, this is the best
 we can do.  For many but not all distributions, it converges to the true risk
 in the limit as n -> infinity.]

Takeaway:  this is why we [usually] minimize the sum of loss fns.


Logistic regression from maximum likelihood
-------------------------------------------
If we accept the logistic regression fn, what cost fn should we use?

Given arbitrary sample x, write probability it is in (not in) the class:
(Fictitious dimension:  x ends w/1; w ends w/alpha)

  P(y = 1 | x; w) = h(x; w)                              <=  h(x; w) = s(w^T x)
  P(y = 0 | x; w) = 1 - h(x; w)                              [s is logistic fn]

Combine these 2 facts into 1:

                    y           1-y       [A bit of a hack, but it works
  P(y | x; w) = h(x)  (1 - h(x))           nicely for intermediate values of y]

Likelihood is
                       n
  L(w; x , ..., x ) = prod P(y  | X ; w)
        1        n    i=1     i    i

Log likelihood is
                    n
  l(w) = ln L(w) = sum ln P(y  | X ; w)
                   i=1       i    i
          n  /                                       \
       = sum | y  ln h(X ) + (1 - y ) ln (1 - h(X )) |
         i=1 \  i       i          i             i   /

...which is negated logistic cost fn J(w).
We want to maximize log likelihood  =>  minimize J.


THE BIAS-VARIANCE DECOMPOSITION
===============================
There are 2 sources of error in a hypothesis h:
_bias_:  error due to inability of hypothesis h to fit f perfectly
         e.g. fitting quadratic f with a linear h
_variance_:  error due to fitting random noise in data
             e.g. we fit linear f with a linear h, yet h != f.

Model:  generate samples X_1 ... X_n from some distribution D
        values y_i = f(X_i) + epsilon_i
        fit hypothesis h to X, y
Now h is a random variable; i.e. its weights are random

Consider an arbitrary pt z in R^d (not necessarily a sample!) and
gamma = f(z) + epsilon.  [So z is *arbitrary*, whereas gamma is *random*.]
Note:  E[gamma] = f(z); Var(gamma) = Var(epsilon)
       [So the mean comes from f, and the variance comes from epsilon.]

Risk fn when loss is squared error:

  R(h) = E[L(h(z), gamma)]
         ^
         | we take expectation over possible training sets X, y
                                             and values of gamma
[Stop and take a close look at this.  Remember that the hypothesis h is
 a random variable.  We are taking a mean over the probability distribution of
 hypotheses.  That seems pretty weird if you've never seen it before.  But
 remember, the training data X and y come from probability distributions.  We
 use the training data to choose weights, so the weights that define h also
 come from some probability distribution.  It might be pretty hard to work out
 what that distribution is, but it exists.  This "E[.]" is integrating the loss
 over all possible values of the weights.]
       = E[(h(z) - gamma)^2]
       = E[h(z)^2] + E[gamma^2] - 2 E[gamma h(z)]
                                    [Observe that gamma & h(z) are independent]
       = Var(h(z)) + E[h(z)]^2 + Var(gamma) + E[gamma]^2 - 2 E[gamma] E[h(z)]
       = (E[h(z)] - E[gamma])^2 + Var(h(z)) + Var(gamma)
       = E[h(z) - f(z)]^2 + Var(h(z)) + Var(epsilon)
         \______________/   \_______/   \__________/
              bias^2        variance    _irreducible_error_
             of method      of method                            [Show bvn.pdf]

This is pointwise version.  Mean version:  let z ~ D be random variable; take
expectation of bias^2, variance over z.
[So you can decompose one test point's error into these three components, or
 you can decompose the error of the hypothesis over its entire range into three
 components, which tells you how big they'll be on a large test set.]

- Underfitting = too much bias
- Overfitting caused by too much variance
- Training error reflects bias but not variance; test error reflects both
  [which is why low training error can fool you when you've overfitted]
- For many distributions, variance -> 0 as n -> infinity
- If h can fit f exactly, for many distributions bias -> 0 as n -> infinity
- If h cannot fit f well, bias is large at "most" points
- Adding a good feature reduces bias; adding a bad feature rarely increases it
- Adding a feature usually increases variance
- Can't reduce irreducible error [hence its name]
- Noise in test set affects only Var(epsilon);
  noise in training set affects only bias & Var(h)
- For real-world data, f is rarely knowable (and noise model might be wrong)
- But we can test learning algs by choosing f & making synthetic data

[Show trade-off figures with spline, increasing degrees of freedom.]


Example:  Least-Squares Linear Reg.
-----------------------------------
For simplicity, assume no fictitious dimension.
[This implies that our linear regression fn has to be zero at the origin.]

Model:  f(z) = v^T z   (reality is linear)
        [So we could fit f perfectly with a linear h if not for the noise in
         the training set.]
        Let e be noise n-vector, e ~ N(0, sigma^2)
        Training values:  y = Xv + e
        [X & y are the inputs to linear regression.  We don't know v or e.]

Lin. reg. computes weights

       +     +                +
  w = X y = X (Xv + e) = v + X e
                             \_/ noise in weights

                            T     T        T  +       T  +
BIAS is E[h(z) - f(z)] = E[w z - v z] = E[z  X  e] = z  X  E[e] = 0
Warning:  This does not mean h(z) - f(z) is everywhere 0!
          Sometimes +ve, sometimes -ve, mean over training sets is 0.
          [Those deviations from the mean are captured in the variance.]
[Not all learning methods give you a bias of zero when a perfect fit is
 possible; here it's a benefit of the squared error loss fn.  With a different
 loss fn, we might have a nonzero bias even fitting a linear h to a linear f.]

                             T     T  +           T  +
VARIANCE is Var(h(z)) = Var(z v + z  X  e) = Var(z  X  e)

[This is the dot product of a vector z^T X^+ with an isotropic, normally
 distributed vector e.  The dot product reduces it to a one-dimensional
 Gaussian along the direction z^T X^+, so its variance is just the variance
 of the 1D Gaussian times the squared length of the vector z^T X^+.]

                             2 | T  +|2        2  T   T  -1  T    T  -1
                      = sigma  |z  X |  = sigma  z  (X X)   X X (X X)   z

                             2  T   T  -1
                      = sigma  z  (X X)   z

If we choose coordinate system so E[X] = 0,
then X^T X -> n Cov(D) as n -> infinity, so one can show that for z ~ D,

  Var(h(z)) ~ sigma^2 d / n

[Where d is the dimension--the number of features per sample.]

Takeaways:  Bias can be zero when hypothesis function can fit the real one!
              [Nice property of squared error loss fn.]
            Variance portion of RSS (overfitting) decreases as 1 / n (samples),
                                                  increases as d (features).

[I've used linear regression because it's a relatively simple example.  But
 this bias-variance trade-off applies to nearly all learning algorithms,
 including classification as well as regression.  Of course, for many learning
 algorithms the math gets a lot more complicated than this, if you can do it at
 all.]

** lec 13
RIDGE REGRESSION (aka Tikhonov regularization)
================
(1) + (A) + l_2 penalized mean loss (d).

  --------------------------------------------------
  |                               2              2 |
  | Find w that minimizes |Xw - y|  + lambda |w'|  | = J(w)
  --------------------------------------------------
where w' is w with component alpha replaced by 0.
X has fictitious dimension but we DON'T penalize alpha.

Adds a penalty term to encourage small |w'|--called _shrinkage_.  Why?
- Guarantees positive definite normal eq's; always unique solution.
  [When samples lie on a common hyperplane in feature space.]  E.g. when d > n.
  [Show figure comparing +ve semidef bowl w/+ve definite bowl (ridgequad.png)]
  [That was the original motivation, but the next has become more important...]
- Reduces overfitting by reducing variance.  Why?
  Imagine:  275238 x_1^2 - 543845 x_1 x_2 + 385832 x_2^2 is best fit for
            well-spaced samples all with |y| < 1.
            Small change in x  =>  big change in y!
  [Given that all the y values in the data are small, it's a sure sign of
   overfitting if tiny changes in x cause huge changes in y.]
  So we penalize large weights.
  [Show plot of isocontours for the two terms of J (ridgeterms.pdf)]
  [In this plot, beta is the least-squares solution.  The red ellipses are the
   isocontours of |Xw - y|^2.  The isocontours of |w'| are circles centered at
   the origin (blue).  The solution lies where a red isocontour just touches
   a blue isocontour tangentially.  As lambda increases, the solution will
   occur at a more outer red isocontour and a more inner blue isocontour.]

Setting grad J = 0 gives normal eq'ns
    T                    T
  (X X + lambda I') w = X y
where I' is identity matrix w/bottom right set to zero.
                                         T
Algorithm:  Solve for w.  Return h(z) = w z.

Increasing lambda  =>  more _regularization_; smaller |w'|
Given our data model y = Xv + e, where e is noise,
                               T   T              -1  T
variance of ridge reg. is Var(z  (X X + lambda I')   X e).
As lambda -> infinity, variance -> 0, but bias increases.
[Show graph of bias^2 & variance as lambda increases (ridgebiasvar.pdf)]
[So, as usual for the bias-variance trade-off, the test error as a function of
 lambda is a U-shaped curve, and we find the bottom by validation.]

lambda is a hyperparameter; tune by (cross-)validation.

Ideally, features should be "normalized" to have same variance.
Alternative:  use asymmetric penalty by replacing I' w/other diagonal matrix.

Bayesian justification for ridge reg.
-------------------------------------
Assign a prior probability on w':  a Gaussian centered at 0.

  Posterior prob ~ likelihood of w  x  prior P(w')        <= Gaussian PDF
  Maximize log posterior = ln likelihood + ln P(w')
                         = - const |Xw - y|^2 - const |w'|^2 - const

This method (using likelihood, but maximizing posterior) is called
_maximum_a_posteriori_ (MAP).

KERNELS
=======
Recall: with d input features, degree-p polynomials blow up to O(d^p) features.
[When d is large, this gets computationally intractible really fast.
 As I said in Lecture 4, if you have 100 features per sample and you want to
 use degree-4 predictor functions, then each lifted feature vector has a length
 on the order of 100 million.]
Today we use magic to use those features without computing them!

Observation:  In many learning algs,
- the weights can be written as a linear combo of input samples, &
- we can use inner products of Phi(x)'s only  =>  don't need to compute Phi(x)!

               T     n                         n
  Suppose w = X a = sum a  X    for some a in R .
                    i=1  i  i

Substitute this identity into alg. and optimize n _dual_weights_ a
(aka _dual_parameters_) instead of d+1 _primal_weights_ w.

Kernel Ridge Regression
-----------------------
_Center_ X and y so their means are zero; e.g. X_i <- X_i - mu_X
This lets us replace I' with I in normal equations:

    T                   T
  (X X + lambda I) w = X y

[When we center X and y, the expected value of the intercept w_{d+1} = alpha
 is zero.  The actual value won't usually be exactly zero, but it will be close
 enough that we won't do much harm by penalizing the intercept.]

            1      T     T        T                 1
  =>  w = ------ (X y - X X w) = X a    where a = ------ (y - X w)
          lambda                                  lambda        ^      T
                                                                | w = X a
This shows that w is a linear combo of samples.  To compute a:

                    T                 T            -1
  lambda a = (y - XX a)   =>   a = (XX  + lambda I)   y

a is the _dual_solution_; solves the _dual_form_ of ridge regression:
  ------------------------------------------------------
  |                          T      2            T  2 |
  | Find a that minimizes |XX a - y|  + lambda |X a|  |
  ------------------------------------------------------

Regression fn is

          T     T       n       T
  h(z) = w z = a X z = sum a  (X  z)          <= weighted sum of inner products
                       i=1  i   i
               T
Let k(x, z) = x z be _kernel_fn_.
[Later, we'll replace x and z with Phi(x) and Phi(z), and that's where the
 magic will happen.]

          T
Let K = XX  be n-by-n _kernel_matrix_.  Note K   = k(X , X ).
                                              ij      i   j
K is singular if n > d.  [And sometimes otherwise.]
In that case, no solution if lambda = 0.  [Then the penalty term is necessary.]

Summary of kernel ridge reg.:
  K   = k(X , X )   for all i, j             <= O(n^2 d) time
   ij      i   j
  Solve (K + lambda I) a = y    for a        <= O(n^3) time
  for each test pt z
            n
    h(z) = sum a  k(X , z)                   <= O(nd) time
           i=1  i    i

Does not use X directly!  Only k.            [This will become important soon.]

Dual:    solve n-by-n linear system
Primal:  solve d-by-d linear system
[So we prefer the dual form when d > n.  If we add new features like polynomial
 terms, this d goes up, but the d in the kernel running time doesn't.]

The Kernel Trick (aka _kernelization_)
----------------
[Here's the magic part.  We will see that we can compute a polynomial kernel
 that involves many monomial terms without actually computing those terms.]

                                                   T      p
The _polynomial_kernel_ of degree p is k(x, z) = (x z + 1)

            T      p         T
Theorem:  (x z + 1)  = Phi(x)  Phi(z) where Phi(x) contains every monomial in x
                                            of degree 0...p.
Example for d = 2, p = 2:

    T      2    2 2    2 2
  (x z + 1)  = x z  + x z  + 2 x z x z  + 2 x z  + 2 x z  + 1
                1 1    2 2      1 1 2 2      1 1      2 2

                 2    2     _          _        _
             = [x    x    \/2 x x    \/2 x    \/2 x    1]
                 1    2        1 2        1        2
                 2    2     _          _        _        T
               [z    z    \/2 z z    \/2 z    \/2 z    1]
                 1    2        1 2        1        2

                     T
             = Phi(x)  Phi(z)                 [This is how we're defining Phi.]

[Notice the factors of sqrt(2).  If you try a higher polynomial degree p,
 you'll see a wider variety of these constants.  We have no control of the
 constants used in Phi(x), but they don't matter very much, because the primal
 weights w will scale themselves to compensate.  Even though we aren't directly
 computing the primal weights...they still implicitly exist.]

Key win:  compute Phi(x)^T Phi(z) in O(d) time instead of O(d^p),
          even though Phi(x) has length O(d^p).

Kernel ridge regression replaces X_i with Phi(X_i):
                      T
  Let k(x, z) = Phi(x)  Phi(z)

[I think what we've done here is pretty mind-blowing:  we can now do polynomial
 regression with an exponentially long, high-order polynomial in less time than
 it would take even to compute the final polynomial.  The running time is
 sublinear, actually much smaller than linear, in the size of the Phi vectors.]

** lec 14
Kernel Perceptrons
------------------
Note:  Everywhere below, we can replace X  with Phi(X )
                                         i           i
Recall perceptron alg:
  while some y  X  . w < 0
              i  i
    w <- w + epsilon y  X
                      i  i
  for each test pt z
             T
    f(z) <- w z
                    T                 T
Kernelize with w = X a:   X  . w = (XX a)  = (Ka)
                           i             i       i
Dual perceptron alg:
                      T
  a = [y_1, 0, ..., 0]            [starting point is arbitrary, but can't be 0]
  K   = k(X , X )   for all i, j  <= O(n^2 d) time (kernel trick)
   ij      i   j
  while some y  (Ka)  < 0
              i     i
    a  <- a  + epsilon y          <= optimization:  can update Ka in O(n) time
     i     i            i
  for each test pt z
             n
    f(z) <- sum a  k(X , z)       <= O(nd) time
            j=1  j    j

[The d's above do not increase if we replace X_i with a kernelized Phi(X_i)!]
OR we can compute w = X^T a once in O(nd') time
   & evaluate test pts in O(d') time/pt, where d' is length of Phi(.)

Interpretation:  a_i reflects how many times we have added epsilon X
                 (or -epsilon X ) to w.                             i
                               i
Kernel Logistic Regression
--------------------------
[The stochastic gradient ascent step for logistic regression is just a small
 modification of the step for perceptrons.  However, remember that we're no
 longer looking for misclassified samples; instead, we apply the gradient
 ascent rule to weights in a stochastic, random order--or, alternatively, to
 all the weights at once.]

Stochastic gradient ascent step:
  a_i <- a_i + epsilon (y  - s((Ka) ))       [where s is the logistic function]
                         i         i
[Just like with perceptrons, every time you update one weight a_i, if you're
 clever you can update Ka in O(n) time so you don't have to compute it from
 scratch on the next iteration.]

Gradient ascent step:
  a <- a + epsilon (y - s(Ka))        <= applying s component-wise to vector Ka

for each test pt z:
             n
  h(z) <- s(sum a  k(X , z) )
            j=1  j    j

[or, if you're using logistic regression as a classifier and you don't care
 about the probability, you can skip the logistic fn and just compute the sum.]


The Gaussian Kernel
-------------------
[...But I think our next act is even more mind-blowing.  Since we can now do
 fast computations in spaces with exponentially large dimension, why don't we
 go all the way and do computations in infinite-dimensional space?]

_Gaussian_kernel_, aka _radial_basis_fn_kernel_:
There exists a Phi(x) such that
                            2
                     |x - z|
  k(x, z) = exp ( - --------- )
                    2 sigma^2

[In case you're curious, here's the feature vector that gives you this kernel,
 for the case where you have only one input feature per sample.]

e.g. for d = 1,         2                                  2
                       x                x                 x               T
  Phi(x) = exp ( - --------- ) [1, --------------, ----------------, ... ]
                   2 sigma^2       sigma sqrt(1!)  sigma^2 sqrt(2!)

[This is an infinite vector, and Phi(x)^T Phi(z) is a converging series.
 Nobody actually uses this value of Phi(x) directly, or even cares about it;
 they just use the kernel function k(.,.).]
[At this point, it's best *not* to think of points in a high-dimensional space.
 Instead, think of the kernel k as a measure of how similar or close together
 two samples are to each other.]
                                     n
Key observation:  hypothesis h(z) = sum a  k(X , z) is a linear combo of
                                    j=1  j    j
                  Gaussians centered at samples.
                  [The dual weights are the coefficients of the linear combo.]
                  [Think of the Gaussians as a basis for the hypothesis.]
                  [Show example hypothesis (gausskernel.pdf)]

Very popular in practice!  Why?
- Gives very smooth h
- Behaves somewhat like k-nearest neighbor, but smoother
- Oscillates less than polynomials (depending on sigma)
- k(x, z) can be interpreted as a "similarity measure".
  Gaussian is maximum when x = z, goes to 0 as distance increases.
- Samples "vote" for value at z, but closer samples get weighter vote.

[The "standard" kernel x . y assigns high value to vectors that point in
 roughly the same direction.  The Gaussian kernel assigns high value to points
 near each other.]

sigma trades off bias vs. variance:
  larger sigma -> wider Gaussians, smoother h -> more bias, less variance
Choose by (cross-)validation.

[Show decision boundary of SVM with Gauss kernel (gausskernelsvm.pdf)]

[By the way, there are many other kernels like this that are defined directly
 as kernel functions without worrying about Phi.  But not every function can be
 a kernel function.  A function is qualified only if it always generates
 a positive semidefinite kernel matrix, no matter what the samples are.]


SUBSET SELECTION
================
All features increase variance, but not all features reduce bias (much).
Idea:  Identify poorly predictive features, ignore them (weight zero).
       Less overfitting, lower test errors.
2nd motivation:  Inference.  Simpler models convey interpretable wisdom.
Useful in all classification & regression methods.
Sometimes it's hard:  Different features can partly encode same information.
                      Combinatorially hard to choose best feature subset.

Alg:  Best subset selection.  Try all 2^d - 1 nonempty subsets of features.
      Choose the best model by (cross-)validation.  Slow.

[Obviously, best subset selection isn't tractible if we have a lot of features.
 But it gives us an "ideal" algorithm to compare practical algorithms with.
 If d is large, there is no algorithm that's both guaranteed to find the best
 subset and that runs in acceptable time.  But heuristics often work well.]

Heuristic:  Forward stepwise selection.
Start with _null_model_ (0 features); repeatedly add best feature until
test errors start increasing (due to overfitting) instead of decreasing.
At each outer iteration, inner loop tries every feature and chooses the best
by cross-validation.  Requires training O(d^2) models instead of O(2^d).
Not perfect:  Won't find the best 2-feature model if neither of those features
              yields the best 1-feature model.

Heuristic:  Backward stepwise selection.
Start with all d features; repeatedly remove feature whose removal gives best
reduction in test errors.  Also trains O(d^2) models.
Additional heuristic:  Only try to remove features with small weights.
  Q:  small relative to what?
                                                                2   T   -1
  Recall:  variance of least-squ. regr. is proportional to sigma  (X  X)
                                       w_i
  _z-score_ of weight w  is z  = ---------------
                       i     i   sigma sqrt(v_j)
                                       T   -1
  where v_j is jth diagonal entry of (X  X)  .
  Small z-score hints "true" w_i could be zero.

[Forward stepwise selection is a better choice when you suspect only a few
 features will be good predictors.  Backward stepwise is better when you
 suspect most of the features will be necessary.  If you're lucky, you'll stop
 early.]


LASSO (Tibshirani, 1996)
=====
Regression w/regularization:  (1) + (A) + l_1 penalized mean loss (e).
"Least absolute shrinkage and selection operator"
[This is a regularized regression method similar to ridge regression, but it
 has the advantage that it often naturally sets some of the weights to zero.]

  --------------------------------------------------
  |                               2                |
  | Find w that minimizes |Xw - y|  + lambda |w'|  |
  |                                              1 |
  --------------------------------------------------
               n
where |w'|  = sum |w |                                  (Don't penalize alpha.)
          1   i=1   i
                                             2
Recall ridge regr.:  isosurfaces of |w'|  are hyperspheres.
                                             2
The isosurfaces of |w'|  are _cross-polytopes_.
                       1
The unit cross-polytope is the convex hull of all the positive and negative
unit coordinate vectors.

               ^
              /|\
             / | \
      2D:   <--+-->           3D:  [Draw 3D cross-polytope]
             \ | /
              \|/
               v
[You get other cross-polytope isosurfaces by scaling this.]
[Show plot of isocontours for the two objective fn terms (lassoridge.pdf)]
[Recall that beta is the least-squares solution, and the red ellipses are the
 isocontours of |Xw - y|^2.  The isocontours of |w'|_1 are diamonds centered at
 the origin (blue).  The solution lies where a red isocontour just touches
 a blue diamond.  What's interesting here is that in this example, the red
 isocontour touches just the tip of the diamond.  So one of the weights gets
 set to zero.  That's what we want to happen to weights that don't have enough
 influence.  This doesn't always happen--for instance, the red isosurface could
 touch a side of the diamond instead of a tip of the diamond.]
[When you go to higher dimensions, you might have several weights set to zero.
 For example, in 3D, if the red isosurface touches a sharp corner of the
 cross-polytope, two of the three weights get set to zero.  If it touches
 a sharp edge of the cross-polytope, one weight gets set to zero.  If it
 touches a flat side of the cross-polytope, no weight is zero.]
[Show plot of weights as a function of lambda (lassoweights.pdf)]
[This shows the weights for a typical linear regression problem with about 10
 variables.  You can see that as lambda increases, more and more of the weights
 become zero.  Only four of the weights are really useful for prediction;
 they're in color.  Statisticians used to choose lambda by looking at a chart
 like this and trying to eyeball a spot where there aren't too many predictors
 and the weights aren't changing too fast.  But nowadays they prefer
 cross-validation, as always.]

Sometimes sets some weights to zero, especially for large lambda.
Algs:  subgradient descent, least-angle regression (LARS), forward stagewise
[Lasso's objective function is not smooth, and that makes it tricky to
 optimize.  I'm not going to teach you an optimization method for Lasso.  If
 you need one, look up the last two of these names; LARS is built into the
 R Programming Language for statistics.]
[As with ridge regression, you should probably normalize the features first
 before applying this method.]

** lec 15
DECISION TREES
==============
Nonlinear method for classification and regression.

Uses tree w/two node types:
- internal nodes test feature values (usually just one) & branch accordingly
- leaf nodes specify class h(x)

                     -----------------
                     | Outlook (x_1) |                   sunny overc  rain
                     -----------------                  ------------------- 100
                      /      |      \                   |     |     |     | x
                     /       |       \                  | no  |     |     |  2
                 sunny   overcast    rain               |-----|     |     | 75
                   /         |         \                |     |     |     |
                  /          |          \               |     |     |check|
      ------------------     v      --------------      |     | yes | x   | 50
      | Humidity (x_2) |    yes     | Wind (x_3) |      | yes |     |  3  |
      ------------------     v      --------------      |     |     |     |
       /              \              /          \       |     |     |     | 25
      /                \            /            \      |     |     |     |
   > 75%             <= 75%      > 20           <= 20   |     |     |     |
    /                    \        /                \    ------------------- 0
   v                      v      v                  v             x
  no                     yes    no                 yes             1

- Cuts x-space into rectangular cells
- Works well with both categorical and quantitative features
- Interpretable result (inference)
- Decision boundary can be arbitrarily complicated
[Show comparison of SVM vs. dec. tree on 2 examples (treelinearcompare.pdf)]

Consider classification first.  Greedy, top-down learning heuristic:
[This algorithm is more or less obvious, and has been rediscovered many times.
 It's naturally recursive.  I'll show how it works for classification first;
 later I'll talk about how it works for regression.]

Let S subseteq {1, 2, ..., n} be list of sample indices.
Top-level call:  S = {1, 2, ..., n}.

GrowTree(S)
if (y_i = C for all i in S and some class C) then {
  return new leaf(C)                             [We say the leaves are "pure"]
} else {
  choose best splitting feature j and splitting point beta (*)
  S_l = {i : X_ij < beta}                           [Or you could use <= and >]
  S_r = {i : X_ij >= beta}
  return new node(j, beta, GrowTree(S_l), GrowTree(S_r))
}

(*) How to choose best split?
- Try all splits.              [All features, and all splits within a feature.]
- For a set S, let J(S) be the _cost_ of S.
- Choose the split that minimizes J(S_l) + J(S_r); or,
                                                   |S_l| J(S_l) + |S_r| J(S_r)
         the split that minimizes weighted average ---------------------------.
                                                          |S_l| + |S_r|

[Here, I'm using the vertical brackets |.| to denote set cardinality.]

How to choose cost J(S)?
[I'm going to start by suggesting a mediocre cost function, so you can see why
 it's mediocre.]
Idea 1 (bad):  Label S with the class C that labels the most samples in S.
               J(S) <- # of samples in S not in class C.

                           -----------------
                           | 6 C       5 D |  J(S) = 5
                           -----------------
                            /      x      \
                           /        1      \
                          /                 \
                         /                   \
                        /                     \
             -----------------           -----------------
  J(S ) = 1  | 4 C       1 D |           | 2 C       4 D |  J(S ) = 2
     l       -----------------           -----------------     r

Problem:  Sometimes we make "progress", yet J(S ) + J(S ) = J(S).
                                               l       r
                           -----------------
                           | 20 C     10 D |  J(S) = 10
                           -----------------
                            /      x      \
                           /        1      \
                          /                 \
                         /                   \
                        /                     \
             -----------------           -----------------
  J(S ) = 8  | 12 C      8 D |           | 8 C       2 D |  J(S ) = 2
     l       -----------------           -----------------     r
              /     x       \              /    x      \
             /       2       \            /      3      \
            /                 \          /               \
           /                   \        /                 \
          /                     \      /                   \
      --------              -------  -------            -------
      | 12 C |              | 8 D |  | 8 C |            | 2 D |
      --------              -------  -------            -------

[Notice that even though the first split doesn't reduce the total cost at all,
 we're still making progress, because after one more level of splits, we're
 done!]

Idea 2 (good):  Measure the _entropy_.       [An idea from information theory.]
Let Y be a random class variable, and suppose P(Y = C) = p_C.
The _surprise_ of Y being class C is S(Y = C) = - log_2 p_C.     [Nonnegative.]
- event w/prob. 1 gives us zero surprise.
- event w/prob. 0 gives us infinite surprise!
[In information theory, the surprise is equal to the expected number of bits of
 information we need to transmit which events happened to a recipient who knows
 the probabilities of the events.  Often this mean using fractional bits, which
 only makes sense when you're compiling lots of events into a single message;
 e.g. a sequence of biased coin flips.]

The _entropy_ of an index set S is the average surprise
                                     |{i in S : y_i = C}|    [The proportion of
 H(S) = - sum p  log  p , where p  = --------------------.    samples in S that
           C   C    2  C         C           |S|              are in class C.]

If all samples in S belong to same class?  H(S) = -1 log_2 1 = 0.
Half class C, half class D?  H(S) = -0.5 log_2 0.5 - 0.5 log_2 0.5 = 1.
n samples, all different classes?  H(S) = - log_2 (1/n) = log_2 n.

[Show graph of entropy for 2-class case (entropy.pdf)]

                                             |S_l| H(S_l) + |S_r| H(S_r)
Weighted avg entropy after split is H      = ---------------------------.
                                     after          |S_l| + |S_r|

Choose split that maximizes _information_gain_ H(S) - H_after.

                       -----------------           14    14   16    16
                       | 14 C     16 D |  H(S) = - -- lg -- - -- lg -- = 0.997
                       -----------------           30    30   30    30
                        /      x      \
                       /        1      \
                      /                 \
                     /                   \
                    /                     \
         -----------------           -----------------
         | 13 C      4 D |           | 1 C      12 D |
         -----------------           -----------------
            13    13   4     4                      1     1    12    12
  H(S ) = - -- lg -- - -- lg -- = 0.787   H(S ) = - -- lg -- - -- lg -- = 0.391
     l      17    17   17    17              r      13    13   13    13

H_after = 0.615;    info gain = 0.382

Info gain always positive *except* when one child is empty or
for all C, P(y_i = C | i in S_l) = P(y_i = C | i in S_r):

[Recall graph of entropy.]

            entropy                        % misclassified
          ^                                 ^
        1 |   ---                       0.5 |    ^
          |  -   -                          |   / \
          | /     \                         |  /   \
          |/       \                        | /     \
          |         |                       |/       \
          +----+----+> P                    +----+----+> P
          0   0.5   1                       0   0.5   1

        strictly concave                concave, not strict

[Suppose we pick two points on the entropy curve, then draw a line segment
 connecting them.  Because the entropy curve is strictly concave, the interior
 of the line segment is strictly below the curve.  Any point on that segment
 represents a weighted average of the two entropies for suitable weights.
 Whereas the point directly above that point represents the entropy if you
 unite the two sets into one.  So the union of two nonempty sets has greater
 entropy than the weighted average of the entropies of the two sets, unless the
 two sets both have exactly the same p.]
[For the graph on the right, on the other hand, if we draw a line segment
 connecting two points on the curve, the segment might lie entirely on the
 curve.  In that case, uniting the two sets into one, or splitting one into
 two, changes neither the total misclassified samples nor the % misclassified.]
[By the way, the entropy is not the only function that works well.  Many
 concave functions work fine, including the simple polynomial p(1-p).]

More on choosing a split:
- For binary feature x_i, children are x_i = 0 & x_i = 1.
- If x_i has 3+ discrete values, split depends on application.
  [Sometimes it makes sense to use multiway splits; sometimes binary splits.]
- If x_i is quantitative, sort samples in S by feature x_i; remove duplicates;
  try splitting between each pair of consecutive samples.
  [We can radix sort the samples in linear time, and if n is huge we should.]
  Clever Bit:  As you scan sorted list from left to right, you can update
               entropy in O(1) time per sample!
[This is important for obtaining a fast tree-building time.]
[Draw a row of X's and O's; show how we update the # of X's and # of O's in
 each of S_l and S_r as we scan from left to right.]

Algs & running times:
- Test point:  Walk down tree until leaf.  Return its label.
  Worst-case time is O(tree depth).
  For binary features, that's <= d.      [Quantitative features may go deeper.]
  Usually (not always) <= O(log n).
- Training:  For binary features, try O(d) splits at each node.
  For quantitative features,      try O(n'd) splits; n' = samples in node
  Either way                       => O(n'd) time at this node
  [Quantitative features are asymptotically just as fast as binary features
   because of our clever way of computing the entropy for each split.]
  Each sample participates in O(depth) nodes, costs O(d) time in each node.
  Running time <= O(nd depth).
  [As nd is the size of the design matrix X, and the depth is often
   logarithmic, this is a surprisingly reasonable running time.]

** lec 16
DECISION TREES (continued)
==============

Multivariate splits
-------------------
Find non-axis-aligned splits with other classification algs or by generating
them randomly.
[Show figure where regular decision tree needs many splits to approximate
 a diagonal linear decision boundary (multivariate.pdf).]
[This gives you all the power of classification algorithms such as SVMs,
 logistic regression, and Gaussian discriminant analysis; moreover, it can make
 them more powerful by making them hierarchical, so they're not limited to just
 one boundary.]
May gain better classifier at cost of worse interpretability.
Can limit # of features per split:  forward stepwise selection, Lasso.

Decision Tree Regression
------------------------
Creates a piecewise constant regression fn.
[Show decision tree regression figures (regresstree.pdf, regresstreefn.pdf).]

                        _ 2        _
Cost J(S) = sum   (y  - y) , where y is the mean y  for sample subset S.
           i in S   i                             i

[So if all the points in a node have the same y-value, then the cost is zero.]
[We choose the split that minimizes the weighted average of the costs of the
 children after the split.]

Stopping early
--------------
Why?
- Limit tree depth (for speed)
- Limit tree size (big data sets)
- Complete tree may overfit
- Given noise or overlapping distributions, purity of leaves is
  counterproductive; better to estimate posterior probs
[When you have overlapping class distributions, it's better to estimate
 a posterior probability than to always give a yes/no answer.  Refining
 the tree down to one sample point per leaf is absolutely guaranteed to
 overfit.  Whereas if you have enough points in each leaf, you can estimate
 posterior probabilities.]

[Show figure of multi-class leaves and posterior probability histogram for
 a leaf (leaf.pdf).]

How?  Select stopping condition(s):
- Next split doesn't reduce entropy/error enough (dangerous; pruning better)
- Most of node's points (e.g. > 95%) have same class    [to deal with outliers]
- Node contains few sample points (e.g. < 10)
- Node covers tiny volume
- Depth too great
- Use (cross-)validation to compare.
[The last is the slowest but most effective way to know when to stop:  use
 validation to decide whether splitting the node is a win or a loss.  But if
 your goal is to avoid overfitting, it's generally even more effective to grow
 the tree a little too large and then use validation to prune it back.  We'll
 talk about that next.]

Leaves with multiple points return
- a majority vote or class posterior probs (classification) or
- an average (regression).


Pruning
-------
Grow tree too large; greedily remove each split whose removal improves
cross-validation performance.  More reliable than stopping early.
[We have to do cross-validation once for each split that we're considering
 removing.  But you can do that pretty cheaply.  What you *don't* do is
 reclassify every sample point from scratch.  Instead, you keep track of which
 points in the validation set end up at which leaf.  When you are deciding
 whether to remove a split, you just look at the points in the two leaves
 you're thinking of removing, and see how they will be reclassified and how
 that will change the error rate.  You can do this very quickly.]
[The reason why pruning often works better than stopping early is because
 often a split that doesn't seem to make much progress is followed by a split
 that makes a lot of progress.  If you stop early, you'll never get there.]

[Show chart of tree size vs. error for baseball hitter data and final decision
 tree (prunehitters.pdf; prunedhitters.pdf).  Players' salaries:
 R1 = $165,174, R2 = $402,834, R3 = $845,346.]

ENSEMBLE LEARNING
=================
Decision trees are fast, simple, interpretable, easy to explain,
invariant under scaling/translation, robust to irrelevant features.

But not the best at prediction.  [Compared to previous methods we've seen.]
High variance.

[For example, suppose we take a training data set, split it into two halves,
 and train two decision trees, one on each half of the data.  It's not uncommon
 for the two trees to turn out very different.  In particular, if the two trees
 pick different features for the very first split at the top of the tree, then
 it's quite common for the trees to be completely different.  So decision trees
 tend to have high variance.]
[So let's think about how to fix this.  As an analogy, imagine that you are
 generating random numbers from some distribution.  If you generate just one
 number, it might have high variance.  But if you generate n numbers and take
 their average, then the variance of that average is n times smaller.  So you
 might ask yourself, can we reduce the variance of decision trees by taking an
 average answer of a bunch of decision trees?  Yes we can.]

[Show book cover "The Wisdom of Crowds" and Penelope the cow (wisdom.jpg,
 penelope.jpg).]

[A 1906 county fair in Plymouth, England had a contest to guess the weight of
 an ox.  A scientist named Francis Galton was there, and he did an experiment.
 He calculated the median of everyone's guesses.  The median guess was 1,207
 pounds, and the true weight was 1,198 pounds, so the error was less than 1%.
 Even the cattle experts present didn't estimate it that accurately.]
[NPR repeated the experiment in 2015 with a cow named Penelope whose photo they
 published online.  They got 17,000 guesses, and the average guess was 1,287
 pounds.  Penelope's actual weight was 1,355 pounds, so the crowd got it to
 within 5 percent.]
[The main idea is that sometimes the average opinion of a bunch of idiots is
 better than the opinion of one expert.  And so it is with learning algorithms.
 We call a learning algorithm a "weak learner" if it does better than guessing
 randomly.  And we combine a bunch of weak learners to get a strong one.]
[Incidentally, James Surowiecki, the author of the book, guessed 725 pounds for
 Penelope.  So he was off by 87%.  He's like a bad decision tree who wrote
 a book about how to average decision trees.]


We can take average of output of:
- Different learning algs
- Same learning alg on many training sets             [if we have tons of data]
- _Bagging_:  Same learning alg on many random subsamples of one training set.
- _Random_forests_:  Randomized decision trees on random subsamples.
[These last two are the most common ways to use averaging, because usually we
 don't have enough training data to use fresh data for every learner.]

[Averaging is not specific to decision trees; it can work with many different
 learning algorithms.  But it works particularly well with decision trees.]

Regression algs:  take median or mean output
Classification algs:  take majority vote OR average posterior probs

[Show averageaxis.mov]  [Here's a simple classifier that takes an average of
                         "stumps", trees of depth 1.  Observe how good the
                         posterior probabilities look.]]
[Show averageaxistree.mov]  [Here's a 4-class classifier with depth-2 trees.]

[The Netflix Prize was an open competition for the best collaborative filtering
 algorithm to predict user ratings for films, based on previous ratings.  It
 ran for three years and ended in 2009.  The winners used an extreme ensemble
 method that took an average of many different learning algorithms.  In fact, a
 couple of top teams combined into one team so they could combine their
 methods.  They said, "Let's average our models and split the money," and
 that's what happened.]

Use learners with low bias (e.g. deep decision trees).
High variance & some overfitting is okay.  Averaging reduces the variance!
[Each learner may overfit, but each overfits in its own unique way.]
Averaging sometimes reduces the bias & increases flexibility;
  e.g. creating nonlinear decision boundary from linear classifiers.
Hyperparameter settings usually different than 1 learner.
[Because averaging learners reduces their variance.  But averaging rarely
 reduces bias as much as it reduces variance, so you want to get the bias nice
 and small before you average.]
# of trees is another hyperparameter.

Bagging = _Bootstrap_AGGregatING_ (Leo Breiman, 1994)
-------
[Leo Breiman was a statistics professor right here at Berkeley.  He did his
 best work after he retired in 1993.  The bagging algorithm was published the
 following year, and then he went on to co-invent random forests as well.
 Unfortunately, he died in 2005.]
[Show photo (breiman.gif).]
[Bagging is a randomized method for creating many different learners from the
 same data set.  It works well with many different learning algorithms.
 One exception seems to be k-nearest neighbors; bagging mildly degrades it.]
Given n-point training sample, generate random subsample of size n' by sampling
*with*replacement*.  Some points chosen multiple times; some not chosen.

                                1 3 4 6 8 9
                              /             \
                              v             v
                       6 3 6 1 1 9       8 4 6 9 1 8

If n' = n, ~ 63.2% are chosen.  [On average; this fraction varies randomly.]
Build learner.  Points chosen j times have greater weight:
[If a point is chosen j times, we want to treat it the same way we would treat
 j different points all bunched up infinitesimally close together.]
- Decision trees:  j-time point has j x weight in entropy.
- SVMs:  j-time point incurs j x penalty to violate margin.
- Regression:  j-time point incurs j x loss.

Repeat until T learners.  Metalearner takes test point, feeds it into
all T learners, returns average/majority output.

Random Forests
--------------
Random sampling isn't random enough!
[With bagging, often the decision trees look very similar.  Why is that?]
One really strong predictor -> same feature split at top of every tree.
[For example, if you're building decision trees to identify spam, the first
 split might always be "viagra".  Random sampling doesn't change that.  If the
 trees are very similar, then taking their average doesn't reduce the variance
 much.]
Idea:  At each split, take random sample of m features (out of d).
       Choose best split from m features.
       [We're not allowed to split on the other d - m features!]
       Different random sample for each split.
       m ~ sqrt(d) works well for classification; m ~ d/3 for regression.
       [So if you have 100-dimensional feature space, you randomly choose 10
        features and pick the one of those 10 that gives the best split.
        But m is a hyperparameter, and you might get better results by tuning
        it for your particular application.]
       Smaller m -> more randomness, less tree correlation, more bias
[One reason this works is if there's a really strong predictor, only a fraction
 of the trees can choose that predictor as the first split.  That fraction is
 m/d.  So the split tends to "decorrelate" the trees.  And that means that when
 you take the average of the trees, you'll have less variance.]
[You have to be careful, though, because you don't want to dumb down the trees
 too much in your quest for decorrelation.  Averaging works best when you have
 very strong learners that are also diverse.  But it's hard to create a lot of
 learners that are very different yet all very smart.  The Netflix Prize
 winners did it, but it was a huge amount of work.]

Sometimes test error reduction up to 100s or even 1,000s of decision trees!
Disadvantage:  loses interpretability/inference.
[But the compensation is it's more accurate than a single decision tree.]

Variation:  generate m random multivariate splits (oblique lines, quadrics);
choose best split.
[You have to be a bit clever about how you generate random decision boundaries;
 I'm not going to discuss that today.]

[Show treesidesdeep.mov]  [Lots of good-enough conic random decision trees.]
[Show averageline.mov]
[Show averageconic.mov]
[Show square.mov]  [Depth 2; look how good the posterior probabilities look.]
[Show squaresmall.mov]  [Depth 2; see the uncertainty away from the center.]
[Show spiral2.mov]  [Doesn't look like a decision tree at all, does it?]
[Show overlapdepth14.mov]  [Overlapping classes.  This example overfits!]
[Show overlapdepth5.mov]  [Better fit.]

[Show spiral at depths 4 & 12, axis/line/conic (500.pdf).]
[Show spiral w/axis-aligned tree at depths 4 & 12, with 1, 5, or 50 random
 multivariate splits (randomness.pdf).]

** lec 17
NEURAL NETWORKS
===============
Can do both classification and regression.

[Ties together several ideas from the course:  perceptrons, logistic
 regression, ensembles of learners, and stochastic gradient descent.
 It also ties in the idea of lifting samples to a higher-dimensional feature
 space, but with a new twist:  neural nets can learn features themselves.]

[I want to begin by reminding you of the story I told you at the beginning of
 the semester, about Frank Rosenblatt's invention of perceptrons in 1957.
 Remember that he held a press conference where he predicted that perceptrons
 would be "the embryo of an electronic computer that [the Navy] expects will be
 able to walk, talk, see, write, reproduce itself and be conscious of its
 existence."]
[Perceptron research continued, but something monumental happened in 1969.
 Marvin Minsky, one of the founding fathers of AI, and Seymour Papert published
 a book called "Perceptrons".  Sounds good, right?  Well, part of the book was
 devoted to things perceptrons can't do.  And one of those things is XOR.]

                                      x
                                       1

                           XOR  |  0  |  1
                         -------+-----+-----
                            0   |  0     1
                     x   -------+
                      2     1   |  1     0

[Think of the four outputs here as sample points in two-dimensional space.
 Two of them are in class 1, and two of them are in class 0.  We want to find
 a linear classifier that separates the 1's from the 0's.  Can we do it?  No.]
[So Minsky and Papert were basically saying, "Frank.  You're telling us this
 machine is going to be conscious of its own existence but it can't do XOR?"]
[The book had a devastating effect on the field.  After its publication, almost
 no research was done on neural net-like ideas for a decade, a time we now call
 the "AI Winter".  Shortly after the book was published, Frank Rosenblatt died.
 Officially, he died in a boating accident.  But we all know he died of
 a broken heart.]

[One thing I don't understand is why nobody at the time pointed out some almost
 obvious ways to get around this problem.  Here's the easiest.]

If you add one new quadratic feature, x_1 x_2, XOR is linearly separable in 3D.

                               0----------1
                              /|         /|
                             / |        / |
                            +----------+  |
                            |  |       |  |
                            |  |       |  |
                            |  1-------|--+
                            | /        | /
                            |/         |/
                            +----------0

[Now we can find a plane that cuts through the cube obliquely and separates the
 0's from the 1's.]

[However, there's an even more powerful way to do XOR.  The idea is to design
 linear classifiers whose output is the input to other linear classifers.
 That way, you should be able to emulate arbitrarily logical circuits.
 Suppose I put together some linear predictor functions like this.]

                 ------------------
    x  --------->|  linear combo  |--
     1   ------->------------------  \    ------------------
       \/                             --->|  linear combo  |--> z
       /\                             --->------------------
      /  ------->------------------  /
    x ---------->|  linear combo  |--
     2           ------------------

[If I interpret the output as 1 if z is positive or 0 is z is negative, can
 I do XOR with this?]

A linear combo of a linear combo is a linear combo...only works for linearly
separable samples.

[We need one more idea to make neural nets.  We need to add some sort of
 nonlinearity between the linear combinations.  Let's call these boxes that
 compute linear combinations "neurons".  If a neuron runs the linear
 combination it computes through some nonlinear function before sending it on
 to other neurons, then the neurons can act somewhat like logic gates.  The
 nonlinearity could be as simple as clamping the output so it can't go below
 zero.  And that's actually used in practice sometimes.]
[The most popular traditional choice has been to use the logistic function.
 The logistic function can't go below zero or above one, which is nice because
 it can't ever get huge and oversaturate the other neurons it's sending
 information to.  The logistic function is also smooth, which means it has
 well-defined gradients we can use in gradient descent.]
[With logistic functions between the linear combinations, here's a two-level
 perceptron that computes the XOR function.  Note that the logistic function
 at the output is optional; we could just take the sign of the output instead.]

                         NAND
                 --------------------\    v
    x  --------->| s(30 - 20x - 20y)  )O--             AND
         ------->--------------------/    \    -------------------\
       \/                                  --->| s(20v + 20w - 30) )-> x XOR y
       /\                                  --->-------------------/
      /  ------>\--------------------\    /
    y ---------> ) s(20x + 20y - 10)  >---
                /--------------------/    w
                          OR

Network with 1 Hidden Layer
---------------------------
[Note:  see the hand-drawn version of this section with an illustration.]

Input layer:  x , ..., x  ; x    = 1
               1        d    d+1

Hidden units:  h , ..., h  ; h    = 1
                1        m    m+1

Output layer:  a , ..., z
                1        k

[We might have more than one output so that we can build multiple classifiers
 that share hidden units.  One of the interesting advantages of neural nets is
 that if you train multiple classifiers simultaneously, sometimes some of them
 come out better because they can take advantage of particularly useful hidden
 units that first emerged to support one of the other classifiers.]

Layer 1 weights:  m x (d+1) matrix V            V_i is row i
Layer 2 weights:  k x (m+1) matrix W            W_i is row i

                                   1
Recall logistic fn s(gamma) = -----------.  Other nonlinear fns can be used.
                                   -gamma
                              1 + e

                     -        -
                     | s(v_1) |
                     |        |
For vector v, s(v) = | s(v_2) |.       [We apply s to a vector component-wise.]
                     |   .    |
                     |   .    |
                     -   .    -

[At this point, see hand-drawn page for depiction of neural net with one
 hidden layer.]

          3
  h  = s(sum V   x )               In short, h = s(Vx)
   i     i=1  ij  j

  z = s(Wh) = s(W s (Vx))
                   1
                   ^
                  add a 1 to end of vector

[We can add more hidden layers, and for some tasks it's common to have up to
 five hidden layers.  There are many variations you can experiment with--for
 instance, you can have connections that go forward more than one layer.]

Training
--------
Usually stochastic or batch gradient descent.

Pick loss fn L(z, y)              e.g. L(z, y) = |z - y|^2
               ^  ^
     predictions  true values (could be a vector)

                   n
Cost fn is J(h) = sum L(h(X ), Y )
                  i=1      i    i

[I'm using a capital Y here because now Y is a matrix with one row for each
 sample point and one column for each output of the neural net.  Often there
 will be just one output, but many neural net applications have more.]
Usually there are many local minima!
[The cost function for a neural net is, generally, not even close to convex.
 For that reason, it's possible to wind up in a bad minimum.  We'll talk later
 about some clever ways to coax neural nets into better minima.]
[Now let me ask you this.  Suppose we start by setting all the weights to zero,
 and then we do gradient descent on the weights.  What will go wrong?]
[The gradient descent has no way to break symmetry, you can get stuck in
 a situation where all the weights in each layer have the same value, and they
 have no way to become different from each other.  To avoid this problem, and
 in the hopes of finding a better minimum, we start with random weights.]

Rewrite all the weights in V & W as a vector w.  Batch gradient descent:

  w <- vector of random weights
  repeat
    w <- w - epsilon grad J(w)

[It's important to make sure the random weights aren't too big, because if
 a unit's output gets too close to zero or one, it can get "stuck", meaning
 that it always has roughly the same output value regardless of the input.
 Stuck units tend to stay stuck because in that operating range, the gradient
 s'(.) of the logistic function is close to zero.]
[We can instead use stochastic gradient descent, which means we use the
 gradient of one sample point's loss function at each step.  Typically, we
 shuffle the points in a random order, or just pick one randomly at each step.]
[The hard part of this algorithm is computing the gradient.  If you simply
 derive one derivative for each weight, you'll find that it takes time linear
 in the size of the neural network to compute a derivative for one weight in
 the first layer.  Multiply that by the number of weights.  We're going to
 spend some time learning to improve the running time to linear in the number
 of weights.]

Naive gradient computation:  O(units x edges) time
Backpropagation:  O(edges) time

Computing Gradients for Arithmetic Expressions
----------------------------------------------
[See the two hand-drawn pages.]

The Backpropagation Alg
-----------------------
[See the hand-drawn page.]

** lec 18
NEURONS
=======
[The field of artificial intelligence started with some wrong premises.
 The early AI researchers attacked problems like chess and theory proving,
 because they thought those exemplified the essence of intelligence.  They
 didn't pay much attention at first to problems like vision and speech
 understanding.  Any four-year-old can do those things, and so researchers
 underestimated their difficulty.  Today, we know better.  Computers can
 effortlessly beat four-year-olds at chess, but they still can't play with toys
 nearly as well.  We've come to realize that rule-based symbol manipulation is
 not the sole defining mark of intelligence.  Even rats do computations that
 we're hard pressed to match with our computers.  We've also come to realize
 that these are different classes of problems that require very different
 styles of computation.  Brains and computers have very different strengths and
 weaknesses, which reflect these different computing styles.]
[Neural networks are partly inspired by the workings of actual brains.  Let's
 take a look at a few things we know about biological neurons, and contrast
 them with both neural nets and traditional computation.]

- CPUs:  largely sequential, nanosecond gates, fragile if gate fails
  superior for 234 x 718, logical rules, perfect key-based memory
- Brains:  very parallel, millisecond neurons, fault-tolerant
  [Neurons are continually dying.  You've probably lost a few since this
   lecture started.  But you probably didn't notice.  And that's interesting,
   because it points out that our memories are stored in our brains in
   a diffuse representation.  There is no one neuron whose death will make you
   forget that 2 + 2 = 4.  Artificial neural nets often share that resilience.
   Brains and neural nets seem to superpose memories on top of each other, all
   stored together in the same weights, sort of like a hologram.]
  superior for vision, speech, associative memory
  [By "associative memory", I mean noticing connections between things.
   One thing our brains are very good at is retrieving a pattern if we specify
   only a portion of the pattern.]

[It's impressive that even though a neuron needs a few milliseconds to transmit
 information to the next neurons downstream, we can perform very complex tasks
 like interpreting a visual scene in a tenth of a second.  This is possible
 because neurons are parallel, but also because of their computation style.]
[Neural nets try to emulate the parallel, associative thinking style of brains,
 and they are among the best techniques we have for many fuzzy problems,
 including some problems in vision and speech.  Not coincidentally, neural nets
 are also inferior at many traditional computer tasks such as multiplying
 numbers with lots of digits or compiling source code.]

[Show figure of neurons (neurons.pdf).]

- _Neuron_:  A cell in brain/nervous system for thinking/communication
- _Action_potential_ or _spike_:  An electrochemical impulse _fired_ by
  a neuron to communicate w/other neurons
- _Axon_:  The limb(s) along which the action potential propagates; "output"
  [Most axons branch out eventually, sometimes profusely near their ends.]
  [It turns out that giant squids have a very large axon they use for fast
   water jet propulsion.  The mathematics of action potentials was first
   characterized in these giant squid axons, and that work won a Nobel Prize
   in 1963.]
- _Dendrite_:  Smaller limbs by which neuron receives info; "input"
- _Synapse_:  Connection from one neuron's axon to another's dendrite
  [Some synapses connect axons to muscles or glands.]
- _Neurotransmitter_:  Chemical released by axon terminal to stimulate dendrite

[When an action potential reaches an axon terminal, it causes tiny containers
 of neurotransmitter, called _vesicles_, to empty their contents into the space
 where the axon terminal meets another neuron's dendrite.  That space is called
 the _synaptic_cleft_.  The neurotransmitters bind to receptors on the dendrite
 and influence the next neuron's body voltage.  This sounds incredibly slow,
 but it all happens in 1 to 5 milliseconds.]

You have about 10^11 neurons, each with about 10^4 synapses.
[Maybe 10^5 synapses after you pass CS 189.]

Analogies:  [between artificial neural networks and brains]
- Output of unit <-> firing rate of neuron
  [An action potential is "all or nothing"--they all have the same shape and
   size.  The output of a neuron is not a fixed voltage like the output of
   a transistor.  The output of a neuron is the frequency at which it fires.
   Some neurons can fire at nearly 1,000 times a second, which you might think
   of as a strong "1" output.  Conversely, some types of neurons can go for
   minutes without firing.  But some types of neurons never stop firing, and
   for those you might interpret a firing rate of 10 times per second as "0".]
- Weight of connection <-> synapse strength
- Positive weight <-> excitatory neurotransmitter (e.g. glutamine)
- Negative weight <-> inhibitory neurotransmitter (e.g. GABA, glycine)
  [Gamma aminobutyric acid.]
  [A typical neuron is either excitatory at all its axon terminals, or
   inhibitory at all its terminals.  It can't switch from one to the other.
   Artificial neural nets have an advantage here.]
- Linear combo of inputs <-> _summation_
  [A neuron fires when the sum of its inputs, integrated over time, reaches
   a high enough voltage.  However, the neuron body voltage also decays
   slowly with time, so if the action potentials coming in are slow enough, the
   neuron might not fire at all.]
- Logistic/sigmoid fn <-> firing rate saturation
  [A neuron can't fire more than 1,000 times a second, nor less than zero times
   a second.  This limits its ability to be the sole determinant of whether
   downstream neurons fire.  We accomplish the same thing with the sigmoid fn.]
- Weight change/learning <-> _synaptic_plasticity_
  Hebb's rule (1949):  "Cells that fire together, wire together."
  [This doesn't mean that the cells have to fire at exactly the same time.
   But if one cell's firing tends to make another cell fire more often, their
   excitatory synaptic connection tends to grow stronger.  There's a reverse
   rule for inhibitory connections.  And there are ways for neurons that aren't
   even connected to grow connections.]
  [There are simple computer learning algorithms based on Hebb's rule.
   It can work, but it's generally not nearly as fast or effective as
   backpropagation.]
[Backpropagation is one part of artificial neural networks for which there is
 no analogy in the brain.  Brains definitely do not do backpropagation.]

[Show Geoff Hinton Hebbian learning slides (hebbian.pdf).]

[But this two-layer network is not flexible enough to do digit recognition
 well, especially when you have multiple writers with different handwriting.
 You can do much better with a three-layer network and backpropagation.]

[The brain is very modular.]  [Show figure of brain lobes (brain.png).
- The part of our brain we think of as most characteristically human is the
  cerebral cortex, the seat of self-awareness, language, and abstract thinking.
But the brain has a lot of other parts that take the load off the cortex.
- Our brain stem regulates functions like heartbeat, breathing, and sleep.
- Our cerebellum governs fine coordination of motor skills.  When we talk about
  "muscle memory", much of that is in the cerebellum, and it saves us from
  having to consciously think about how to walk or talk or brush our teeth, so
  the cortex can focus on where to walk and what to say and checking our phone.
- Our limbic system is the center of emotion and motivation, and as such, it
  makes a lot of the big decisions.  I sometimes think that 90% of the job of
  our cerebral cortex is to rationalize decisions that have alread been made by
  the limbic system.  "Oh yeah, I made a careful, reasoned, logical decision to
  eat that fourth pint of ice cream."
- Our visual cortex performs a lot of processing on the input from your eyes
  to change it into a more useful form.  Neuroscientists and computer
  scientists are particularly interested in the visual cortex for several
  reasons.  Vision is an important problem for computers.  The visual cortex is
  one of the easier parts of the brain to study in lab animals.  The visual
  cortex is largely a feedforward network with few neurons going backward, so
  it's easier for us to train computers to behave like the visual cortex.]

[Although the brain has lots of specialized modules, one thing that's
 interesting about the cerebral cortex is that it seems to be made of
 general-purpose neural tissue that looks more or less the same everywhere, at
 least before it's trained.  If you experience damage to part of the cortex
 early enough in life, while your brain is still growing, the functions will
 just relocate to a different part of the cortex, and you'll probably never
 notice the difference.]

[As computer scientists, our primary motivation for studying neurology is to
 try to get clues about how we can get computers to do tasks that humans are
 good at.  But neurologists and psychologists have also been part of the study
 of neural nets from the very beginning.  Their motivations are scientific:
 they're curious how humans think, and why we can do what we can do.]


NEURAL NET VARIATIONS
=====================
[I want to show you a few basic variations on the standard neural network
 I showed you last class, and how some of them change backpropagation.]

Regression:  usually omit sigmoid fn from output unit(s).
[If you make that change, the gradient changes too, and you have to derive it
 for backprop.  The gradient gets simpler, so I'll leave it as an exercise.]

Classification:
- Logistic loss fn (aka _cross-entropy_) often preferred to squared error.
  L(z, y) = - sum (y  ln z  + (1 - y ) ln (1 - z ))
    ^  ^       i    i     i         i           i
    |  |
    |  true values  \ vectors
   prediction       /
- For 2 classes, use one sigmoid output; for k >= 3 classes, use _softmax_fn_.
  Let t = Wh be k-vector of linear combos in final layer.

                                t_j        d z                   d z
                               e              j                     j
  Softmax output is z (t) = --------- .    ---- = z  (1 - z )    ---- = - z  z
                     j       k    t_i      d t     j       j     d t       i  j
                            sum  e            j                     i
                            i=1                                       i != j

  [Each z_j is in the range (0, 1), and their sum is 1.]

[See my hand-drawn derivation of the backprop equations for the softmax output
 and logistic loss function.]

[Next I'm going to talk about a bunch of heuristics that make gradient descent
 faster, or make it find better local minima, or prevent it from overfitting.
 I suggest implementing vanilla stochastic backprop first, and experimenting
 with the other heuristics only after you get that working.]


Unit Saturation
---------------
Problem:  When unit output s is close to 0 or 1 for all training points,
s' = s (1 - s) ~ 0, so gradient descent changes s very slowly.  Unit is
"stuck".  Slow training & bad network.
[Show sigmoid function (logistic.pdf); show flat spots & linear region.]
[Wikipedia calls this the "vanishing gradient problem."]
[The more layers your network has, the more problematic this problem becomes.
 Most of the early attempts to train deep, many-layered neural nets failed.]

Mitigation:                                 [None of these are complete cures.]
(1)  Set target values to 0.15 & 0.85 instead of 0 & 1.
     [Recall that the sigmoid function can never be 0 or 1; it can only come
      close.  Relaxing the target values helps prevent the output units from
      getting saturated.  The numbers 0.15 and 0.85 are reasonable because
      the sigmoid function achieves its greatest curvature when its output is
      near 0.21 or 0.79.  But experiment to find the best values.]
     [This helps to avoid stuck output units, but not stuck hidden units.]

(2)  Modify backprop to add small constant (typically ~0.1) to s'.
     [This hacks the gradient so a unit can't get stuck.  We're not doing
      *steepest* descent any more, because we're not using the real gradient.
      But often we're finding a better descent direction that will get us to
      a minimum faster.]

(3)  Initial weight of edge into unit with fan-in eta:
     random with mean zero, std. dev. sqrt(eta).
     [The bigger the fan-in of a unit, the easier it is to saturate it.  So we
      choose smaller random initial weights for gates with bigger fan-in.]

(4)  Replace sigmoid with ReLUs:  _rectified_linear_units_.
     _ramp_fn_ aka _hinge_fn_:  s(gamma) = max { 0, gamma }
                                             /   1     gamma >= 0,
                                s'(gamma) = <
                                             \   0     gamma < 0.
     [The derivative is not defined at zero, but we can just make one up.]
     [Obviously, the gradient can be zero, so you might wonder if ReLUs can get
      stuck too.  Fortunately, it's rare for a ReLU's gradient to be zero for
      *all* the training data; it's usually zero for just some sample points.
      But yes, ReLUs sometimes get stuck too; just not as often as sigmoids.]
     Popular for many-layer networks with large training sets.
     [One nice thing about ramp functions is that they and their gradients are
      very fast to compute.  Computers compute exponentials slowly.]
     [Even though ReLUs are linear in half their range, they're still nonlinear
      enough to easily compute functions like XOR.]

[Note that option (4) makes the first three options irrelevant.]

Heuristics for Avoiding Bad Local Minima
----------------------------------------
- (1) or (4) above.

- Stochastic gradient descent.  A local minimum for batch descent is not
  a minimum for one typical training point.
  [The idea is that instead of trying to optimize one risk function, we descend
   on one example's loss function and then we descend on another example's loss
   function, and every loss function has different local minima.  It looks like
   a random walk or Brownian motion, and that random noise gets you out of
   shallow local minima.]
  [Show example of stochastic gradient descent (stochasticnnet.pdf).]

- Momentum.  Gradient descent changes "velocity" slowly.
  Carries us right through shallow local minima to deeper ones.

  Delta w <- - epsilon grad w
  repeat
    w <- w + Delta w
    Delta w <- - epsilon grad w + beta Delta w

  Good for both stochastic & batch descent.  Choose hyperparameter beta < 1.
  [Think of Delta w as the velocity.  The hyperparameter beta specifies how
   strongly momentum persists from iteration to iteration.]
  [I've seen conflicting advice on beta.  Some researchers set it to 0.9;
   some set it close to zero.]
  [If beta is large, epsilon tends to be smaller to compensate, in which case
   you might want to change the first line so the initial velocity is larger.]

** lec 19
Heuristics for Avoiding Bad Local Minima (continued)
----------------------------------------
- Train several nets; pick best.
  [If you train 10 neural nets on the same data, but each starting with
   different random weights, it's unlikely that any two of them will end up
   the same.  Some might fall into bad local minima, but probably not all.]

- _Layerwise_pretraining_.
  [We train one layer of the neural net at a time, starting from the input
   layer.  The goal is to get the first layer to develop useful features, then
   for the second layer to develop even more useful features, and so on.
   This is a complicated topic on the forefront of research that I don't have
   time to do justice to.  If you're interested, do a search.]

Heuristics for Faster Training
------------------------------
[One of the biggest disadvantages of neural nets is that they take a long, long
 time to train compared to most other classification methods we've studied.
 Here are some ideas for how to speed them up.  Unfortunately, you often have
 to experiment with techniques and parameters for a while to figure out which
 ones will help with your particular application.]

- (1), (2), (3), (4) above.

- Stochastic gradient descent faster than batch on large, redundant data sets.
  [For example, if you have many different examples of the number "9", they
   contain some redundant information, and stochastic gradient descent learns
   the redundant information quickly.]
  [Show 3-weight neural net and the convergence for batch and stochastic
   gradient descent (nnet2d.pdf, batchnnet.pdf, stochasticnnet.pdf).]
  One _epoch_ presents every training example once.  Training usually takes
  many epochs, but if data is huge it can take less than one.

- _Normalizing_ the data:
--- First _center_ each feature so mean is zero.
--- Then scale each feature so variance is constant (~1 is good).
  [The first step seems to make it easier for hidden units to get into a good
   operating region of the sigmoid.  The second step makes the objective
   function better conditioned, so gradient descent converges faster.]
  [Show example of slow convergence of steepest descent on an objective
   function with an ill-conditioned Hessian (illcondition.jpg).]
  [Show 2D example of data normalization (normalize.jpg).]

- Centering the hidden units helps too.
  Replace sigmoids with 2 s(gamma) - 1 or with tanh gamma.
  [These functions range from -1 to 1 instead of from 0 to 1.]
  [If you do this, don't forget that you also need to change s' in backprop.
   Also, good output target values change to roughly -0.7 and 0.7.]

- Use different learning rate for each layer of weights:
  earlier layers have smaller gradients, need larger learning rate.
  [Show second derivatives at different layers (curvaturelayers.pdf).]

- _Emphasizing_schemes_:
  [Networks learn fastest from the most unexpected examples.  They learn
   slowest from the most redundant examples.  So we try to emphasize the
   uncommon examples.]
--- Shuffle so successive examples are never/rarely same class.
--- Present examples from rare classes more often, or w/bigger epsilon.
--- Warning:  can backfire on bad outliers.

- Second-order optimization
--- No Newton's method; Hessian too expensive.
--- Nonlinear conjugate gradient; for small nets + small data + regression.
    Batch descent only!  ->  Too slow with redundant data.
--- Stochastic Levenberg Marquardt; approximates a diagonal Hessian.
    [The authors claim convergence is typically three times faster than
     well-tuned stochastic gradient descent.  The algorithm is complicated and
     I don't have time to cover it.]

Heuristics to Avoid Overfitting
-------------------------------
- Ensemble of neural nets.  Random initial weights; bagging.
  [We saw how well ensemble learning works for decision trees.
   It works really well for neural nets too.  The combination of random initial
   weights and bagging helps ensure that each neural net comes out different,
   even though they're trained on the same data.  Obviously, this is slow.]

- L_2 regularization, aka _weight_decay_.
  Add lambda |w|^2 to the cost/loss fn, where w is vector of all weights.
  [Thus w includes all the weights in matrices V and W, rewritten as a vector.]
  [We do this for the same reason we do it in ridge regression:
   penalizing large weights reduces overfitting by reducing the variance.]
  Effect:  -dJ/dw_i has extra term -2 lambda w_i
           Weight decays by factor 1 - 2 lambda if not reinforced by training.
  [Show examples of 2D classification without and with weight decay
   (weightdecayoff.pdf, weightdecayon.pdf).  "softmax + logistic loss".
   Observe that second example comes close to the Bayes rule.]
  [One of the tricky parts of neural nets is deciding how many hidden units
   there should be.  If there's too few, you can't learn very well, but if
   there's too many, they tend to overfit.  L_2 regularization and weight decay
   make it safer to have too many hidden units, so it's less critical to find
   just the right number.]

- _Dropout_ emulates an ensemble in one network.  (Faster training too.)
  [Show figure of neural net with dropout (dropout.pdf).]
  [During training, we temporarily disable a random subset of the units, along
   with all the edges in and out of those units.  It seems to work well to
   disable each hidden unit with probability 0.5, and to disable input units
   with a smaller probability.  We do stochastic gradient descent and we
   frequently change which random subset of units is disabled.  The authors
   claim that their method generalizes better than L_2 regularization.
   It gives some of the advantages of an ensemble, but it's faster to train.]


CONVOLUTIONAL NEURAL NETWORKS (ConvNets)
=============================
[Convolutional neural nets have caused a big resurgence of interest in neural
 nets in the last few years.  Often you'll hear the buzzword _deep_learning_,
 which refers to neural nets with many layers.]

Vision:  inputs are large images.  200 x 200 image = 40,000 pixels.
If we connect them all to 40,000 hidden units -> 1.6 billion connections.
Neural nets are often _overparametrized_:  too many weights, too little data.
[As a rule of thumb, you should have more data than you have weights to
 estimate.  If you don't follow that rule, you usually overfit very very badly.
 With images, it's impossible to get enough data to correctly train billions of
 weights.  We could shrink the images, but then we're throwing away useful
 data.  Another problem with having billions of weights is that the network
 becomes very slow to train or even to use.]

[Researchers have addressed these problems by taking inspiration from the
 neurology of the visual system.  Remember that early in the semester, I told
 you that you can get better performance on the handwriting recognition task
 by using edge detectors.  Edge detectors have two interesting properties.
 First, each edge detector looks at just one small part of the image.  Second,
 the edge detection computation is the same no matter which part of the image
 you apply it to.  So let's apply these two properties to neural net design.]

ConvNet ideas:
(1)  Local connectivity:  A hidden unit (in early layer) connects only to small
     patch of units in previous layer.
     [This improves the overparametrization problem, and speeds up the network
      considerably.]
(2)  Shared weights:  Groups of hidden units share same set of input weights,
     called a _mask_ aka _filter_ aka _kernel_.  We learn several masks.
     [Each mask operates on every patch of image.]
     Masks x patches = hidden units (in first hidden layer).
     If one patch learns to detect edges, *every* patch has an edge detector.
     ConvNets automatically exploit repeated structure in images, audio.
     _Convolution_:  the same linear transformation applied to different parts
     of the input by shifting.
     [Shared weights improve the overparametrization problem even more, because
      shared weights means fewer weights.  It's a sort of regularization.
     [But shared weights have another big advantage.  Suppose that gradient
      descent starts to develop an edge detector.  That edge detector is being
      trained on *every* part of every image, not just on one spot.  And that's
      good, because edges appear at different locations in different images.
      The location no longer matters; the edge detector can learn from edges in
      every part of the image.]

[In a neural net, you can think of hidden units as features that we learn, as
 opposed to features that you code up yourself.  Convolutional neural nets
 take them to the next level by learning features from multiple patches
 simultaneously and then applying those features everywhere, not just in the
 patches where they were originally learned.]
[By the way, although local connectivity is inspired by our visual systems,
 shared weights obviously don't happen in biology.]

[Go to slides on computing in the visual cortex and ConvNets (cnn.pdf).]

[Neurologists can stick needles into individual neurons in animal brains.
 After a few hours the neuron dies, but until then they can record its action
 potentials.  In this way, biologists quickly learned how some of the neurons
 in the retina, called _retinal_ganglion_cells_, respond to light.  They have
 interesting _receptive_fields_, illustrated in the slides, which show that
 each ganglion cell receives excitatory stimulation from receptors in a small
 patch of the retina but inhibitory stimulation from other receptors around
 it.]

[The signals from these cells propagate to the V1 visual cortex in the
 occipital lobe at the back of your skull.  These cells proved much harder to
 understand.  David Hubel and Torsten Wiesel of the Johns Hopkins University
 put probes into the V1 visual cortex of cats, but they had a very hard time
 getting any neurons to fire there.  However, a lucky accident unlocked the
 secret and ultimately won them the 1981 Nobel Prize in Physiology.]

[Show YouTube movie:  https://www.youtube.com/watch?v=IOHayh06LJ4 ]

[The glass slide happened to be at the particular orientation the neuron was
 sensitive to.  The neuron doesn't respond to other orientations; just that
 one.  So they were pretty lucky to catch that.]

[The _simple_cells_ act as line detectors and/or edge detectors by taking
 a linear combination of inputs from retinal ganglion cells.]
[The _complex_cells_ act as location-independent line detectors by taking
 inputs from many simple cells.]

[Later researchers showed that local connectivity runs through the V1 cortex
 by projecting certain images onto the retina and using radioactive tracers in
 the cortex to mark which neurons had been firing.  Those images show that the
 neural mapping from the retina to V1 is "retinatopic", i.e., local.]

[Unfortunately, as we go deeper into the visual system, layers V2 and V3 and
 so on, we know less and less about what processing the visual cortex does.]

[ConvNets were first popularized by the success of Yann LeCun's "LeNet 5"
 handwritten digit recognition software.  LeNet 5 has six hidden layers!
 Layers 1 and 3 are _convolutional_layers_ in which groups of units share
 weights.  Layers 2 and 4 are _pooling_layers_ that make the image smaller.
 These are just hardcoded max-functions with no weights and nothing to train.
 Layers 5 and 6 are just regular layers of hidden units with no shared weights.
 A great deal of experimentation went into figuring out the number of layers
 and their sizes.  At its peak, LeNet 5 was responsible for reading the zip
 codes on 10% of US Mail, and another Yann LeCun system was deployed in ATMs
 and check reading machines and was reading 10 to 20% of all the checks in the
 US by the late 90's.]

[When ConvNets were first applied to image analysis, researchers found that
 some of the learned masks are edge detectors or line detectors, similar to
 the ones that Hubel and Wiesel discovered!  This created a lot of excitement
 in both the computer learning community and the neuroscience community.
 The fact that a neural net can naturally learn the same features as the
 mammalian visual cortex is impressive.]

[I told you last week that neural nets research was popular in the 60's, but
 the 1969 book "Perceptrons" killed interest in them throughout the 70's.  They
 came back in the 80's, but interest was partly killed off a second time in the
 00's by...guess what?  By support vector machines.  SVMs work well for a lot
 of tasks, they're faster to train, and they more or less have only one
 hyperparameter, whereas neural nets take a lot of work to tune.]
[Neural nets are now in their third wave of popularity.  The single biggest
 factor in bringing them back is probably big data.  Thanks to the internet,
 we now have absolutely huge collections of images to train neural nets with,
 and researchers have discovered that neural nets often give better performance
 than competing algorithms when you have huge amounts of data to train them
 with.  In particular, convolutional neural nets are now learning better
 features than hand-tuned features.  That's a recent change.]

[One event that brought attention back to neural nets was the ImageNet Image
 Classification Challenge in 2012.  The winner of that competition was a neural
 net, and it won by a huge margin, about 10%.  It's called AlexNet, and it's
 surprisingly similarly to LeNet 5, in terms of how its layers are structured.
 However, there are some new innovations that led to their prize-winning
 performance, besides the fact that the training set had 1.4 million images:
 they used ReLUs, GPUs for training, and dropout.]

** lec 20
[I told you last week that neural net research was popular in the 60's, but
 the 1969 book "Perceptrons" killed interest in them throughout the 70's.  They
 came back in the 80's, but interest was partly killed off a second time in the
 00's by...guess what?  By support vector machines.  SVMs work well for a lot
 of tasks, they're faster to train, and they more or less have only one
 hyperparameter, whereas neural nets take a lot of work to tune.]
[Neural nets are now in their third wave of popularity.  The single biggest
 factor in bringing them back is probably big data.  Thanks to the internet,
 we now have absolutely huge collections of images to train neural nets with,
 and researchers have discovered that neural nets often give better performance
 than competing algorithms when you have huge amounts of data to train them
 with.  In particular, convolutional neural nets are now learning better
 features than hand-tuned features.  That's a recent change.]

[One event that brought attention back to neural nets was the ImageNet Image
 Classification Challenge in 2012.]
[Show ImageNet slide (imagenet.png).]
[The winner of that competition was a neural net, and it won by a huge margin,
 about 10%.  It's called AlexNet, and it's surprisingly similarly to LeNet 5,
 in terms of how its layers are structured.  However, there are some new
 innovations that led to their prize-winning performance, besides the fact that
 the training set had 1.4 million images:  they used ReLUs, GPUs for training,
 and dropout.]
[Show AlexNet convolutional neural net diagram (alexnet.pdf).]


UNSUPERVISED LEARNING
=====================
We have sample points, but no labels!
No classes, no y-values, nothing to predict.
Goal:  Discover structure in the data.

Examples:
- Clustering:  partition data into groups of similar/nearby points.
- Dimensionality reduction:  data often lies near a low-dimensional subspace
  (or manifold) in feature space; matrices have low-rank approximations.
  [Whereas clustering is about grouping similar sample points, dimensionality
   reduction is more about identifying a continuous variation from sample point
   to sample point.]
- Density estimation:  fit a continuous distribution to discrete data.
  [When we use maximum likelihood estimation to fit Gaussians to sample points,
   that's density estimation, but we can also fit functions more complicated
   than Gaussians, with more local variation.]


PRINCIPAL COMPONENTS ANALYSIS (PCA) (Karl Pearson, 1901)
=============================
Goal:  Given sample points in R^d, find k directions that capture most of the
variation.  (Dimensionality reduction.)
[Show 3D points projected to 2D (3dpca.pdf).]
[Show MNIST digits projected to 2D (pcadigits.pdf).]

Why?
- Find a small basis for representing variations in complex things, e.g. faces.
- Reducing # of dimensions makes some computations cheaper, e.g. regression.
- Remove irrelevant dimensions to reduce overfitting in learning algs.
  Like subset selection, but we can choose features that aren't axis-aligned,
  i.e., linear combos of input features.
[Sometimes PCA is used as a preprocess before regression or classification for
 the last two reasons.]

Let X be n-by-d design matrix.  [No fictitious dimension.]
From now on, assume X is centered:  mean X_i is zero.
[As usual, we can center the data by computing the mean x-value, then
 subtracting the mean from each sample point.]

[Let's start by seeing what happens if we pick just one principal direction.]
Let w be a unit vector.                                 ~
The _orthogonal_projection_ of point x onto vector w is x = (x . w) w
               ~   x . w
If w not unit, x = ----- w
                   |w|^2

                    o x
                    |
                    |
       w            v ~
   O---------->     o x

[The idea is that we're going to pick the best direction w, then project all
 the data down onto w so we can analyze it in a one-dimensional space.
 Of course, we lose a lot of information when we project down from d dimensions
 to just one.  So, suppose we pick several directions.  Those directions span
 a subspace, and we want to project points orthogonally onto the subspace.
 This is easy *if* the directions are orthogonal to each other.]
                                            ~    k
Given orthonormal directions v_1, ..., v_k, x = sum (x . v ) v
                                                i=1       i   i
[The word "orthonormal" implies they're mutually orthogonal and length 1.]
[Draw picture of orthogonal projection of a point onto a plane in 3D space.]
[Usually we don't actually want the projected point in R^d;
 usually we want the coordinates x . v_i in principal components space.]

X^T X is square, symmetric, positive semidefinite, d-by-d matrix.
Let 0 <= lambda_1 <= lambda_2 <= ... <= lambda_d be its eigenvalues.   [sorted]
Let v_1, v_2, ..., v_d be corresponding orthogonal *unit* eigenvectors.
[It turns out that the principal directions will be these eigenvectors, and
 the most important ones will be the ones with the greatest eigenvalues.
 I will show you this in three different ways.]

PCA derivation 1:  Fit a Gaussian to data with maximum likelihood estimation.
Choose k Gaussian axes of greatest variance.
[Show Gaussian fitted to sample points (gaussfitpca.png).]
                                                ^     1  T
Recall that MLE estimates a covariance matrix Sigma = - X  X.  [If X centered.]
                                                      n
PCA Alg:
- Center X.
- Optional:  Normalize X.  Units of measurement different?
  * Yes:  Normalize.
    [Bad for principal components to depend on arbitrary choice of scaling.]
  * No:  Usually don't.
    [If several features have the same unit of measurement, but some of them
     have much smaller variance, that difference is usually meaningful.]
  [Show difference outcomes between normalized and not (normalize.pdf).]
- Compute unit eigenvectors/values of X^T X.
- Optional:  choose k based on the eigenvalue sizes.
- For the best k-dimensional subspace, pick eigenvectors v_{d-k+1}, ..., v_d.
- Compute the coordinates of training/test data in principal components space.
  [When we do this projection, we have two choices:  we can un-center the input
   data before projecting it, OR we can translate the test data by the same
   vector we used to translate the training data when we centered it.]
[Show graph of # of eigenvectors vs. variance captured (variance.pdf).
 In this example, just 3 eigenvectors capture 70% of the variance.]
[If you are using PCA as a preprocess for a supervised learning algorithm,
 there's a more effective way to choose k:  (cross-)validation.]


PCA derivation 2:  Find direction w that maximizes variance of projected data
[In other words, when we project the data down, we don't want it all to bunch
 up; we want to keep it as spread out as possible.]
[Show projection of points (project.jpg).]
                                                               T  T
        ~   ~        ~       1  n         w  2   1 |Xw|^2   1 w  X  X w
  Var({ X , X , ..., X  }) = - sum (X  . ---)  = - ------ = - ---------
         1   2        n      n i=1   i   |w|     n  |w|^2   n   w^T w
                                                              \_______/
                                             _Rayleigh_quotient_ of X^T X and w

[This fraction is a well-known construction called the Rayleigh quotient.  When
 you see it, you should smell eigenvectors nearby.  How do we maximize this?]
If w is an eigenvector v_i, Ray. quo. = lambda_i
-> of all eigenvectors, v_d achieves maximum variance lambda_d / n.
One can show v_d beats every other vector too.
[Because every vector w is a linear combination of eigenvectors, and so
 its Rayleigh quotient will be a convex combination of eigenvalues.
 It's easy to prove this, but I don't want to take the time.
 For the proof, look up "Rayleigh quotient" in Wikipedia.]
[So the top eigenvector gives us the best direction.  But we typically want
 k directions.  After we've picked one direction, then we have to pick
 a direction that's orthogonal to the best direction.  But subject to that
 constraint, we again pick the direction that maximizes the variance.]
What if we constrain w to be orthogonal to v_d?  Then pick v_{d-1}.


PCA derivation 3:  Find direction w that minimizes "projection error"
[Show animation of PCA projection (PCAanimation.gif).]
[You can think of this as a sort of least-squares linear regression, with one
 important change.  Instead of measuring the error in a fixed vertical
 direction, we're measuring the error in a direction orthogonal to the
 principal component direction we choose.]
[Show linear regression vs. PCA (mylsq.png, mypca.png).]

   n  |     ~ |2    n  |     X_i . w  |2    n       2          w  2
  sum |X  - X |  = sum |X  - ------- w|  = sum (|X |  - (X  . ---) )
  i=1 | i    i|    i=1 | i    |w|^2   |    i=1    i       i   |w|

                 = constant - n (variance from derivation 2).

Minimizing projection error = maximizing variance.
[From this point, we carry on with the same reasoning as derivation 2.]

[Show illustration of the first two principal components of the single
 nucleotide polymorphism (SNP) matrix for the genes of various Europeans
 (europegenetics.pdf).  The input matrix has 2,541 people from these locations
 in Europe, and 309,790 SNPs.  Each SNP is binary, so think of it as 309,790
 dimensions of zero or one.  The output shows spots on the first two principal
 components where the projected people from a particular national type are
 denser than a certain threshold.  What's amazing about this is how closely the
 projected genotypes match the geography of Europe.  (From Lao et al., 2008.)]

Eigenfaces
----------
X contains n images of faces, d pixels each.
[If we have a 200 x 200 image of a face, we represent it as a vector of length
 40,000, the same way we represent the MNIST digit data.]
Face recognition:  Given a query face, compare it to all training faces;
                   find nearest neighbor in R^d.
[This works best if you have several training photos of each person you want to
 recognize, with different lighting and different facial expressions.]
Problem:  Each query takes Theta(nd) time.
Solution:  Run PCA on faces.  Reduce to much smaller dimension d'.
           Now nearest neighbor takes O(nd') time.
           [Possibly even less.  We'll talk about speeding up nearest-neighbor
            search at the end of the semester.  If the dimension is small
            enough, you can sometimes do better than linear time.]
[Show images of average face and eigenfaces (facerecaverage.jpg,
 facereceigen0.jpg, facereceigen119.jpg, facereceigen.jpg).]
[Show images of a face projected onto the first 4 and 50 eigenvectors
 (eigenfaceproject.pdf).  Latter is blurry but good enough for recognition.]
For best results, equalize the intensity distributions first.
[Show image equalization (facerecequalize.jpg).]
[If each image has 40,000 pixels, and you reduce it to 40 principal components,
 then each query face requires you to read 20,000 stored coordinates instead of
 20 million pixels.]

[Eigenfaces are not perfect.  They encode both face shape *and* lighting.
 Ideally, we would have some way to factor out lighting and analyze face shape
 only, but that's harder.  Some people say that the first 3 eigenfaces are
 usually all about lighting, and you sometimes get better facial recognition by
 dropping the first 3 eigenfaces.]
[Show Blanz-Vetter face morphing video (morphmod.mpg).]
[Blanz and Vetter use PCA in a more sophisticated way for 3D face modeling.
 They take 3D scans of people's faces and find correspondences between peoples'
 faces and an idealized model.  For instance, they identify the tip of your
 nose, the corners of your mouth, and other facial features, which is something
 the original eigenface work did not do.  Instead of feeding an array of pixels
 into PCA, they feed the 3D locations of various points on your face into PCA.
 This works more reliably.]

** lec 21
The Singular Value Decomposition (SVD) [and its application to PCA]
--------------------------------
Problems:  Computing X^T X takes Theta(n d^2) time.
           X^T X is poorly conditioned -> numerically inaccurate eigenvectors.
           [The SVD improves both these problems.]

[Earlier this semester, we learned about the eigendecomposition of a square,
 symmetric matrix.  Unfortunately, nonsymmetric matrices don't decompose nearly
 as nicely, and non-square matrices don't have eigenvectors at all.  Happily,
 there is a similar decomposition that works for all matrices, even if they're
 not symmetric and not square.]

Fact:  If n >= d, we can find a _singular_value_decomposition_

                                                          T     d             T
         X       =       U               D               V   = sum delta  u  v
                                      diagonal                 i=1      i  i  i
    -----------     -----------     -----------     -----------    \__________/
    |         |     |^       ^|     |d_1     0|     |<-- v -->|       rank 1
    |         |     ||       ||     |  d_2    |     |     1   |   outer product
    |         |  =  ||       ||     |    ..   |     |         |       matrix
    |         |     |u       u|     |     ..  |     |         |
    |         |     ||1      |d     |0     d_d|     |<-- v -->|
    |         |     ||       ||     -----------     ------d----
    |         |     |v       v|        d x d           d x d
    -----------     -----------                        T
       n x d           n x d                          V  V = I
                                               orthonormal v_i's are
                       T                       _right_singular_vectors_ of X
                      U  U = I
               orthonormal u 's are _left_singular_vectors_ of X
                            i

Diagonal entries delta_1, ..., delta_d of D are nonnegative _singular_values_
of X.

[Some of the singular values might be zero.  The number of nonzero singular
 values is equal to the rank of the centered design matrix X.  If all the
 sample points lie on a line, there is only one nonzero singular value.  If the
 points span a subspace of dimension r, there are r nonzero singular values.]
[If n < d, an SVD still exists, but now U is square and V is not.]

Fact:  v_i is an eigenvector of X^T X w/eigenvalue delta_i^2.
Proof:  X^T X = V D U^T U D V^T = V D^2 V^T
        which is an eigendecomposition of X^T X.

[The columns of V are the eigenvectors of X^T X, which is what we need for PCA.
 If n < d, V will omit some of the eigenvectors that have eigenvalues zero, but
 those are useless for PCA.  The SVD also tells us the eigenvalues, which are
 the squares of the singular values.  By the way, that's related to why the SVD
 is more numerically stable:   because the ratios between singular values are
 smaller than the ratios between eigenvalues.]

Fact:  We can find the k greatest singular values & corresponding vectors
       in O(n d k) time.
       [So we can save time by computing some of the singular vectors without
        computing all of them.]
       [There are approximate, randomized algorithms that are even faster,
        producing an approximate SVD in O(n d log k) time.  These are starting
        to become popular in algorithms for very big data.]
        https://code.google.com/archive/p/redsvd/ ]

Row i of UD gives the coordinates of sample point X_i in principal components
space (i.e. X_i . v_j for each j).  So we don't need to project the input
points onto that space; the SVD has already done it for us.

CLUSTERING
==========
Partition data into clusters so points within a cluster are more similar than
across clusters.
Why?
- Discovery:  Find songs similar to songs you like; determine market segments
- Hierarchy:  Find good taxonomy of species from genes
- _Quantization_:  Compress a data set by reducing choices
- Graph partitioning:  Image segmentation; find groups in social networks
[Show clusters that classify Barry Zito's baseball pitches (zito.pdf).
 Here we discover that there really are distinct classes of baseball pitches.]

k-Means Clustering aka Lloyd's Algorithm (Stuart Lloyd, 1957)
----------------------------------------
Goal:  Partition n points into k disjoint clusters.
       Assign each input point X_i a cluster label y_i in [1, k].
                                   1
       Cluster i's _mean_ is mu  = --  sum  X , given n  points in cluster i.
                               i   n   y =i  j         i
                                    i   j

  ----------------------------------------------
  |                        k       |        |2 |  [Sum of the squared distances
  | Find y that minimizes sum sum  |X  - mu |  |   from points to their cluster
  |                       i=1 y =i | j     i|  |   means.]
  |                            j               |
  ----------------------------------------------

NP-hard.  Solvable in O(n k^n) time.  [Try every partition.]
k-means heuristic:  Alternate between
                      (1)  y_j's are fixed; update mu_i's
                      (2)  mu_i's are fixed; update y_j's
                    Halt when step (2) changes no assignments.
[So, we have an assignment of points to clusters.  We update the cluster means.
 Then we reconsider the assignment.  A point might change clusters if some
 other's cluster's mean is closer than its own cluster's mean.  Then repeat.]

Step (1):  One can show (calculus) the optimal mu_i is the mean of the points
           in cluster i.
           [This is really easy calculus, so I leave it as a short exercise.]
Step (2):  The optimal y assigns each point X_j to the closest center mu_i.
           [This should be even more obvious than step (1).]
           [If there's a tie, and one of the choices is for X_j to stay in the
            same cluster as the previous iteration, always take that choice.]

[Show example of 2-means (2means.png).]
[Show animation of 4-means with many points (4meansanimation.gif).]
Both steps decrease objective fn *unless* they change nothing.
[Therefore, the algorithm never returns to a previous assignment.]
Hence alg. must terminate.  [As there are only finitely many assignments.]
[This argument doesn't say anything optimistic about the running time, because
 we might see O(k^n) different assignments before we halt.  In theory, one can
 actually construct point sets in the plane that take an exponential number of
 iterations, but those never happen in practice.]
Usually very fast in practice.  Finds a local minimum, often not global.
[...which is not surprising, as this problem is NP-hard.]
[Show example where 4-means clustering fails (4meansbad.png).]

Getting started:
- Forgy method:  choose k random sample points to be initial mu_i's; go to (2).
- Random partition:  randomly assign each sample point to a cluster; go to (1).
[Forgy seems to be better, but Wikipedia mentions some variants of k-means for
 which random partition is better.]

For best results, run k-means multiple times with random starts.
[Show clusters found by running 3-means 6 times on the same sample points
 (ISL, Figure 10.7) (kmeans6times.pdf).]

[Why did we choose that particular objective function to minimize?  Partly
 because it is equivalent to minimizing the following function.]

Equivalent objective fn:  the _within-cluster_variation_

  -----------------------------------------------------
  |                        k  1            |       |2 |
  | Find y that minimizes sum -- sum  sum  |X  - X |  |
  |                       i=1 n  y =i y =i | j    m|  |
  |                            i  j    m              |
  -----------------------------------------------------
[This objective function is equal to twice the previous one.  It's a worthwhile
 exercise to show that.  The nice thing about this expression is that it
 doesn't include the means; it's a direct function of the input points and
 the clusters we assign them to.]

Normalize the data?  Same advice as for PCA.  Sometimes yes, sometimes no.
[If some features are much larger than others, they will tend to dominate the
 Euclidean distance.  So if you have features in different units of
 measurement, you probably should normalize them.  If you have features in the
 same unit of measurement, you probably shouldn't, but it depends on context.]

k-Medoids Clustering
--------------------
Generalizes k-means beyond Euclidean distance.
Specify a _distance_fn_ d(x, y) between points x, y, aka _dissimilarity_.
Can be arbitrary; ideally satisfies
  triangle inequality d(x, y) <= d(x, z) + d(z, y).
[Sometime people use the l_1 norm or the l_infinity norm.  Sometimes people
 specify a matrix of pairwise distances between the input points.]
[Suppose you have a database that tells you how many of each product each
 customer bought.  You'd like to cluster together customers who buy similar
 products for market analysis.  But if you cluster customers by distance,
 you'll get one big cluster of all the customers who have only ever bought one
 thing.  So distance is not a good measure of dissimilarity.  Instead, it makes
 more sense to treat each customer as a vector and measure the *angle* between
 two customers.  If there's a large angle between customers, they're
 dissimilar.]
Replace mean computation with _medoid_, the sample point that minimizes total
  distance to other points in same cluster.
[So the medoid of a cluster is always one of the input points.]

[One difficulty with k-means is that you have to choose the number k of
 clusters before you start, and there isn't any reliable way to guess how many
 clusters will best fit the data.  The next method, hierarchical clustering,
 has the advantage in that respect.  By the way, there is a whole Wikipedia
 article on "Determining the number of clusters in a data set".]

Hierarchical Clustering
-----------------------
Creates a tree; every subtree is a cluster.
[So some clusters contain smaller clusters.]

Bottom-up, aka _agglomerative_clustering_:
start with each point a cluster; repeatedly fuse pairs.
[Draw figure of points in the plane; pair clusters together until all points
 are in one top-level cluster.]

Top-down, aka _divisive_clustering_:
start with all pts in one cluster; repeatedly split it.
[Draw figure of points in the plane; divide points into subsets hierarchicially
 until each point is in its own subset.]

[When the input is a point set, agglomerative clustering is used much more in
 practice than divisive clustering.  But when the input is a graph, it's the
 other way around:  divisive clustering is more common.]

We need a distance fn for clusters A, B:

_complete_linkage_:  d(A, B) = max { d(w, x) : w in A, x in B }
_single_linkage_:    d(A, B) = min { d(w, x) : w in A, x in B }
                                  1
_average_linkage_:   d(A, B) = -------  sum     sum   d(w, x)
                               |A| |B| w in A  x in B
_centroid_linkage_:  d(A, B) = d(mu_A, mu_B)            where mu_S is mean of S
[The first three of these methods work for any distance function, even if the
 input is just a matrix of distances between all pairs of points.  The centroid
 linkage only really makes sense if we're using the Euclidean distance.  But
 there's a variation of the centroid linkage that uses the medoids instead of
 the means, and medoids are defined for any distance function.  Moreover,
 medoids are more robust to outliers than means.]

Greedy agglomerative alg.:
Repeatedly _fuse_ the two clusters that minimize d(A, B)
Naively takes O(n^3) time.
[But for complete and single linkage, there are more sophisticated algorithms
 called CLINK and SLINK, which run in O(n^2) time.]

_Dendrogram_:  Illustration of the cluster hierarchy (tree) in which the
vertical axis encodes all the linkage distances.
[Show example of dendogram (ISL, Figure 10.9) (dendrogram.pdf).]
Cut dendrogram into clusters by horizontal line according to your choice of
# of clusters OR intercluster distance.
[It's important to be aware that the horizontal axis of a dendrogram has no
 meaning.  You could swap some node's left children and right children and it
 would still be the same dendrogram.  It doesn't always mean anything that two
 leaves happen to be next to each other.]

[Show comparison of average/complete/single linkages (ISL, Figure 10.12)
 (linkages.pdf).]
[Probably the worst of these is the single linkage, because it's very sensitive
 to outliers.  Notice that if you cut it into three clusters, two of them have
 only one node.  It also tends to give you a very unbalanced tree.]
[The complete linkage tends to be the best balanced, because when a cluster
 gets large, the furthest point in the cluster is always far away.  So large
 clusters are more resistant to growth than small ones.  If balanced clusters
 are your goal, this is your best choice.]
[In most cases you probably want the average or complete linkage.]
Warning:  centroid linkage can cause _inversions_ where a parent cluster is
          fused at a lower height than its children.
[So statisticians don't like it, but nevertheless, centroid linkage is popular
 in genomics.]

[As a final note, all the clustering algorithms we've studied so far are
 unstable, in the sense that deleting a few input points can sometimes give you
 very different results.  But these unstable heuristics are still the most
 commonly used clustering algorithms.  And it's not clear to me whether a truly
 stable clustering algorithm is even possible.]

** lec 22
SPECTRAL GRAPH CLUSTERING
=========================
Input:  Weighted, undirected graph G = (V, E).  No self-edges.
        w_ij = weight of edge (i, j) = (j, i); zero if (i, j) not in E.
[Think of the edge weights as a similarity measure.  A big weight means that
 the two vertices want to be in the same cluster.  So the circumstances are
 the opposite of the last lecture on clustering.  Then, we had a distance or
 dissimilarity function, so small numbers meant that points wanted to stay
 together.  Today, big numbers mean that vertices want to stay together.]
Goal:  Cut G into 2 (or more) pieces G_i of similar sizes,
       but don't cut too much edge weight.
       [That's a vague goal.  There are many ways to make this precise.
        Here's a typical goal, which we'll solve approximately.]

                                       Cut(G_1, G_2)
       e.g. Minimize the _sparsity_ -------------------, aka _cut_ratio_
                                    Mass(G_1) Mass(G_2)

       where Cut(G_1, G_2) = total weight of cut edges
             Mass(G_1) = # of vertices in G_1 OR assign masses to vertices
[The denominator "Mass(G_1) Mass(G_2)" penalizes imbalanced cuts.]
[Show illustration of four cuts (graph.pdf).  All edges have weight 1.
 Upper left:  the _minimum_bisection_; a _bisection_ is perfectly balanced.
 Upper right:  the _minimum_cut_.  Usually very unbalanced; not what we want.
 Lower left:  the _sparsest_cut_, which is good for many applications.
 Lower right:  the _maximum_cut_; in this case also the maximum bisection.]
       Sparsest cut is NP-hard.  [We will look for an approximate solution.]

[We will turn this combinatorial graph cutting problem into algebra.]
Let n = |V|.  Let y in R^n be an _indicator_vector_:
        / 1     vertex i in G_1,
  y  = <
   i    \ -1    vertex i in G_2.

       w_ij          2    / w       (i, j) is cut,
  Then ---- (y  - y )  = <   ij
        4     i    j      \ 0       (i, j) is not cut.

                            w_ij          2
  Cut(G , G ) =    sum      ---- (y  - y )
       1   2    (i,j) in E   4     i    j

                1                   2                      2
              = -    sum      (w   y  - 2 w   y  y  + w   y )
                4 (i,j) in E    ij  i      ij  i  j    ij  j

                1                                n   2
              = - (    sum      - 2 w   y  y  + sum y  sum  w   )
                4   (i,j) in E       ij  i  j   i=1  i k!=i  ik
                    \_______________________/   \_____________/
                y^T L y     off-diagonal terms   diagonal terms
              = -------
                   4
                                                   (1)           -            -
               / - w_ij        i != j,          1 /   \ 3        |  4  -1  -3 |
  where L   = <                                  /     \     L = | -1   6  -5 |
         ij    \ sum  w        i = j.          (2)-----(3)       | -3  -5   8 |
                 k!=i  ik                           5            -            -

L is symmetric, n-by-n _Laplacian_matrix_ for G.

[L is effectively a matrix representation of G.  For the purpose of
 partitioning a graph, there is no need to distinguish edges of weight zero
 from edges that are not in the graph.]
[We see that minimizing the weight of the cut is equivalent to minimizing the
 _Laplacian_quadratic_form_ y^T L y.  This lets us turn graph partitioning into
 a problem in matrix algebra.]
[Usually we assume there are no negative weights, in which case Cut(G_1, G_2)
 can never be negative, so it follows that L is positive semidefinite.]

If y = *1* = [ 1  1  ...  1 ]^T, then Cut(G_1, G_2) = 0, so     [*1* = bold 1.]
*1* is an eigenvector of L with eigenvalue 0.

[If G is connected and all the edge weights are positive, then this is the
 only zero eigenvalue.  But if G is not connected, L has one zero eigenvalue
 for each connected component of G.  It's easy to prove, but time prevents me.]

_Bisection_:  exactly n/2 vertices in G_1, n/2 in G_2.  Write *1*^T y = 0.
[So we have reduced graph bisection to this constrained optimization problem.]

  --------------------------------------------
  |                        T                 |
  | Find y that minimizes y  L y             |
  |                                          |
  | subject to  for all i, y  = 1 or y  = -1 | <- _binary_constraint_
  |                         i         i      |
  |                T                         |
  |        and  *1*  y = 0                   | <- _balance_constraint_
  --------------------------------------------

Also NP-hard.  We _relax_ the binary constraint.  -> fractional vertices!

[A very common approach in combinatorial optimization algorithms is to relax
 some of the constraints so a discrete problem becomes a continuous problem.
 Intuitively, this means that you can put 1/3 of vertex 7 in graph G_1 and
 the other 2/3 of vertex 7 in graph G_2.  You can even put -1/2 of vertex 7 in
 graph G_1 and 3/2 of vertex 7 in graph G_2.  This sounds crazy, but the
 continuous problem is much easier to solve than the combinatorial problem.
 After we solve it, we will _round_ the vertex values to +1/-1, and we'll hope
 that our solution is still close to optimal.]
[We can't just drop the binary constraint, though.  We still need *some*
 constraint to rule out the solution y = 0.]

New constraint:  y must lie on sphere of radius sqrt(n).
[Draw figure showing constraint before--y lies at a vertex of the hypercube--
 and the constraint after--y lies on the sphere through those vertices.]

Relaxed problem:
  --------------------------
  |           T            |                T
  | Minimize y  L y        | \             y  L y
  |              T         |  > = Minimize ------ = Rayleigh quotient of L & y
  | subject to  y  y = n   | /               T
  |                        |                y  y
  |                T       |
  |        and  *1*  y = 0 |
  --------------------------

[Show illustration of isosurfaces of y^T Ly (cylinder.pdf) and
 illustration restricted to the hyperplane 1^T y = 0 (endview.pdf).]

[You should remember this Rayleigh quotient from the lecture on PCA.  As I said
 then, when you see a Rayleigh quotient, you should smell eigenvalues nearby.
 The y that minimizes this Rayleigh quotient is the eigenvector with the
 smallest eigenvalue.  We already know what that eigenvector is:  it's *1*.
 But that violates our balance constraint.  As you should recall from PCA, when
 you've used the most extreme eigenvector and you need an orthogonal one, the
 next-best optimizer of the Rayleigh quotient is the next eigenvector.]

Let lambda_2 = second-smallest eigenvalue of L.
Eigenvector v_2 is the _Fiedler_vector_.
[It would be wonderful if every component of the Fiedler vector was 1 or -1,
 but that happens more or less never.  So we round it.  The simplest way is to
 round all positive entries to 1 and all negative entries to -1.  But in both
 theory and practice, it's better to choose the threshold as follows.]

Spectral partitioning alg.:
- Compute Fiedler vector v_2 of L
- _Round_ v_2 with a _sweep_cut_:
  * Sort components of v_2.
  * Try the n - 1 cuts between successive components.  Choose min-sparsity cut.
    [If we're clever about it, we can try all these cuts in time linear in the
     number of edges in G.]

[Show example of graph partitioned by the sweep cut (specgraph.pdf).]
[Show what the un-rounded Fiedler vector looks like (specvector.pdf).]
[One consequence of relaxing the binary constraint is that the balance
 constraint no longer forces an exact bisection.  But that's okay; we're cool
 with a slightly off-balance constraint if it means we cut fewer edges.]
[Show illustration where an off-balance cut is better (lopsided.pdf).]

                                                                 L_ii
Fact:  Sweep cut finds a cut w/sparsity <= sqrt(2 lambda_2 max_i ----);
       _Cheeger's_inequality_.                                   M_ii
       The optimal cut has sparsity >= lambda_2 / 2.
[So the spectral partitioning algorithm is an approximation algorithm, albeit
 not one with a constant factor of approximation.  Cheeger's inequality is
 a very famous result in spectral graph theory, because it's one of the
 most important cases where you can relax a combinatorial optimization problem
 to a continuous optimization problem, round the solution, and still have
 a provably decent solution to the original combinatorial problem.]

Vertex Masses
-------------
[Sometimes you want the notion of balance to accord more prominence to some
 vertices than others.  We can assign masses to vertices.]
Let M be diagonal matrix with vertex masses on diagonal.
New balance constraint:  *1*^T M y = 0.
[This new balance constraint says that G_1 and G_2 should each have the same
 total mass.  It turns out that this new balance constraint is easier
 to satisfy if we also revise the sphere constraint a little bit.]
New ellipsoid constraint:   y^T M y = Mass(G) = sum M_ii.
[Instead of a sphere, now we constrain y to lie on an axis-aligned ellipsoid.]
[Draw the ellipsoid, which passes through points of hypercube.]

Now we want Fiedler vector of _generalized_eigensystem_ Lv = lambda Mv.
[Most algorithms for computing eigenvectors and eigenvalues of symmetric
 matrices can easily be adapted to compute eigenvectors and eigenvalues of
 symmetric generalized eigensystems like this too.]


Vibration Analogy
-----------------
[Show figure of system of springs and masses (vibrate.pdf).]
[For intuition about spectral partitioning, think of the eigenvectors as
 vibrational modes in a physical system of springs and masses.  Each vertex
 models a point mass that is constrained to move freely along a vertical rod.
 Each edge models a vertical spring with rest length zero and stiffness
 proportional to its weight, pulling two point masses together.  The masses are
 free to oscillate sinusoidally on their rods.  The eigenvectors of the
 generalized eigensystem Lv = lambda Mv are the vibrational modes of this
 physical system, and their eigenvalues are proportional to their frequencies.]

[Show figure of vibrational modes in path graph and grid graph (grids.pdf).]
[These illustrations show the first four eigenvectors for two simple graphs.
 On the left, we see that the first eigenvector is the eigenvector of all 1's,
 which represents a vertical translation of all the masses in unison.  That's
 not really a vibration, which is why the eigenvalue is zero.  The second
 eigenvector is the Fiedler vector, which represents the vibrational mode with
 the lowest frequency.  Each component indicates the amplitude with which the
 corresponding point mass oscillates.  At any point in time as the masses
 vibrate, roughly half the mass is moving up while half is moving down.  So it
 makes sense to cut between the positive components and the negative
 components.  The third eigenvector also gives us a nice bisection of the grid
 graph, entirely different from the Fiedler vector.  Some more sophisticated
 graph clustering algorithms use multiple eigenvectors.]

[I want to emphasize that spectral partitioning takes a global view of a graph.
 It looks at the whole gestalt of the graph and finds a good cut.
 By comparison, the clustering algorithms we saw last lecture were much more
 local in nature, so they're easier to fool.]

Greedy Divisive Clustering
--------------------------
Partition G into 2 subgraphs; recursively cluster them.
[The sparsity is a good criterion for graph clustering.  Use G's sparsest cut
 to divide it into two subgraphs, then recursively cut them.  You can stop when
 you have the right number of clusters, or you could keep going until each
 subgraph is a single vertex and create a dendrogram.]
Can form a dendrogram, but it may have inversions.
[There's no reason to expect that the sparsity of a subgraph is smaller than
 the sparsity of the parent graph, so the dendrogram can have inversions.
 But it's still useful for getting an arbitrary number of clusters on demand.]

The Normalized Cut
------------------
Set vertex i's mass M_ii = L_ii.  [Sum of edge weights adjoining vertex i.]
[That is how we define a "normalized cut", which turns out to be a good choice
 for many different applications.]
Popular for _image_segmentation_.
[Image segmentation is the problem of looking at a photograph and separating it
 into different objects.  To do that, we define a graph on the pixels.]
For pixels with location w_i, brightness b_i, use graph weights

                        2                    2
  w   = exp( - |w  - w |  / alpha - |b  - b |  / beta )
   ij            i    j               i    j

or zero if |w_i - w_j| large.
[We choose a distance threshold, typically less than 4 to 10 pixels apart.
 Pixels that are far from each other aren't connected.  alpha and beta are
 empirically chosen constants.  It often makes sense to choose beta
 proportional to the variance of the brightness values.]

[Show segmentation of baseball image (baseballsegment.pdf).  The upper left
 figure is a photo of a scene during a baseball game.  The other figures show
 segments of the image extracted by recursive spectral partitioning.]
[Show eigenvectors 2-9 from the baseball image (baseballvectors.pdf).]

Invented by [our own] Prof. Jitendra Malik and his student Jianbo Shi.

** lec 23
*** Clustering w/Multiple Eigenvectors
----------------------------------
For k clusters, compute first k eigenvectors v_1 = 1, v_2, ..., v_k of
generalized eigensystem Lv = lambda Mv.

       -----------     -----------
       |1 ^     ^|     |<-- V -->|   [V's columns are the eigenvectors with
       |1 .     .|     |     1   |    the k largest eigenvalues.]
       |1 .     .|     |         |
  V =  |1 v     v|  =  |         |
       |1 .2    .k     |         |   [Yes, we do include the all-1's vector v
       |1 .     .|     |         |    as one of the columns of V.]           1
       |1 v     v|     |<-- V -->|
       -----------     ------n----
          n x k

Row V_i is _spectral_vector_ [my name] for vertex i.
[These are vectors in a k-dimensional space I'll call the "spectral space".
 When we were using just one eigenvector, it made sense to cluster vertices
 together if their components were close together.  When we use more than one
 eigenvector, it turns out that it makes sense to cluster vertices together if
 their spectral vectors point in similar directions.]
Normalize each row V_i to unit length.
[Now you can think of the spectral vectors as points on a unit sphere centered
 at the origin.]
[Draw 2D example showing two clusters on a circle.  If the input graph has
 k components, the points in each cluster will have spectral vectors that are
 exactly orthogonal to all the other components' spectral vectors.]
k-means cluster these vectors.
[Because all the spectral vectors lie on the sphere, k-means clustering will
 cluster together vectors that are separated by small angles.]

[Show comparison of k-means vs. spectral (compkmeans.png, compspectral.png).
 These examples use an exponentially decaying function to assign weights to
 pairs of points, like we used for image segmentation but without the
 brightnesses.]

Invented by [our own] Prof. Michael Jordan, Stanford Prof. Andrew Ng [when he
was still a student at Berkeley], Yair Weiss.
[This wasn't the first algorithm that uses multiple eigenvectors for spectral
 clustering, but it has become one of the most popular.]

*** LATENT FACTOR ANALYSIS [aka latent semantic indexing]
======================
[You can think of this as dimensionality reduction for matrices.]
Suppose X is a _term-document_matrix_:  [aka _bag-of-words_model_]
row i represents document i; column j represents term j.  [Term = word.]
[Term-document matrices are usually _sparse_, meaning most entries are zero.]
X_ij = occurrences of term j in doc i?
       better:  log (1 + occurrences)  [So frequent words don't dominate.]
[Better still is to weight the entries so rare words give big entries and
 common words like "the" give small entries.  I'll omit the details.]

                  T    d             T
Recall SVD X = UDV  = sum delta  u  v .  Suppose delta  <= delta  for i >= j.
                      i=1      i  i  i                i         j

(Unlike PCA, we usually don't center X.)
For greatest delta_i, each v_i lists terms in a genre/cluster of documents
                      each u_i   "


E.g. u_1 might have large components for the romance novels,
     v_1   "    "     "       "      for terms "passion", "ravish", "bodice"...
[...and delta_1 would give us an idea how much bigger the romance novel market
 is than the markets for every other genre of books.]
[v_1 and u_1 tell us that there is a large subset of books that tend to use the
 same large subset of words.  We can read off the words by looking at the
 larger components of v_1, and we can read off the books by looking at the
 larger components of u_1.]
[The property of being a romance novel is an example of a _latent_factor_.
 So is the property of being the sort of word used in romance novels.
 There's nothing in X that tells you explicitly that romance novels exist,
 but the genre is a hidden connection between them that gives them a large
 singular value.  The vector u_1 reveals which books have that genre, and
 v_1 reveals which words are emphasized in that genre.]

Like clustering, but clusters overlap:  if u_1 picks out romances &
u_2 picks out histories, they both pick out historical romances.
[So latent factor analysis is a sort of clustering that permits clusters to
 overlap.  Another way in which it differs from traditional clustering is that
 the u-vectors contain real numbers, and so some points have stronger cluster
 membership than others.  One book might be just a bit romance, another more.]

Application in market research:
identifying consumer types (hipster, soccer mom) & items bought together.
[For applications like this, the first few singular vectors are the most
 useful.  Most of the singular vectors are mostly noise, and they have small
 singular values to tell you so.]

                    r             T
Truncated sum X' = sum delta  u  v  is _low-rank_approximation_ (rank r) of X.
                   i=1      i  i  i
[We choose the singular vectors with the largest singular values, because they
 carry the most/best information.]

    -----------     ------     -------     -----------
    |         |     |^  ^|     |d_1 0|     |<-- v -->|
    |         |     ||  ||     |  .. |     |     1   |
    |         |  =  ||  ||     |0 d_r|     |<-- v -->|
    |    X'   |     |u  u|     -------     ------r----
    |         |     ||1 |r      r x r          r x d
    |         |     ||  ||
    |         |     |v  v|
    -----------     ------
       n x d         n x r
  -------------------------------------------------------------------
  | X' is the rank-r matrix that minimizes [squared] Frobenius norm |
  |             2                   2                               |
  |   ||X - X'||  = sum (X   - X'  )                                |
  |             F   i,j   ij     ij                                 |
  -------------------------------------------------------------------
Applications:
- Fuzzy search.  [Suppose you want to find a document about gasoline prices,
  but the document you want doesn't have the word "gasoline"; it has the word
  "petrol".  One cool thing about the reduced-rank matrix X' is that it will
  probably associate that document with "gasoline", because the SVD tends to
  group synonyms together.]
- Denoising.  [The idea is to assume that X is a noisy measurement of some
  unknown matrix which probably has low rank.  If that assumption is partly
  true, then the reduced-rank matrix X' might be better than the input X.]
- Collaborative filtering:  fills in unknown values, e.g. user ratings.
  [Suppose the rows of X repesents Netflix users and the columns represent
   movies.  The entry X_ij is the review score that user i gave to movie j.
   But most users haven't reviewed most movies.  Just as the rank reduction
   will associate "petrol" with "gasoline", it will tend to associate users
   with similar tastes in movies, so the reduced-rank matrix X' can predict
   ratings for users who didn't supply any.  You'll try this out in the last
   homework.]

*** NEAREST NEIGHBOR CLASSIFICATION
===============================
[We're done with unsupervised learning.  Now I'm going back to classifiers, and
 I saved one of the simplest for the end of the semester.]

Idea:  Given query point v, find the k input points nearest v.
       Distance metric of your choice.
       Regression:  Return average value of the k points.
       Classification:  Return class with the most votes from the k points OR
                        return histogram of class probabilities.
[Obviously, the histogram of class probabilities has limited precision.  If
 k = 3, then the only probabilities you'll ever return are 0, 1/3, 2/3, or 1.
 You can improve the precision by making k larger, but you might underfit.
 It works best when you have a huge amount of data.]

[Show examples of 1-NN, 10-NN, and 100-NN (ISL, Figures 2.15, 2.16)
 (allnn.pdf).  You see that a larger k smooths out the boundary.  In this
 example, the 1-NN classifier is badly overfitting the data, and the 100-NN
 classifier is badly underfitting.  The 10-NN classifier does well:  it's
 reasonably close to the Bayes decision boundary.  Generally, the ideal k
 depends on how dense your data is.  As your data gets denser, the optimal k
 increases.]

[There are theorems showing that if you have a lot of data, nearest neighbors
 can work quite well.]
Theorem (Cover & Hart, 1967):
As n -> infinity, the 1-NN error rate is < B (2 - B)      where B = Bayes risk.
  if only 2 classes, <= 2B (1 - B)
[There are a few technical requirements of this theorem.  The most important
 is that the training points and the test points all have to be drawn
 independently from the same probability distribution.  The theorem applies to
 any separable metric space, so it's not just for the Euclidean metric.]

Theorem (Fix & Hodges, 1951):
As n -> infinity, k -> infinity, k/n -> 0,
k-NN error rate converges to B.  [Which means optimal.]

**** The Geometry of High-Dimensional Spaces
---------------------------------------
Consider unit ball B = { p in R^d : |p| <= 1 }
       & hypercube H = { p in R^d : |p_i| <= 1 }
[Draw 2D circle in square, 3D ball in cube.]
[In two dimensions, it looks like the circle fills most of the square.  But in
 100 dimensions, the ball takes almost no volume compared to the hypercube, and
 the corners of the cube are a distance of 10 away from the center.  And since
 there are 2^100 corners, there's a lot of volume out toward the corners.]
Consider a shell of the sphere.
[Draw ball of radius r, and concentric ball of radius r - epsilon inside.]
Volume of outer ball \propto r^d
Volume of inner ball \propto (r - epsilon)^d
Ratio of inner ball volume to outer =
  (r - epsilon)^d        epsilon             epsilon d
  --------------- = (1 - -------)^d ~ exp (- ---------) for large d -> small!
        r^d                 r                    r

         epsilon
E.g.  if ------- = 0.1 & d = 100, inner ball has 0.0027% of volume.
            r

Random points from uniform distribution in ball: nearly all are in outer shell.
  "      "     "   Gaussian     "       "   "      "     "   "  "  some    "

[A multivariate Gaussian distribution is weighted to put points closer to the
 center, but if the dimension is very high, you'll still find that the vast
 majority of points lie in a thin shell.  As the dimension grows, the
 *standard*deviation* of a random point's distance to the center gets smaller
 and smaller compared to the distance itself.]

[This is one of the things that makes machine learning hard in high dimensions.
 Sometimes the farthest points aren't much farther away than the nearest ones.]

**** Exhaustive k-NN alg.
--------------------
Given query point v:
- Scan through all n input points, computing (squared) distances to v.
- Maintain max-heap with the k shortest distances seen so far.
  [Whenever you encounter an input point closer to v than the point at the top
   of the heap, you remove the heap-top point and insert the better point.
   Obviously you don't need a heap if k = 1 or even 3, but if k = 100 a heap
   will speed up keeping track of the distance to beat.]

Time to construct the classifier:  0
[This is the only O(0)-time algorithm we'll learn this semester.]
Query time:  O(nd + n log k)
             expected O(nd + k log^2 k) if random point order
[Though in practice I don't recommend randomizing the point order; you'll
 probably lose more from the cache than you'll gain from randomization.]

**** Speeding Up NN
--------------
Can we preprocess the training points to obtain sublinear query time?

Very low dimensions:  Voronoi diagrams
Medium dim (up to ~30):  k-d trees
Larger dim:  locality sensitive hashing [still researchy, not widely adopted]
Largest dim:  no [stick with exhaustive k-NN.]
Can use PCA or other dimensionality reduction as preprocess
[Most fast nearest-neighbor algorithms in more than a few dimensions are
 *approximate* nearest neighbor algorithms; we don't necessarily expect to find
 the exact nearest neighbors any more.  That's usually okay, as machine
 learning classifiers are rarely perfect.  If we use PCA as a preprocess, then
 it's even more approximate, but it's much faster.]

PCA:  Row i of UD gives the coordinates of sample point X_i in principal
components space (i.e. X_i . v_j for each j).  So we don't need to project the
input points onto that space; the SVD does it for us.


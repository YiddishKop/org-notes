<p>A friend from grad school pointed out a great foundational paper on Boltzmann Machines.  It is a 1987 paper from complex systems theory</p>
<p style="text-align:center;"><a href="http://www.complex-systems.com/pdf/01-5-6.pdf">A Mean Field Theory Learning Algorithm for Neural Networks</a></p>
<p>just a couple years after Hinton&#8217;s seminal 1985 paper , &#8220;<a href="https://pdfs.semanticscholar.org/c7ba/792ec773d7c034a682a05588f763339f7fc5.pdf">A Learning Algorithm for Boltzmann Machines</a>&#8220;.</p>
<p>What I really like is how we see the foundations of deep learning arose from statistical physics and theoretical chemistry. My top 10 favorite take-a-ways are:</p>
<ul>
<li>The relation between Boltzmann Machines and the nearly forgotten Hopfield Associative Memory.  And why Hidden nodes made a big difference in just coming up with reasonable training algorithm.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>What an actual mean field theory (MFT) is.  They don&#8217;t just factor the Energy function or use a bi-partite graph. They introduce continuous fields (U,V)  via the delta function, and then take a saddle point approximation. Today we only see MFTs expressed as the resulting factorized models like RBMs and Sum-Product Networks; we don&#8217;t see the fields.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>That an annealing schedule was not about adjusting the learning rate&#8211;it was about adjusting the temperature schedule.   And that adjusting the annealing schedule is a huge flexible part of the model.  Yes, Boltzmann Machines were originally Temperature dependent.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>How to derive the learning rules for neural nets using Markov chains and the principle of microscopic reversibility.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>Where the tanh activation function came from.  They come from the MFT.  So today, sure, we use ReLUs.  But we don&#8217;t just have a random Energy function.  There is a deep reason for these activation functions.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>How the MFT here is also a quenched approximation.  This comes up all the time in analyzing the replica symmetric solutions of mean field spin glasses (i.e. replica symmetry breaking (RSB)).  You can&#8217;t understand the phase diagram of a spin glass without understanding this.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li> Why the Free Energy is <em>smoother (i.e. closer to convex)</em> than the highly non-convex, T=0 Energy landscape.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>We do not anneal to T=0.  RBMs, and, I suspect, Deep Learning in general, is not about traversing the T=0 Energy Landscape.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>The paper presents a very simple example that anyone can code up and run in a day:  2 input nodes, 1 output node, 4 hidden nodes.</li>
</ul>
<p>&nbsp;</p>
<ul>
<li>The references !  Great stuff.</li>
</ul>
<div></div>
<p>&nbsp;</p>
<div>This is, all in all, a fantastic paper from the statistical physics point of view.  And if there is interest, I can go through the details here.</div>
<p style="text-align:center;"><strong>Happy New Year everyone!</strong></p>
<p>&nbsp;</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/charlesmartin14.wordpress.com/9947/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/charlesmartin14.wordpress.com/9947/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=calculatedcontent.com&#038;blog=32496692&#038;post=9947&#038;subd=charlesmartin14&#038;ref=&#038;feed=1" width="1" height="1" />
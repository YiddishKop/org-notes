<p style="text-align:center;">&#8220;<strong><a href="https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe">Why does deep and cheap learning work so well?</a>&#8220;</strong></p>
<p style="text-align:left;">This is the question posed by a recent article.  Deep Learning seems to require knowing <a href="https://charlesmartin14.wordpress.com/2013/11/14/metric-learning-some-quantum-statistical-mechanics/">the Partition Function</a>&#8211;at least in old fashioned Restricted Boltzmann Machines (RBMs).</p>
<p style="text-align:left;">Here, I will discuss some aspects of this paper,  in the context of RBMs.</p>
<h3 style="text-align:left;">Preliminaries</h3>
<p>We can use RBMs for unsupervised learning, as a clustering algorithm, for pretraining larger nets, and for <a href="https://colinmorris.github.io/blog/dreaming-rbms">generating sample data</a>.   Mostly, however, RBMs are an older, toy model useful for understanding unsupervised Deep Learning.</p>
<h4>RBMs</h4>
<p>We define an RBM with an Energy Function</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3D-%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7BW%7D%5Cmathbf%7Bh%7D-%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Ba%7D-%5Cmathbf%7Bb%7D%5E%7BT%7D%5Cmathbf%7Bh%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{v},&#92;mathbf{h})=-&#92;mathbf{v}^{T}&#92;mathbf{W}&#92;mathbf{h}-&#92;mathbf{v}^{T}&#92;mathbf{a}-&#92;mathbf{b}^{T}&#92;mathbf{h} " title="E(&#92;mathbf{v},&#92;mathbf{h})=-&#92;mathbf{v}^{T}&#92;mathbf{W}&#92;mathbf{h}-&#92;mathbf{v}^{T}&#92;mathbf{a}-&#92;mathbf{b}^{T}&#92;mathbf{h} " class="latex" /></p>
<p>and it&#8217;s associated Partition Function</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3D%5Csum%5Climits_%7B%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%7D+e%5E%7B-E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z(&#92;mathbf{v},&#92;mathbf{h})=&#92;sum&#92;limits_{&#92;mathbf{v},&#92;mathbf{h}} e^{-E(&#92;mathbf{v},&#92;mathbf{h}}) " title="Z(&#92;mathbf{v},&#92;mathbf{h})=&#92;sum&#92;limits_{&#92;mathbf{v},&#92;mathbf{h}} e^{-E(&#92;mathbf{v},&#92;mathbf{h}}) " class="latex" /></p>
<p style="text-align:left;">The joint probability is then</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=P%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3D%5Cdfrac%7B1%7D%7BZ%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%7De%5E%7B-E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="P(&#92;mathbf{v},&#92;mathbf{h})=&#92;dfrac{1}{Z(&#92;mathbf{v},&#92;mathbf{h})}e^{-E(&#92;mathbf{v},&#92;mathbf{h})} " title="P(&#92;mathbf{v},&#92;mathbf{h})=&#92;dfrac{1}{Z(&#92;mathbf{v},&#92;mathbf{h})}e^{-E(&#92;mathbf{v},&#92;mathbf{h})} " class="latex" /></p>
<p style="text-align:left;">and the probability of  the visible units is computed by marginalizing over the hidden units</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bv%7D%29%3D%5Cdfrac%7B1%7D%7BZ%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%7D%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{v})=&#92;dfrac{1}{Z(&#92;mathbf{v},&#92;mathbf{h})}&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v},&#92;mathbf{h})} " title="p(&#92;mathbf{v})=&#92;dfrac{1}{Z(&#92;mathbf{v},&#92;mathbf{h})}&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v},&#92;mathbf{h})} " class="latex" /></p>
<p style="text-align:left;">Note we also mean the probability of observing the data <b>X={v}</b>, given the weights<strong> W.</strong></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bv%7D%29%3Dp%28%5Cmathbf%7Bv%7D%7C%5Cmathbf%7BW%2Ca%2Cb%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{v})=p(&#92;mathbf{v}|&#92;mathbf{W,a,b}) " title="p(&#92;mathbf{v})=p(&#92;mathbf{v}|&#92;mathbf{W,a,b}) " class="latex" /></p>
<p style="text-align:left;">The Likelihood is just the log of the probability</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%3D%5Cln%5C%3Bp%28%5Cmathbf%7Bv%7D%29%3D%5Cln%5C%3B%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%2Ch%7D%29%7D-%5Cln%5C%3BZ%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{L}=&#92;ln&#92;;p(&#92;mathbf{v})=&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v,h})}-&#92;ln&#92;;Z(&#92;mathbf{v,h}) " title="&#92;mathcal{L}=&#92;ln&#92;;p(&#92;mathbf{v})=&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v,h})}-&#92;ln&#92;;Z(&#92;mathbf{v,h}) " class="latex" /></p>
<p style="text-align:left;">We can break this into 2 parts:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%3D-F%5E%7Bc%7D%28%5Cmathbf%7Bv%7D%29%2BF%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{L}=-F^{c}(&#92;mathbf{v})+F(&#92;mathbf{v,h}) " title="&#92;mathcal{L}=-F^{c}(&#92;mathbf{v})+F(&#92;mathbf{v,h}) " class="latex" /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=F%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F(&#92;mathbf{v,h}) " title="F(&#92;mathbf{v,h}) " class="latex" /> is just the standard Free Energy</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%3D%5Cln%5C%3BZ%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F=&#92;ln&#92;;Z(&#92;mathbf{v,h}) " title="F=&#92;ln&#92;;Z(&#92;mathbf{v,h}) " class="latex" /></p>
<p style="text-align:left;">We call <img src="https://s0.wp.com/latex.php?latex=F%5E%7Bc%7D%28%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F^{c}(&#92;mathbf{v}) " title="F^{c}(&#92;mathbf{v}) " class="latex" /> the <em>clamped</em> Free Energy</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%5E%7Bc%7D%28%5Cmathbf%7Bv%7D%29%3D%5Cln%5C%3B%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%2Ch%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F^{c}(&#92;mathbf{v})=&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v,h})} " title="F^{c}(&#92;mathbf{v})=&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v,h})} " class="latex" /></p>
<p style="text-align:left;">because it is like a Free Energy, but with the visible units clamped to the data <strong>X</strong>.</p>
<p style="text-align:left;">The <em>clamped</em> <img src="https://s0.wp.com/latex.php?latex=F%5E%7Bc%7D%28%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F^{c}(&#92;mathbf{v}) " title="F^{c}(&#92;mathbf{v}) " class="latex" /><a href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/neural_network/rbm.py#L184"> is easy to evaluate</a> in the RBM formalism, whereas <img src="https://s0.wp.com/latex.php?latex=F%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F(&#92;mathbf{v,h}) " title="F(&#92;mathbf{v,h}) " class="latex" /> is computationally intractable.</p>
<p style="text-align:left;">Knowing the <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z(&#92;mathbf{v,h}) " title="Z(&#92;mathbf{v,h}) " class="latex" /> is &#8216;<em>like&#8217;</em> knowing the equilibrium distribution function, and methods like RBMs appear to approximate <img src="https://s0.wp.com/latex.php?latex=%5Cln%5C%3BZ%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;ln&#92;;Z(&#92;mathbf{v,h}) " title="&#92;ln&#92;;Z(&#92;mathbf{v,h}) " class="latex" /> in some form or another.</p>
<h4>RBM Training</h4>
<p>Training an RBM proceeds iteratively by approximating the Free Energies at each step,</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%28%5Cmathbf%7Bv%2Ch%7D%29%3D%5Ccdots+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F(&#92;mathbf{v,h})=&#92;cdots " title="F(&#92;mathbf{v,h})=&#92;cdots " class="latex" /></p>
<p>and then updating W with a gradient step</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5Crightarrow%5Cmathbf%7BW%7D%2B%5Clambda%5Cmathbf%7Bdw%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{W}&#92;rightarrow&#92;mathbf{W}+&#92;lambda&#92;mathbf{dw} " title="&#92;mathbf{W}&#92;rightarrow&#92;mathbf{W}+&#92;lambda&#92;mathbf{dw} " class="latex" /></p>
<p style="text-align:left;">RBMs are usually trained via <a href="http://www.gatsby.ucl.ac.uk/aistats/fullpapers/217.pdf">Contrastive Divergence</a> (CD or PCD).  The Energy function, being quadratic, lets us readily factor Z using a mean field approximation, leading to simple expressions for the conditional probabilities</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28h_%7Bi%7D%3D1%7Cv%29%3D%5Csigma%28b_%7Bi%7D%2B%5Cmathbf%7BW%7D_%7Bi%7D%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(h_{i}=1|v)=&#92;sigma(b_{i}+&#92;mathbf{W}_{i}&#92;mathbf{v}) " title="p(h_{i}=1|v)=&#92;sigma(b_{i}+&#92;mathbf{W}_{i}&#92;mathbf{v}) " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28v_%7Bi%7D%3D1%7Ch%29%3D%5Csigma%28a_%7Bi%7D%2B%5Cmathbf%7BW%7D_%7Bi%7D%5Cmathbf%7Bh%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(v_{i}=1|h)=&#92;sigma(a_{i}+&#92;mathbf{W}_{i}&#92;mathbf{h}) " title="p(v_{i}=1|h)=&#92;sigma(a_{i}+&#92;mathbf{W}_{i}&#92;mathbf{h}) " class="latex" /></p>
<p>and the weight update rule</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=d%5Cmathbf%7Bw%7D%3D%5Clangle+%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D+%5Crangle_%7B0%7D-%5Clangle%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%5Crangle_%7B%5Cinfty%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="d&#92;mathbf{w}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle_{0}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle_{&#92;infty} " title="d&#92;mathbf{w}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle_{0}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle_{&#92;infty} " class="latex" /></p>
<h5 style="text-align:left;">positive and negative phases</h5>
<p>RBM codes may use the terminology of positive and negative phases:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=positive%5C%3B%5C%3B%5Clangle%5Cmathbf%7Bv%2Ch%7D%5Crangle%5E%7B%2B%7D%5Crightarrow+negative%5C%3B%5C%3B%5Clangle%5Cmathbf%7Bv%2Ch%7D%5Crangle%5E%7B-%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="positive&#92;;&#92;;&#92;langle&#92;mathbf{v,h}&#92;rangle^{+}&#92;rightarrow negative&#92;;&#92;;&#92;langle&#92;mathbf{v,h}&#92;rangle^{-} " title="positive&#92;;&#92;;&#92;langle&#92;mathbf{v,h}&#92;rangle^{+}&#92;rightarrow negative&#92;;&#92;;&#92;langle&#92;mathbf{v,h}&#92;rangle^{-} " class="latex" /></p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cmathbf%7Bv%2Ch%7D%5Crangle%5E%7B%2B%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;langle&#92;mathbf{v,h}&#92;rangle^{+} " title="&#92;langle&#92;mathbf{v,h}&#92;rangle^{+} " class="latex" />:  The expectation <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cmathbf%7Bv%2Ch%7D%5Crangle_%7B0%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;langle&#92;mathbf{v,h}&#92;rangle_{0} " title="&#92;langle&#92;mathbf{v,h}&#92;rangle_{0} " class="latex" /> is evaluated, or <strong><em>clamped</em></strong>, on the data.</p>
<p><img src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cmathbf%7Bv%2Ch%7D%5Crangle%5E%7B-%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;langle&#92;mathbf{v,h}&#92;rangle^{-} " title="&#92;langle&#92;mathbf{v,h}&#92;rangle^{-} " class="latex" />:   The expectation <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cmathbf%7Bv%2Ch%7D%5Crangle_%7B%5Cinfty%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;langle&#92;mathbf{v,h}&#92;rangle_{&#92;infty} " title="&#92;langle&#92;mathbf{v,h}&#92;rangle_{&#92;infty} " class="latex" /> is <em>to be</em> evaluated on the prior distribution <img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7BX%7D%7C%5Cmathbf%7BW%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{X}|&#92;mathbf{W}) " title="p(&#92;mathbf{X}|&#92;mathbf{W}) " class="latex" />.  We also say <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cdot%5Crangle_%7B%5Cinfty%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;langle&#92;dot&#92;rangle_{&#92;infty} " title="&#92;langle&#92;dot&#92;rangle_{&#92;infty} " class="latex" /> is evaluated in the limit of infinite sampling, at the so-called equilibrium distribution.  <span style="color:#800080;"><em>But we don&#8217;t take the infinite limit.</em></span></p>
<p style="text-align:left;">CD approximates <img src="https://s0.wp.com/latex.php?latex=%5Clangle%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%5Crangle_%7B%5Cinfty%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle_{&#92;infty} " title="&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle_{&#92;infty} " class="latex" /> &#8211;effectively evaluating the (mean field) Free Energy <strong>&#8212;</strong>  by running only 1 (or more) steps of Gibbs Sampling.</p>
<p>So we may see</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=d%5Cmathbf%7Bw%7D%3D%5Clangle+%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D+%5Crangle_%7B0%7D-%5Clangle%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%5Crangle_%7B1%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="d&#92;mathbf{w}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle_{0}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle_{1} " title="d&#92;mathbf{w}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle_{0}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle_{1} " class="latex" /></p>
<p style="text-align:left;">or, more generally, and in some code bases, something effectively like</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=d%5Cmathbf%7Bw%7D%3D%5Clangle+%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D+%5Crangle%5E%7B%2B%7D-%5Clangle%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%5Crangle%5E%7B-%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="d&#92;mathbf{w}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle^{+}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle^{-} " title="d&#92;mathbf{w}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle^{+}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle^{-} " class="latex" /></p>
<h4 style="text-align:left;">Pseudocode is:</h4>
<hr />
<p style="text-align:left;padding-left:30px;"><span style="color:#333333;">Initialize the positive  <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7Bv%2Ch%7D%29%5E%7B%2B%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{v,h})^{+} " title="(&#92;mathbf{v,h})^{+} " class="latex" /> and negative <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7Bv%2Ch%7D%29%5E%7B-%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{v,h})^{-} " title="(&#92;mathbf{v,h})^{-} " class="latex" /></span></p>
<p style="text-align:left;padding-left:30px;"><span style="color:#333333;">Run N iterations of:</span></p>
<p style="padding-left:60px;"><span style="color:#808080;">Run 1 Step of Gibbs Sampling to get the negative  <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7Bv%2Ch%7D%29%5E%7B-%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{v,h})^{-} " title="(&#92;mathbf{v,h})^{-} " class="latex" />:</span></p>
<p style="padding-left:120px;"><span style="color:#808080;">sample the hiddens given the (current) visibles: <img src="https://s0.wp.com/latex.php?latex=p%28h%5E%7B-%7D_%7Bi%7D%3D1%7C%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(h^{-}_{i}=1|&#92;mathbf{v}) " title="p(h^{-}_{i}=1|&#92;mathbf{v}) " class="latex" /></span></p>
<p style="padding-left:120px;"><span style="color:#808080;">sample the visibles given the hiddens (above): <img src="https://s0.wp.com/latex.php?latex=p%28v%5E%7B-%7D_%7Bi%7D%3D1%7C%5Cmathbf%7Bh%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(v^{-}_{i}=1|&#92;mathbf{h}) " title="p(v^{-}_{i}=1|&#92;mathbf{h}) " class="latex" /></span></p>
<p style="padding-left:60px;"><span style="color:#808080;">Calculate the weight gradient:</span></p>
<p style="padding-left:120px;"><span style="color:#808080;"><img src="https://s0.wp.com/latex.php?latex=dw_%7Bi%2Cj%7D%3D%5Clangle+%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D+%5Crangle%5E%7B%2B%7D-%5Clangle%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%5Crangle%5E%7B-%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="dw_{i,j}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle^{+}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle^{-} " title="dw_{i,j}=&#92;langle &#92;mathbf{v}^{T}&#92;mathbf{h} &#92;rangle^{+}-&#92;langle&#92;mathbf{v}^{T}&#92;mathbf{h}&#92;rangle^{-} " class="latex" /></span></p>
<p style="padding-left:60px;"><span style="color:#808080;">Apply Weight decay or other regularization (optional):</span></p>
<p style="padding-left:120px;"><span style="color:#808080;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bdw%7D%5Crightarrow%5Cmathbf%7Bdw%7D%2B%5Cdelta%5CVert%5Cmathbf%7BW%7D%5CVert+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{dw}&#92;rightarrow&#92;mathbf{dw}+&#92;delta&#92;Vert&#92;mathbf{W}&#92;Vert " title="&#92;mathbf{dw}&#92;rightarrow&#92;mathbf{dw}+&#92;delta&#92;Vert&#92;mathbf{W}&#92;Vert " class="latex" /></span></p>
<p style="padding-left:60px;"><span style="color:#808080;">Apply a momentum (optional):</span></p>
<p style="padding-left:120px;"><span style="color:#808080;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bdw%7D%5Crightarrow%5Cmathbf%7Bdw%7D%2B%5Cmu%5Cmathbf%7BW%7D_%7Bprev%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{dw}&#92;rightarrow&#92;mathbf{dw}+&#92;mu&#92;mathbf{W}_{prev} " title="&#92;mathbf{dw}&#92;rightarrow&#92;mathbf{dw}+&#92;mu&#92;mathbf{W}_{prev} " class="latex" /></span></p>
<p style="padding-left:60px;"><span style="color:#808080;">Update the Weights:</span></p>
<p style="padding-left:120px;"><span style="color:#808080;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5Crightarrow%5Cmathbf%7BW%7D%2B%5Clambda%5Cmathbf%7Bdw%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{W}&#92;rightarrow&#92;mathbf{W}+&#92;lambda&#92;mathbf{dw} " title="&#92;mathbf{W}&#92;rightarrow&#92;mathbf{W}+&#92;lambda&#92;mathbf{dw} " class="latex" /></span></p>
<hr />
<h3 style="text-align:left;">Energy Renormalizations</h3>
<p>What is <em><strong>Cheap</strong></em> about learning ?  A technical proof in the Appendix notes that</p>
<p style="text-align:center;"><span style="color:#800000;"><em>knowing the Partition function <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z(&#92;mathbf{v,h}) " title="Z(&#92;mathbf{v,h}) " class="latex" /> is not the same as knowing the underlying distribution <img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{v}) " title="p(&#92;mathbf{v}) " class="latex" />.</em></span></p>
<p>This is because the Energy <em><img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{v,h}) " title="E(&#92;mathbf{v,h}) " class="latex" /></em> can be rescaled, or renormalized, in many different ways, without changing <em><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z(&#92;mathbf{v,h}) " title="Z(&#92;mathbf{v,h}) " class="latex" /> .</em></p>
<p>This is a also key idea in Statistical Mechanics.</p>
<p>The Partition function is a generating function; we can write all the macroscopic, observable thermodynamic quantities as partial derivatives of <em><img src="https://s0.wp.com/latex.php?latex=%5Cln%5C%3BZ+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;ln&#92;;Z " title="&#92;ln&#92;;Z " class="latex" />.  </em>And we can do this <em>without</em> knowing the exact distribution functions or energies&#8211;just their renormalized forms.</p>
<p>Of course, our <strong>W</strong> update rule is a derivative of <em><img src="https://s0.wp.com/latex.php?latex=%5Cln%5C%3BZ%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;ln&#92;;Z(&#92;mathbf{v,h}) " title="&#92;ln&#92;;Z(&#92;mathbf{v,h}) " class="latex" /></em></p>
<p>The proof is technically straightforward, albeit a bit odd at first.</p>
<h4>Matching Z(y) does not imply matching p(y)</h4>
<p>Let&#8217;s start with the visible units <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D%3D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v}= " title="&#92;mathbf{v}= " class="latex" />.  Write</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bv%7D%29%3D%5Cdfrac%7B1%7D%7BZ%7De%5E%7B-E%28%5Cmathbf%7Bv%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{v})=&#92;dfrac{1}{Z}e^{-E(&#92;mathbf{v})} " title="p(&#92;mathbf{v})=&#92;dfrac{1}{Z}e^{-E(&#92;mathbf{v})} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf%7Bv%7D%29%3D%5Csum%5Climits_%7B%5Cmathbf%7Bv%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z(&#92;mathbf{v})=&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-E(&#92;mathbf{v})} " title="Z(&#92;mathbf{v})=&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-E(&#92;mathbf{v})} " class="latex" /></p>
<p style="text-align:left;">We now introduce the hidden units, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bh%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{h} " title="&#92;mathbf{h} " class="latex" />, into the model, so that we have a new, joint probability distribution</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bv%2Ch%7D%29%3D%5Cdfrac%7B1%7D%7BZ%7De%5E%7B-E%28%5Cmathbf%7Bv%2Ch%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{v,h})=&#92;dfrac{1}{Z}e^{-E(&#92;mathbf{v,h})} " title="p(&#92;mathbf{v,h})=&#92;dfrac{1}{Z}e^{-E(&#92;mathbf{v,h})} " class="latex" /></p>
<p style="text-align:left;">and a new, Renormalized , partition function</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Z%5E%7BRG%7D%28%5Cmathbf%7Bv%2Ch%7D%29%3D%5Csum%5Climits_%7B%5Cmathbf%7Bv%2Ch%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%2Ch%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z^{RG}(&#92;mathbf{v,h})=&#92;sum&#92;limits_{&#92;mathbf{v,h}}e^{-E(&#92;mathbf{v,h})} " title="Z^{RG}(&#92;mathbf{v,h})=&#92;sum&#92;limits_{&#92;mathbf{v,h}}e^{-E(&#92;mathbf{v,h})} " class="latex" /></p>
<p style="text-align:left;">Where RG means Renormalization Group.  We have already discussed that the general RBM approach resembles the Kadanoff <a href="http://And it is related to an earlier post Why Deep Learning Works II: the Renormalization Group.">Variational Renormalization Group (VRG) method</a>, circa 1975. This new paper points out a small but important technical oversight made in the ML literature, namely that</p>
<p style="text-align:center;"><strong><span style="color:#ff6600;">having <img src="https://s0.wp.com/latex.php?latex=Z%5E%7BRG%7D%28%5Cmathbf%7Bv%2Ch%7D%29%3DZ%28%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z^{RG}(&#92;mathbf{v,h})=Z(&#92;mathbf{v}) " title="Z^{RG}(&#92;mathbf{v,h})=Z(&#92;mathbf{v}) " class="latex" /> <em>does not imply</em> <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7Bp%7D%28%5Cmathbf%7Bv%7D%29%3D%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7D%5Ctilde%7Bp%7D%28%5Cmathbf%7Bv%7D%29%3Dp%28%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{p}(&#92;mathbf{v})=&#92;sum&#92;limits_{&#92;mathbf{h}}&#92;tilde{p}(&#92;mathbf{v})=p(&#92;mathbf{v}) " title="&#92;tilde{p}(&#92;mathbf{v})=&#92;sum&#92;limits_{&#92;mathbf{h}}&#92;tilde{p}(&#92;mathbf{v})=p(&#92;mathbf{v}) " class="latex" /></span></strong></p>
<p style="text-align:left;">That is, just because we can estimate the Partition function well does not mean we know the probability distributions.</p>
<p style="text-align:left;">Why?   Define an arbitrary non-constant function <img src="https://s0.wp.com/latex.php?latex=K%28%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="K(&#92;mathbf{v}) " title="K(&#92;mathbf{v}) " class="latex" /> and write</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BZ%7D%28%5Cmathbf%7Bv%7D%29%3D%5Csum%5Climits_%7B%5Cmathbf%7Bv%7D%7De%5E%7B-%5BE%28%5Cmathbf%7Bv%7D%29%2BK%28%5Cmathbf%7BV%7D%29%5D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{Z}(&#92;mathbf{v})=&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-[E(&#92;mathbf{v})+K(&#92;mathbf{V})]} " title="&#92;tilde{Z}(&#92;mathbf{v})=&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-[E(&#92;mathbf{v})+K(&#92;mathbf{V})]} " class="latex" />.</p>
<p style="text-align:center;"><strong>K</strong> is for Kadanoff RG Transform, and <strong>ln K</strong> is the normalization.</p>
<p style="text-align:left;">We can now write an joint Energy <img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{v,h}) " title="E(&#92;mathbf{v,h}) " class="latex" /> with the same Partition function as our RBM <img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bh%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{h}) " title="E(&#92;mathbf{h}) " class="latex" />, but with completely different joint probability distributions.  Let</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bv%2Ch%7D%29%3DE%28%5Cmathbf%7Bv%7D%29%2BE%28%5Cmathbf%7Bh%7D%29%2BK%28%5Cmathbf%7Bv%7D%29%2Bln%5C%3BK%28%5Cmathbf%7Bv%7D%29%C2%A0&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{v,h})=E(&#92;mathbf{v})+E(&#92;mathbf{h})+K(&#92;mathbf{v})+ln&#92;;K(&#92;mathbf{v}) " title="E(&#92;mathbf{v,h})=E(&#92;mathbf{v})+E(&#92;mathbf{h})+K(&#92;mathbf{v})+ln&#92;;K(&#92;mathbf{v}) " class="latex" /></p>
<p style="text-align:left;">Notice what we are actually doing.  We use the K matrix to define the RBM joint Energy function.  In RBM theory, we restrict <img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bv%2Ch%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{v,h}) " title="E(&#92;mathbf{v,h}) " class="latex" /> to a quadratic form, and use variational procedure to learn the weights , thereby learning K.</p>
<p style="text-align:left;">In <a href="https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/" target="_blank">a VRG approach</a>, we have the additional constraint that we restrict the form of K to satisfy constraints on it&#8217;s partition function, or, really, how the Energy function is normalized.  Hence the name &#8216;<em>Renormalization.</em>&#8216;   This is similar, in spirit, but not necessarily in form, to how the RBM training regularizes the weights (above).</p>
<p style="text-align:left;">Write the total, or renormalized, <strong>Z</strong> as</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Z%5E%7BRG%7D%28%5Cmathbf%7Bv%2Ch%7D%29%3DZ_%7Btot%7D%28%5Cmathbf%7Bv%2Ch%7D%29%3D%5Csum%5Climits_%7B%5Cmathbf%7Bv%2Ch%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%2Ch%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z^{RG}(&#92;mathbf{v,h})=Z_{tot}(&#92;mathbf{v,h})=&#92;sum&#92;limits_{&#92;mathbf{v,h}}e^{-E(&#92;mathbf{v,h})} " title="Z^{RG}(&#92;mathbf{v,h})=Z_{tot}(&#92;mathbf{v,h})=&#92;sum&#92;limits_{&#92;mathbf{v,h}}e^{-E(&#92;mathbf{v,h})} " class="latex" /></p>
<p style="text-align:left;">Expanding the Energy function explicitly, we have</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%3D%5Cdfrac%7B1%7D%7B%5Ctilde%7BZ%7D%28%5Cmathbf%7Bv%7D%29%7D%5Csum%5Climits_%7B%5Cmathbf%7Bv%2Ch%7D%7De%5E%7B-%5BE%28%5Cmathbf%7Bv%7D%29%2BE%28%5Cmathbf%7Bh%7D%29%2BK%28%5Cmathbf%7Bv%7D%29%5D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}&#92;sum&#92;limits_{&#92;mathbf{v,h}}e^{-[E(&#92;mathbf{v})+E(&#92;mathbf{h})+K(&#92;mathbf{v})]} " title="=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}&#92;sum&#92;limits_{&#92;mathbf{v,h}}e^{-[E(&#92;mathbf{v})+E(&#92;mathbf{h})+K(&#92;mathbf{v})]} " class="latex" /></p>
<p style="text-align:left;">where the Kadanoff normalization factor <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BZ%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{Z} " title="&#92;tilde{Z} " class="latex" /> appears now the denominator.</p>
<p style="text-align:left;">We can can break the double sum into sums over <strong>v </strong>and <strong>h</strong></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%3D%5Cdfrac%7B1%7D%7B%5Ctilde%7BZ%7D%28%5Cmathbf%7Bv%7D%29%7D%5Csum%5Climits_%7B%5Cmathbf%7Bv%7D%7De%5E%7B-%5BE%28%5Cmathbf%7Bv%7D%29%2BK%28%5Cmathbf%7Bv%7D%29%5D%7D%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bh%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-[E(&#92;mathbf{v})+K(&#92;mathbf{v})]}&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} " title="=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-[E(&#92;mathbf{v})+K(&#92;mathbf{v})]}&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} " class="latex" /></p>
<p style="text-align:left;">Identify <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BZ%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{Z} " title="&#92;tilde{Z} " class="latex" /> in the numerator</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%3D%5Cdfrac%7B1%7D%7B%5Ctilde%7BZ%7D%28%5Cmathbf%7Bv%7D%29%7D%5Ctilde%7BZ%7D%28%5Cmathbf%7Bv%7D%29%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bh%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}&#92;tilde{Z}(&#92;mathbf{v})&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} " title="=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}&#92;tilde{Z}(&#92;mathbf{v})&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} " class="latex" /></p>
<p style="text-align:left;">which factors out, giving a very simple expression in <strong>h</strong></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%3D%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bh%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="=&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} " title="=&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} " class="latex" /></p>
<p style="text-align:left;">In the technical proof in the paper, the idea is that since <strong>h</strong> is just a dummy variable, we can replace <strong>h</strong> with <b>v.  </b>We have to be careful here since this seems to only applies to the case where we have the same number of hidden and visible units&#8211;a rare case.  In an earlier post on VRG, I explain more clearly how to construct an RG transform for RBMs.  Still,  the paper is presenting a counterargument for arguments sake, so, following the argument in the paper,  let&#8217;s say</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bh%7D%29%7D+%3D%5Csum%5Climits_%7B%5Cmathbf%7Bv%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} =&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-E(&#92;mathbf{v})} " title="&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})} =&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-E(&#92;mathbf{v})} " class="latex" /></p>
<p>This is like saying we constrain the Free Energy at each layer to be the same.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cln%5C%3B%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bh%7D%29%7D%5Capprox%5Cln%5C%3B%5Csum%5Climits_%7B%5Cmathbf%7Bv%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})}&#92;approx&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-E(&#92;mathbf{v})} " title="&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{h})}&#92;approx&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{v}}e^{-E(&#92;mathbf{v})} " class="latex" /></p>
<p><span style="color:#000000;">This is also another kind of <strong><span style="color:#0000ff;"><a style="color:#0000ff;" href="https://arxiv.org/pdf/1607.06450v1.pdf" target="_blank">Layer Normalization</a></span></strong>&#8211;a very popular method for modern Deep Learning methods these days.</span></p>
<p style="text-align:left;">So, by construction, the renormalized and data Partition functions are identical</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Z%5E%7BRG%7D%28%5Cmathbf%7Bv%2Ch%7D%29+%3DZ%28%5Cmathbf%7Bv%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Z^{RG}(&#92;mathbf{v,h}) =Z(&#92;mathbf{v}) " title="Z^{RG}(&#92;mathbf{v,h}) =Z(&#92;mathbf{v}) " class="latex" /></p>
<p style="text-align:center;"><span style="color:#008000;"><strong><em>The goal of Renormalization Group theory is to redefine the Energy function on a difference scale, while retaining the macroscopic observables.</em></strong></span></p>
<p style="text-align:left;">But , and apparently this has been misstated in some ML papers and books, <strong>the marginalized probabilities can be different</strong> !</p>
<p style="text-align:left;">To get the marginals, let&#8217;s integrate out <em>only</em> the <strong>h</strong> variables</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7Bp%7D%28%5Cmathbf%7Bv%7D%29%3D%5Cdfrac%7B1%7D%7BZ%5E%7BRG%7D%28%5Cmathbf%7Bv%2Ch%7D%29%7D%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-E%28%5Cmathbf%7Bv%2Ch%7D%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{p}(&#92;mathbf{v})=&#92;dfrac{1}{Z^{RG}(&#92;mathbf{v,h})}&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v,h})} " title="&#92;tilde{p}(&#92;mathbf{v})=&#92;dfrac{1}{Z^{RG}(&#92;mathbf{v,h})}&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-E(&#92;mathbf{v,h})} " class="latex" /></p>
<p style="text-align:left;">Looking above, we can write this in terms of <strong>K </strong>and its normalization <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BZ%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{Z} " title="&#92;tilde{Z} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7Bp%7D%28%5Cmathbf%7Bv%7D%29%3D%5Cdfrac%7B1%7D%7B%5Ctilde%7BZ%7D%28%5Cmathbf%7Bv%7D%29%7De%5E%7B-%5BE%28%5Cmathbf%7Bv%7D%29%2BK%28%5Cmathbf%7Bv%7D%29%5D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{p}(&#92;mathbf{v})=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}e^{-[E(&#92;mathbf{v})+K(&#92;mathbf{v})]} " title="&#92;tilde{p}(&#92;mathbf{v})=&#92;dfrac{1}{&#92;tilde{Z}(&#92;mathbf{v})}e^{-[E(&#92;mathbf{v})+K(&#92;mathbf{v})]} " class="latex" /></p>
<p style="text-align:left;">which implies</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7Bp%7D%28%5Cmathbf%7Bv%7D%29%5Cne+p%28%5Cmathbf%7Bv%7D%29+%C2%A0&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{p}(&#92;mathbf{v})&#92;ne p(&#92;mathbf{v})  " title="&#92;tilde{p}(&#92;mathbf{v})&#92;ne p(&#92;mathbf{v})  " class="latex" /></p>
<hr />
<h4>So what is cheap about deep learning ?</h4>
<p>RBMs let us represent data using a smaller set of hidden features.  This is, <em>effectively</em>, Variational Renormalization Group algorithm,  in which we approximate the Partition function, at each step in the RBM learning procedure, without having to learn the underlying joining probability distribution.  And this is easier.  Cheaper.</p>
<p>In other words, <em>Deep Learning is not Statistics.  It is more like Statistical Mechanics.  </em></p>
<p>And the hope is that we can learn from this old scientific field &#8212; which is foundational to chemistry and physics &#8212; to improve our deep learning models.</p>
<h3>Post-Post Comments</h3>
<p>Shortly after this paper came out, <a href="http://arxiv.org/pdf/1609.03541v1.pdf" target="_blank">Comment on “Why does deep and cheap learning work so well?”</a>that the proof in the Appended is indeed wrong&#8211;as I suspected and pointed out above.</p>
<p>It is noted that the point of the RG theory is to preserve the Free Energy form one layer to another, and, in VRG, this is expressed as a trace condition on the Transfer operator</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7B%5Cmathbf%7Bh%7D%7De%5E%7B-T%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%7D%3D1+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-T(&#92;mathbf{v},&#92;mathbf{h})}=1 " title="&#92;sum&#92;limits_{&#92;mathbf{h}}e^{-T(&#92;mathbf{v},&#92;mathbf{h})}=1 " class="latex" /></p>
<p style="text-align:left;">where <img src="https://s0.wp.com/latex.php?latex=-T%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3DE%28%5Cmathbf%7Bv%7D%29%2BK%28%5Cmathbf%7Bv%7D%29%2Bln%5Ctilde%7BZ%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="-T(&#92;mathbf{v},&#92;mathbf{h})=E(&#92;mathbf{v})+K(&#92;mathbf{v})+ln&#92;tilde{Z} " title="-T(&#92;mathbf{v},&#92;mathbf{h})=E(&#92;mathbf{v})+K(&#92;mathbf{v})+ln&#92;tilde{Z} " class="latex" /></p>
<p style="text-align:left;">It is, however, technically possible to preserve the Free Energy and not preserve the trace condition.  Indeed, because <img src="https://s0.wp.com/latex.php?latex=K%28%5Cmathbf%7By%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="K(&#92;mathbf{y}) " title="K(&#92;mathbf{y}) " class="latex" /> is not-constant, thereby violating the trace condition.</p>
<p style="text-align:left;">From this bloggers perspective, the idea of preserving Free Energy, via either a trace condition, or, say, by layer normalization, is the import point.  And this may mean to only approximately satisfy the trace condition.</p>
<p style="text-align:left;">In Quantum Chemistry, there is a similar requirement, referred to as a Size-Consistency and/ or Size-Extensivity condition.  And these requirements proven essential to obtaining highly accurate, <em>ab initio</em> solutions of the molecular electronic Schrodinger equation&#8211;whether implemented exactly or approximately.</p>
<p style="text-align:left;">And, I suspect, a similar argument, at least in spirit if not in proof, is at play in Deep Learning.</p>
<p style="text-align:left;">Please chime in our my <a href="https://charlesmartin14.wordpress.com/2016/09/10/on-cheap-learning-partition-functions-and-rbms/" target="_blank">YouTube Community Channel</a></p>
<p style="text-align:left;">see also: https://m.reddit.com/r/MachineLearning/comments/4zbr2k/what_is_your_opinion_why_is_the_concept_of/)</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/charlesmartin14.wordpress.com/9023/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/charlesmartin14.wordpress.com/9023/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=calculatedcontent.com&#038;blog=32496692&#038;post=9023&#038;subd=charlesmartin14&#038;ref=&#038;feed=1" width="1" height="1" />
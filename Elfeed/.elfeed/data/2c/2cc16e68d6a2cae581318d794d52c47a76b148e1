<p style="text-align:left;"> ?Deep Learning is presented as <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf" target="_blank">Energy-Based Learning</a></p>
<p>Indeed, we train a neural network by running BackProp, thereby minimizing the model error&#8211;which is like minimizing an Energy.</p>
<figure data-shortcode="caption" id="attachment_10272" style="width: 310px" class="wp-caption aligncenter"><a href="http://www.kdnuggets.com/2015/06/why-does-deep-learning-work.html" rel="attachment wp-att-10272"><img data-attachment-id="10272" data-permalink="https://calculatedcontent.com/2017/02/24/why-deep-learning-works-3-backprop-minimizes-the-free-energy/screen-shot-2017-02-05-at-8-26-58-pm/" data-orig-file="https://charlesmartin14.files.wordpress.com/2017/02/screen-shot-2017-02-05-at-8-26-58-pm.png" data-orig-size="950,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2017-02-05-at-8-26-58-pm" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2017/02/screen-shot-2017-02-05-at-8-26-58-pm.png?w=300&#038;h=206" data-large-file="https://charlesmartin14.files.wordpress.com/2017/02/screen-shot-2017-02-05-at-8-26-58-pm.png?w=840" class="wp-image-10272 size-medium" src="https://charlesmartin14.files.wordpress.com/2017/02/screen-shot-2017-02-05-at-8-26-58-pm.png?w=300&#038;h=206" alt="screen-shot-2017-02-05-at-8-26-58-pm" width="300" height="206" srcset="https://charlesmartin14.files.wordpress.com/2017/02/screen-shot-2017-02-05-at-8-26-58-pm.png?w=300&amp;h=206 300w, https://charlesmartin14.files.wordpress.com/2017/02/screen-shot-2017-02-05-at-8-26-58-pm.png?w=600&amp;h=412 600w, https://charlesmartin14.files.wordpress.com/2017/02/screen-shot-2017-02-05-at-8-26-58-pm.png?w=150&amp;h=103 150w" sizes="(max-width: 300px) 100vw, 300px" /></a><figcaption class="wp-caption-text">Why Deep Learning Works ? Possible Free Energy Landscapes from Physical Chemistry</figcaption></figure>
<p>But what is this Energy ? <span style="font-size:1.0625rem;">Deep Learning (DL) Energy functions look nothing like a typical chemistry or physics Energy. Here, we have Free Energy landscapes, frequently which form <a href="https://en.wikipedia.org/wiki/Folding_funnel">funneled landscapes</a>&#8211;a trade off between energetic and entropic effects.</span></p>
<p><a href="https://arxiv.org/abs/1412.0233">And yet, some researchers, like LeCun, have even compared Neural Network Energies functions to spin glass Hamiltonians</a>.  To me, this seems off.</p>
<p>The confusion arises from assuming Deep Learning is a non-convex optimization problem that looks similar to the<em> zero-Temperature</em> Energy Landscapes from spin glass theory.</p>
<p>I present a different view.  I believe Deep Learning is really optimizing an effective Free Energy function. And this has profound implications on <a href="https://www.youtube.com/watch?v=fHZZgfVgC8U">Why Deep Learning Works.</a></p>
<p style="text-align:center;"><em>This post will attempt to relate recent ideas in RBM inference to Backprop, and argue that Backprop is minimizing a dynamic, temperature dependent, ruggedly convex, effective Free Energy landscape.</em></p>
<p>This is a fairly long post, but at least is basic review.  I try to present these ideas in a semi-pedagogic way, to the extent I can in a blog post, discussing both RBMs, MLPs, Free Energies, and all that entails.</p>
<h4>BackProp</h4>
<p>The Backprop algorithm lets us train a model directly on our data (X) by minimizing the predicted error <img src="https://s0.wp.com/latex.php?latex=E_%7Btrain%7D%28%5Cmathbf%7B%5Ctheta%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="E_{train}(&#92;mathbf{&#92;theta}) " title="E_{train}(&#92;mathbf{&#92;theta}) " class="latex" />, where the parameter set <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Ctheta%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;theta} " title="&#92;mathbf{&#92;theta} " class="latex" /> includes the weights <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BW%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{W}) " title="(&#92;mathbf{W}) " class="latex" />, biases <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7Bb%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{b}) " title="(&#92;mathbf{b}) " class="latex" />, and activations <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7Ba%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{a}) " title="(&#92;mathbf{a}) " class="latex" /> of the network.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctheta%3D%5C%7B%5Cmathbf%7BW%7D%2C%5Cmathbf%7Bb%7D%2C%5Cmathbf%7Ba%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{b},&#92;mathbf{a}&#92;} " title="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{b},&#92;mathbf{a}&#92;} " class="latex" />.</p>
<p style="text-align:left;">Let&#8217;s write</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=E_%7Btrain%7D%28%5Cmathbf%7B%5Ctheta%7D%29%3D%5Cunderset%7B%5Cmathbf%7Bx%7D_%7B%5Cmu%7D%5Cin%5Cmathbf%7BX%7D%7D%7B%5Csum%7Derr%28%5Cmathbf%7Bx%7D_%7B%5Cmu%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="E_{train}(&#92;mathbf{&#92;theta})=&#92;underset{&#92;mathbf{x}_{&#92;mu}&#92;in&#92;mathbf{X}}{&#92;sum}err(&#92;mathbf{x}_{&#92;mu}) " title="E_{train}(&#92;mathbf{&#92;theta})=&#92;underset{&#92;mathbf{x}_{&#92;mu}&#92;in&#92;mathbf{X}}{&#92;sum}err(&#92;mathbf{x}_{&#92;mu}) " class="latex" />,</p>
<p style="text-align:left;">where the error <img src="https://s0.wp.com/latex.php?latex=err%28x%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="err(x)  " title="err(x)  " class="latex" /> could be a mean squared error (MSE), cross entropy, etc. For example, in simple regression, we can minimize the MSE</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=E_%7Btrain%7D%28%5Ctheta%29%3D%5Csum_%7B%5Cmu%7D%28y_%7B%5Cmu%7D-f%28%5Cmathbf%7Bx%7D_%7B%5Cmu%7D%2C%5Ctheta%29%29%5E%7B2%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="E_{train}(&#92;theta)=&#92;sum_{&#92;mu}(y_{&#92;mu}-f(&#92;mathbf{x}_{&#92;mu},&#92;theta))^{2} " title="E_{train}(&#92;theta)=&#92;sum_{&#92;mu}(y_{&#92;mu}-f(&#92;mathbf{x}_{&#92;mu},&#92;theta))^{2} " class="latex" />,</p>
<p style="text-align:left;">whereas for multi-class classification, we might minimize a <a href="https://keras.io/metrics/#categorical_crossentropy">categorical cross entropy</a></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=E_%7Btrain%7D%28%5Ctheta%29%3D%5Csum_%7B%5Cmu%7D%28y_%7B%5Cmu%7D%5Cln+f_%7B%5Cmu%7D%2B%281-y_%7B%5Cmu%7D%29%5Cln+f_%7B%5Cmu%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="E_{train}(&#92;theta)=&#92;sum_{&#92;mu}(y_{&#92;mu}&#92;ln f_{&#92;mu}+(1-y_{&#92;mu})&#92;ln f_{&#92;mu}) " title="E_{train}(&#92;theta)=&#92;sum_{&#92;mu}(y_{&#92;mu}&#92;ln f_{&#92;mu}+(1-y_{&#92;mu})&#92;ln f_{&#92;mu}) " class="latex" /></p>
<p style="text-align:left;">where <img src="https://s0.wp.com/latex.php?latex=y_%7B%5Cmu%7D+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="y_{&#92;mu}  " title="y_{&#92;mu}  " class="latex" /> are the labels  and <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Cmu%7D%3Df%28%5Cmathbf%7Bx%7D_%7B%5Cmu%7D%2C%5Ctheta%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f_{&#92;mu}=f(&#92;mathbf{x}_{&#92;mu},&#92;theta)  " title="f_{&#92;mu}=f(&#92;mathbf{x}_{&#92;mu},&#92;theta)  " class="latex" /> is the network output for each training instance <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mu  " title="&#92;mu  " class="latex" />.</p>
<p style="text-align:left;">Notice that <img src="https://s0.wp.com/latex.php?latex=err%28%5Cmathbf%7Bx%7D_%7B%5Cmu%7D%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="err(&#92;mathbf{x}_{&#92;mu})  " title="err(&#92;mathbf{x}_{&#92;mu})  " class="latex" /> is the training error for instance <img src="https://s0.wp.com/latex.php?latex=%5Cmu+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mu " title="&#92;mu " class="latex" />, not a test or holdout error.  Notice that, unlike an Support Vector Machine (SVM) or Logistic Regression (LR), we don&#8217;t use <a href="http://scikit-learn.org/stable/modules/cross_validation.html">Cross Validation (CV)</a> during training.   We simply minimize the training error&#8211; whatever that is.</p>
<p style="text-align:left;">Of course, we can adjust the network parameters, regularization, etc, to tune the architecture of the network.  Although it appears that <span class="s1"><a href="https://arxiv.org/abs/1611.03530"><em>Understanding deep learning requires rethinking generalization</em></a>.</span></p>
<p style="text-align:left;"><span style="font-size:1.0625rem;">At this point, many people say that BackProp leads to a complex, non-convex optimization problem; IMHO, this is naive.</span></p>
<p style="text-align:center;"><span style="color:#00ccff;"><em><strong>It has been known for 20 years that Deep Learning does not suffer from local minima.</strong></em></span></p>
<p>Anyone who thinks it does has never read a <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">research paper</a> or<a href="https://books.google.com/books?id=Br33IRC3PkQC&amp;pg=PA299&amp;lpg=PA299&amp;dq=section+6.4.4.+pattern+classification&amp;source=bl&amp;ots=2wCVIw9aKu&amp;sig=VVTS68yKoFICQneeZ3MfrcSfNFY&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwimk_qV3cvNAhUU3GMKHa6aBVYQ6AEIHjAB#v=onepage&amp;q=section%206.4.4.%20pattern%20classification&amp;f=false"> book on neural networks.</a>  So what we really would like to know is, <a href="http://www.kdnuggets.com/2016/07/deep-learning-networks-scale.html">Why does Deep Learning Scale ?</a>  Or, maybe, why does it work at all ?!</p>
<p style="text-align:left;">To implement Backprop, we take derivatives <img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7DE_%7Btrain%7D%28%5Ctheta%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta}E_{train}(&#92;theta)  " title="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta}E_{train}(&#92;theta)  " class="latex" /> and apply the the chain rule to the network outputs <img src="https://s0.wp.com/latex.php?latex=f%28%5Cmathbf%7Bx%7D_%7B%5Cmu%7D%2C%5Ctheta%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f(&#92;mathbf{x}_{&#92;mu},&#92;theta)  " title="f(&#92;mathbf{x}_{&#92;mu},&#92;theta)  " class="latex" />, applying it <em>layer-by-layer</em>.</p>
<h4 style="text-align:left;">Layers and Activations</h4>
<p>Let&#8217;s take a closer look at the layers and activations.  Consider a simple 1 layer net:</p>
<p><img class=" aligncenter" src="https://deeplearning4j.org/img/multiple_inputs_RBM.png" alt="" width="423" height="305" /></p>
<p>The Hidden activations <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Ba%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{a} " title="&#92;mathbf{a} " class="latex" /> are thought to mimic the function of actual neurons, and are computed by applying an activation function <img src="https://s0.wp.com/latex.php?latex=f%28%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f() " title="f() " class="latex" />,  to a linear Energy function <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7Bx%7D%2B%5Cmathbf%7Bb%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{W}^{T}&#92;mathbf{x}+&#92;mathbf{b} " title="&#92;mathbf{W}^{T}&#92;mathbf{x}+&#92;mathbf{b} " class="latex" />,</p>
<p><img class=" aligncenter" src="https://i1.wp.com/www.kdnuggets.com/wp-content/uploads/neuron.jpg" alt="" width="809" height="240" /></p>
<p>Indeed, the sigmoid activation function <img src="https://s0.wp.com/latex.php?latex=%5Csigma%28%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;sigma() " title="&#92;sigma() " class="latex" /> was first proposed in 1968 by <a href="https://www.youtube.com/watch?v=7Ht9k824nWA&amp;t=163s">Jack Cowan at the University of Chicago </a>, still used today in <a href="https://mathematical-neuroscience.springeropen.com/articles/10.1186/s13408-015-0034-5">models of neural dynamics</a></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Ba%7D%3D%5Csigma%28%5Cmathbf%7BWx%7D%2B%5Cmathbf%7Bb%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{a}=&#92;sigma(&#92;mathbf{Wx}+&#92;mathbf{b}) " title="&#92;mathbf{a}=&#92;sigma(&#92;mathbf{Wx}+&#92;mathbf{b}) " class="latex" /></p>
<p style="text-align:left;">Moreover, Cowan pioneered using <a href="http://thesciencenetwork.org/programs/raw-science/statistical-mechanics-of-the-neocortex">Statistical Mechanics to study the Neocortex</a>.</p>
<p style="text-align:center;"><span style="color:#33cccc;"><strong><em>And we will need a little Stat Mech to explain what our Energy functions are..but just a little.</em></strong></span></p>
<h4 style="text-align:left;">Sigmoid Activations and Statistical Mechanics</h4>
<p style="text-align:left;">While it seems we are simply proposing an arbitrary activation function, we can, in fact, derive the appearance of sigmoid activations&#8211;at least when performing inference on a single layer (mean field) Restricted Boltzmann Machine (RBM).</p>
<p style="text-align:center;"><a href="https://www.youtube.com/watch?v=lekCh_i32iE">Hugo Larochelle has derived the sigmoid activations nicely for an RBM</a>.</p>
<p style="text-align:left;">Given the (total) RBM Energy function</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3D%5Cmathbf%7Ba%7D%5E%7BT%7D%5Cmathbf%7Bv%7D%2B%5Cmathbf%7Bb%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%2B%5Cmathbf%7BvW%7D%5E%7BT%7D%5Cmathbf%7Bh%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{v},&#92;mathbf{h})=&#92;mathbf{a}^{T}&#92;mathbf{v}+&#92;mathbf{b}^{T}&#92;mathbf{h}+&#92;mathbf{vW}^{T}&#92;mathbf{h} " title="E(&#92;mathbf{v},&#92;mathbf{h})=&#92;mathbf{a}^{T}&#92;mathbf{v}+&#92;mathbf{b}^{T}&#92;mathbf{h}+&#92;mathbf{vW}^{T}&#92;mathbf{h} " class="latex" /></p>
<p style="text-align:left;">The log Energy is an un-normalized probability, such that</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=P%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3D%5Cdfrac%7B1%7D%7BZ%7De%5E%7B-%5Cbeta+E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="P(&#92;mathbf{v},&#92;mathbf{h})=&#92;dfrac{1}{Z}e^{-&#92;beta E(&#92;mathbf{v},&#92;mathbf{h})} " title="P(&#92;mathbf{v},&#92;mathbf{h})=&#92;dfrac{1}{Z}e^{-&#92;beta E(&#92;mathbf{v},&#92;mathbf{h})} " class="latex" /></p>
<p style="text-align:left;">Where the normalization factor, Z, is an object from statistical mechanics called the (total) partition function Z</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3D%5Cunderset%7B%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%7D%7B%5Csum%7De%5E%7B-%5Cbeta+E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="Z(&#92;mathbf{v},&#92;mathbf{h})=&#92;underset{&#92;mathbf{v},&#92;mathbf{h}}{&#92;sum}e^{-&#92;beta E(&#92;mathbf{v},&#92;mathbf{h})} " title="Z(&#92;mathbf{v},&#92;mathbf{h})=&#92;underset{&#92;mathbf{v},&#92;mathbf{h}}{&#92;sum}e^{-&#92;beta E(&#92;mathbf{v},&#92;mathbf{h})} " class="latex" /></p>
<p style="text-align:left;">and <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%3D%5Cdfrac%7B1%7D%7BT%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta=&#92;dfrac{1}{T} " title="&#92;beta=&#92;dfrac{1}{T} " class="latex" /> is an inverse Temperature.  In modern machine learning, we implicitly set <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%3D1+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta=1 " title="&#92;beta=1 " class="latex" />.</p>
<p style="text-align:left;">Following Larochelle, we can factor <img src="https://s0.wp.com/latex.php?latex=P%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="P(&#92;mathbf{v},&#92;mathbf{h}) " title="P(&#92;mathbf{v},&#92;mathbf{h}) " class="latex" /> by explicitly writing <img src="https://s0.wp.com/latex.php?latex=Z%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="Z(&#92;mathbf{v},&#92;mathbf{h}) " title="Z(&#92;mathbf{v},&#92;mathbf{h}) " class="latex" /> in terms of sums over the binary hidden activations <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%3D0%7C1+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="h_{i}=0|1 " title="h_{i}=0|1 " class="latex" />.  This lets us write the conditional probabilities, for each individual neuron as</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28v_%7Bi%7D%7Ch%3D1%29%3D%5Csigma%28%5Csum_%7Bj%7DW_%7Bi%2Cj%7Dh_%7Bj%7D%2Ba_%7Bj%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(v_{i}|h=1)=&#92;sigma(&#92;sum_{j}W_{i,j}h_{j}+a_{j}) " title="p(v_{i}|h=1)=&#92;sigma(&#92;sum_{j}W_{i,j}h_{j}+a_{j}) " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28h_%7Bj%7D%7Cv%3D1%29%3D%5Csigma%28%5Csum_%7Bi%7DW_%7Bi%2Cj%7Dv_%7Bi%7D%2Bb_%7Bi%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(h_{j}|v=1)=&#92;sigma(&#92;sum_{i}W_{i,j}v_{i}+b_{i}) " title="p(h_{j}|v=1)=&#92;sigma(&#92;sum_{i}W_{i,j}v_{i}+b_{i}) " class="latex" />.</p>
<p style="text-align:left;">We note that, this formulation was not obvious, and <a href="https://calculatedcontent.com/2017/01/05/foundations-mean-field-boltzmann-machines-1987/">early work on RBMs</a> used methods from statistical field theory to get this result.</p>
<h4 style="text-align:left;">RBM Training</h4>
<p>We use <img src="https://s0.wp.com/latex.php?latex=p%28v_%7Bi%7D%7Ch%3D1%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(v_{i}|h=1) " title="p(v_{i}|h=1) " class="latex" /> and  <img src="https://s0.wp.com/latex.php?latex=p%28h_%7Bj%7D%7Cv%3D1%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(h_{j}|v=1) " title="p(h_{j}|v=1) " class="latex" /> in Contrastive Divergence (CD) or other solvers as part of the Gibbs Sampling step for (unsupervised) RBM inference.</p>
<p>CD has been a puzzling algorithm to understand.  When first proposed, it was unclear <a href="http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf">what optimization problem is CD solving</a>?  Indeed, Hinton is to have said</p>
<p><a href="https://www.quora.com/What-is-contrastive-divergence">&#8220;&#8216;the Microsoft Algorithm:&#8217; It asks, &#8216;where do you want to go today?&#8217; and then doesn&#8217;t let you go there.&#8221;</a></p>
<p>Specifically, we run several epochs of:</p>
<ol>
<li>n steps of Gibbs sampling, or some other equilibration method, to set the neuron activations.</li>
<li>some form of gradient descent <img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta} " title="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta} " class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%3D%5C%7B%5Cmathbf%7BW%7D%2C%5Cmathbf%7Bb%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{b}&#92;} " title="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{b}&#92;} " class="latex" /></li>
</ol>
<p style="text-align:center;"><strong><span style="color:#0000ff;"><em>We will see below that we can cast RBM inference as directly minimizing a Free Energy&#8211;something that will prove very useful to related RBMs to MLPs </em></span></strong></p>
<h4 style="text-align:left;">Energies and Activations</h4>
<p>The sigmoid, and tanh,  are an old-fashioned activation(s); today we may prefer to use ReLUs (and Leaky ReLUs).</p>
<figure style="width: 910px" class="wp-caption aligncenter"><img class="" src="https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-08-at-11-53-41-am.png?w=910&#038;h=275" alt="Common Activation Functions" width="910" height="275" /><figcaption class="wp-caption-text">Common Activation Functions</figcaption></figure>
<p>The sigmoid itself was, at first, just an approximation to the heavyside step function used in neuron models.  But the presence of sigmoid activations in the total Energy suggests, <em>at least to me,</em> that Deep Learning Energy functions are more than just random (Morse) functions.</p>
<p>RBMs are a special case of unsupervised nets that still use stochastic sampling. In supervised nets, like MLPs and CNNs (and in unsupervised Autoencoders like VAEs), we use Backprop.  But the activations are not conditional probabilities.  Let&#8217;s look in detail:</p>
<h5>MLP outputs</h5>
<p>Consider a MultiLayer Perceptron, with 1 Hidden layer, and 1 output node</p>
<p><img class=" aligncenter" src="https://qph.ec.quoracdn.net/main-qimg-050d12c5da82f4f97fdd942d7777b8e4" alt="" width="340" height="210" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f%5E%7B%5Cmu%7D_%7BMLP%7D%3D%5Csigma%28%5Cunderset%7Bh%5Cin%5Cmathbf%7Bh%7D%7D%7B%5Csum%7D%5Cmathbf%7Ba%7D_%7Bh%7D%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f^{&#92;mu}_{MLP}=&#92;sigma(&#92;underset{h&#92;in&#92;mathbf{h}}{&#92;sum}&#92;mathbf{a}_{h})  " title="f^{&#92;mu}_{MLP}=&#92;sigma(&#92;underset{h&#92;in&#92;mathbf{h}}{&#92;sum}&#92;mathbf{a}_{h})  " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Ba%7D_%7Bh%7D%3D%5Csigma%28%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7Bv%7D%2B%5Cmathbf%7Bb%7D%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{a}_{h}=&#92;sigma(&#92;mathbf{W}^{T}&#92;mathbf{v}+&#92;mathbf{b})  " title="&#92;mathbf{a}_{h}=&#92;sigma(&#92;mathbf{W}^{T}&#92;mathbf{v}+&#92;mathbf{b})  " class="latex" /></p>
<p style="text-align:left;">where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D%3D%5Cmathbf%7Bx%7D%5E%7B%5Cmu%7D+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v}=&#92;mathbf{x}^{&#92;mu}  " title="&#92;mathbf{v}=&#92;mathbf{x}^{&#92;mu}  " class="latex" />  for each data point, leading to<span style="color:#0000ff;"><em> the layer output</em></span></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=g%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%3D%5Csigma%28%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7Bx%7D%5E%7B%5Cmu%7D%2B%5Cmathbf%7Bb%7D%29%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="g^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;mathbf{W}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b}))  " title="g^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;mathbf{W}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b}))  " class="latex" /></p>
<p style="text-align:left;">and total MLP output</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%3D%5Csigma%28%5Csum+g%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%29%3D%5Csigma%28%5Csum+%5Csigma%28%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7Bx%7D%5E%7B%5Cmu%7D%2B%5Cmathbf%7Bb%7D%29%29%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;sum g^{&#92;mu}_{MLP}(&#92;theta))=&#92;sigma(&#92;sum &#92;sigma(&#92;mathbf{W}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b})))  " title="f^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;sum g^{&#92;mu}_{MLP}(&#92;theta))=&#92;sigma(&#92;sum &#92;sigma(&#92;mathbf{W}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b})))  " class="latex" /></p>
<p style="text-align:left;">where <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%3D%5C%7B%5Cmathbf%7BW%7D%2C%5Cmathbf%7Bb%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{b}&#92;} " title="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{b}&#92;} " class="latex" />.</p>
<p style="text-align:left;">If we add a second layer, we have the iterated layer output:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=g%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%27%29%3D%5Csigma%28%5Cmathbf%7BW%7D%5E%7BT%7D%28%5Csigma%28%5Cmathbf%7BW%27%7D%5E%7BT%7D%5Cmathbf%7Bx%7D%5E%7B%5Cmu%7D%2B%5Cmathbf%7Bb%27%7D%29%29%2B%5Cmathbf%7Bb%7D%29%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="g^{&#92;mu}_{MLP}(&#92;theta&#039;)=&#92;sigma(&#92;mathbf{W}^{T}(&#92;sigma(&#92;mathbf{W&#039;}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b&#039;}))+&#92;mathbf{b})) " title="g^{&#92;mu}_{MLP}(&#92;theta&#039;)=&#92;sigma(&#92;mathbf{W}^{T}(&#92;sigma(&#92;mathbf{W&#039;}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b&#039;}))+&#92;mathbf{b})) " class="latex" /></p>
<p style="text-align:left;">where <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%27%3D%5C%7B%5Cmathbf%7BW%7D%2C%5Cmathbf%7BW%27%7D%2C%5Cmathbf%7Bb%7D%2C%5Cmathbf%7Bb%27%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;theta&#039;=&#92;{&#92;mathbf{W},&#92;mathbf{W&#039;},&#92;mathbf{b},&#92;mathbf{b&#039;}&#92;} " title="&#92;theta&#039;=&#92;{&#92;mathbf{W},&#92;mathbf{W&#039;},&#92;mathbf{b},&#92;mathbf{b&#039;}&#92;} " class="latex" />.</p>
<p style="text-align:left;">The final MLP output function has a similar form:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%3D%5Csigma%28%5Csum%5Csigma%28g%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%29%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;sum&#92;sigma(g^{&#92;mu}_{MLP}(&#92;theta)))  " title="f^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;sum&#92;sigma(g^{&#92;mu}_{MLP}(&#92;theta)))  " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%3D%5Csigma%28%5Csum%5Csigma%28%5Cmathbf%7BW%7D%5E%7BT%7D%28%5Csigma%28%5Cmathbf%7BW%27%7D%5E%7BT%7D%5Cmathbf%7Bx%7D%5E%7B%5Cmu%7D%2B%5Cmathbf%7Bb%27%7D%29%29%2B%5Cmathbf%7Bb%7D%29%29%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;sum&#92;sigma(&#92;mathbf{W}^{T}(&#92;sigma(&#92;mathbf{W&#039;}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b&#039;}))+&#92;mathbf{b}))) " title="f^{&#92;mu}_{MLP}(&#92;theta)=&#92;sigma(&#92;sum&#92;sigma(&#92;mathbf{W}^{T}(&#92;sigma(&#92;mathbf{W&#039;}^{T}&#92;mathbf{x}^{&#92;mu}+&#92;mathbf{b&#039;}))+&#92;mathbf{b}))) " class="latex" /></p>
<p style="text-align:left;">So with a little bit of stat mech, we can derive the sigmoid activation function from a general energy function.  And we have activations it in RBMs as well as MLPs.</p>
<p style="text-align:center;"><span style="color:#ff6600;"><strong><em>So when we apply Backprop, what problem are we actually solving ?</em></strong></span></p>
<p style="text-align:left;">Are we simply finding a minima on random high dimensional manifold ?  Or can we say something more, given the special structure of these layers of activated energies ?</p>
<h4 style="text-align:left;">Backprop and Energy Minimization</h4>
<p>To train an MLP, we run several epochs of Backprop.   Backprop has 2 passes: forward and backward:</p>
<p><img class="aligncenter" src="https://i2.wp.com/backlinksforseo.com/wp-content/uploads/2016/12/backprop.png" alt="Backprop" width="444" height="231" /></p>
<ol>
<li><strong>Forward</strong>: Propagate the inputs <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cmathbf%7Bx%7D%5E%7B%5Cmu%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;{&#92;mathbf{x}^{&#92;mu}&#92;} " title="&#92;{&#92;mathbf{x}^{&#92;mu}&#92;} " class="latex" /> forward through the network, activating the neurons</li>
<li><strong>Backward</strong>: Propagate the errors <img src="https://s0.wp.com/latex.php?latex=%5C%7Berr%28%5Cmathbf%7Bx%7D%5E%7B%5Cmu%7D%29%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;{err(&#92;mathbf{x}^{&#92;mu})&#92;} " title="&#92;{err(&#92;mathbf{x}^{&#92;mu})&#92;} " class="latex" /> backward to compute the weight gradients <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5Cmathbf%7BW%7D+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;Delta&#92;mathbf{W}  " title="&#92;Delta&#92;mathbf{W}  " class="latex" /></li>
</ol>
<p>Each epoch usually runs small batches of inputs at time.  (And we may need to normalize the inputs and control the variances.  These details may be important for out analysis, and we will consider them in a later post).</p>
<p>After each pass, we update the weights, using something like an SGD step (or Adam, RMSProp, etc)</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%5Crightarrow%5Cmathbf%7BW%7D%2B%5Ceta%5CDelta%5Cmathbf%7BW%7D+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{W}&#92;rightarrow&#92;mathbf{W}+&#92;eta&#92;Delta&#92;mathbf{W}  " title="&#92;mathbf{W}&#92;rightarrow&#92;mathbf{W}+&#92;eta&#92;Delta&#92;mathbf{W}  " class="latex" /></p>
<p>For an MSE loss, we evaluate the partial derivatives over the Energy parameters <img src="https://s0.wp.com/latex.php?latex=%5Ctheta%3D%5C%7B%5Cmathbf%7BW%7D%2C%5Cmathbf%7BW%27%7D%2C%5Cmathbf%7Bb%7D%2C%5Cmathbf%7Bb%27%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{W&#039;},&#92;mathbf{b},&#92;mathbf{b&#039;}&#92;} " title="&#92;theta=&#92;{&#92;mathbf{W},&#92;mathbf{W&#039;},&#92;mathbf{b},&#92;mathbf{b&#039;}&#92;} " class="latex" />.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%5Cunderset%7B%5Cmu%7D%7B%5Csum%7D%28y%5E%7B%5Cmu%7D-f%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%29%5E%7B2%7D+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta}&#92;underset{&#92;mu}{&#92;sum}(y^{&#92;mu}-f^{&#92;mu}_{MLP}(&#92;theta))^{2}  " title="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta}&#92;underset{&#92;mu}{&#92;sum}(y^{&#92;mu}-f^{&#92;mu}_{MLP}(&#92;theta))^{2}  " class="latex" /></p>
<p>Backprop works by the chain rule, and given the special form of the activations, lets us transform the Energy derivatives into a sum of Energy gradients&#8211;layer by layer</p>
<p><img class="aligncenter" src="https://matthewmazur.files.wordpress.com/2015/03/nn-calculation.png?w=383&#038;h=309" alt="Backprop gradients" width="383" height="309" /></p>
<p style="text-align:left;">I won&#8217;t go into the details here; there are 1000 blogs on BackProp today (<em>which is amazing!</em>).  I will say&#8230;</p>
<p style="text-align:center;"><strong><em><span style="color:#800080;">Backprop couples the activation states of the neurons to the Energy parameter gradients through the cycle of forward-backward phases.</span></em></strong></p>
<p style="text-align:left;">In a crude sense, Backprop resembles our more familiar RBM training procedure, where we equilibrate to set the activations, and run gradient descent to set the weights. Here, I show a direct connection, and derive the MLP functional form directly from an RBM.</p>
<h4>From RBMs to MLPs</h4>
<p>I now consider the <strong>Backward</strong> phase, using the deterministic EMF RBM, as a starting point for understanding MLPs.</p>
<p><a href="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/">An earlier post</a> discusses the EMF RBM, from the context of chemical physics.  For a traditional machine learning perspective, <a href="http://www.mlsalt.eng.cam.ac.uk/foswiki/pub/Main/CurrentMPhils/Pawel_Budzianowski_8224891_assignsubmission_file_Budzianowski_Dissertation.pdf">see this thesis</a>.</p>
<p>In some sense, this is kind-of obvious. And yet, I have not seen a clear presentation of the ideas in this way.  I do rely upon new research, like the EMF RBM, although I also draw upon fundamental ideas from complex systems theory&#8211;something popular in my PhD studies, but which is perhaps ancient history now.</p>
<p style="text-align:center;"><span style="color:#0000ff;"><em>The goal is to relate RBMs, MLPs, and basic Stat Mech under single conceptual umbrella.</em></span></p>
<p>In the EMF approach, we see RBM inference as a sequence of deterministic annealing steps, from 1 quasi-equilibrium state to another, consisting of 2 steps for each epoch:</p>
<ol>
<li><strong>Forward:</strong> equilibrate the neuron activations by minimizing the TAP Free Energy</li>
<li><strong>Backward:</strong> compute weight gradients of the TAP Free Energy</li>
</ol>
<p>At the end of each epoch, we update the weights, with weight (temperature) constraints (i.e. reset the L1 or L2 norm).  BTW, it may not obvious that weight regularization is like a Temperature control; I will address this in a later post.</p>
<p>(1) The so-called<strong><em> Forward step </em></strong>solves a fixed point equation (which is similar in spirit to taking n steps of Gibbs sampling).  This leads to a pair of coupled, recursion relations for the TAP magnetizations (or just nodes).   Suppose we take t+1 iterations.  Let us ignore the second order Onsager correction, and consider the mean field updates:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%5Bt%2B1%5D%5Cleftarrow%5Csigma%5Cleft%5Bb_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dv_%7Bj%7D%5Bt%2B1%5D-%5Ccdots%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="h_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}v_{j}[t+1]-&#92;cdots&#92;right] " title="h_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}v_{j}[t+1]-&#92;cdots&#92;right] " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=v_%7Bi%7D%5Bt%2B1%5D%5Cleftarrow%5Csigma%5Cleft%5Ba_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dh_%7Bj%7D%5Bt%5D-%5Ccdots%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="v_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}h_{j}[t]-&#92;cdots&#92;right] " title="v_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}h_{j}[t]-&#92;cdots&#92;right] " class="latex" /></p>
<p style="text-align:left;"><span style="color:#0000ff;"><em>Because these are deterministic steps</em></span>, we can express the <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%5Bt%2B1%5D%5C+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="h_{i}[t+1]&#92; " title="h_{i}[t+1]&#92; " class="latex" /> in terms of <img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%5Bt%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="h_{i}[t] " title="h_{i}[t] " class="latex" />:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bh%7D%5Bt%2B1%5D%5Cleftarrow%5Csigma%5Cleft%5B%5Cmathbf%7Bb%7D%2B%5Cmathbf%7BW%7D%5E%7BT%7D%5Csigma%28%5Cmathbf%7Bb%7D%2B%5Cmathbf%7BW%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%5Bt%5D%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{h}[t+1]&#92;leftarrow&#92;sigma&#92;left[&#92;mathbf{b}+&#92;mathbf{W}^{T}&#92;sigma(&#92;mathbf{b}+&#92;mathbf{W}^{T}&#92;mathbf{h}[t])&#92;right] " title="&#92;mathbf{h}[t+1]&#92;leftarrow&#92;sigma&#92;left[&#92;mathbf{b}+&#92;mathbf{W}^{T}&#92;sigma(&#92;mathbf{b}+&#92;mathbf{W}^{T}&#92;mathbf{h}[t])&#92;right] " class="latex" /></p>
<p>At the end of the recursion, we will have a forward pass that resembles a multi-layer MLP, but that shares weights and biases between layers:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bh%7D%5Bt%2B1%5D%5Cleftarrow%5Csigma%5Cleft%5B%5Cmathbf%7Bb%7D%2B%5Cmathbf%7BW%7D%5E%7BT%7D%5Csigma%28%5Cmathbf%7Bb%7D%2B%5Ccdots%5Csigma%28%5Cmathbf%7Bb%7D%2B%5Cmathbf%7Bv%7D%5Cmathbf%7BW%7D%5E%7BT%7D%29%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{h}[t+1]&#92;leftarrow&#92;sigma&#92;left[&#92;mathbf{b}+&#92;mathbf{W}^{T}&#92;sigma(&#92;mathbf{b}+&#92;cdots&#92;sigma(&#92;mathbf{b}+&#92;mathbf{v}&#92;mathbf{W}^{T}))&#92;right] " title="&#92;mathbf{h}[t+1]&#92;leftarrow&#92;sigma&#92;left[&#92;mathbf{b}+&#92;mathbf{W}^{T}&#92;sigma(&#92;mathbf{b}+&#92;cdots&#92;sigma(&#92;mathbf{b}+&#92;mathbf{v}&#92;mathbf{W}^{T}))&#92;right] " class="latex" /></p>
<p style="text-align:left;">We can now associate an n-layer MLP, <em>with tied weights,</em></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctheta%3D%5C%7B%5Cmathbf%7BW%7D%3D%5Cmathbf%7BW%27%7D%3D%5Ccdots%3B%5C%3B%5C%3B%5Cmathbf%7Bb%7D%3D%5Cmathbf%7Bb%27%7D%3D%5Ccdots%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;theta=&#92;{&#92;mathbf{W}=&#92;mathbf{W&#039;}=&#92;cdots;&#92;;&#92;;&#92;mathbf{b}=&#92;mathbf{b&#039;}=&#92;cdots&#92;} " title="&#92;theta=&#92;{&#92;mathbf{W}=&#92;mathbf{W&#039;}=&#92;cdots;&#92;;&#92;;&#92;mathbf{b}=&#92;mathbf{b&#039;}=&#92;cdots&#92;} " class="latex" />,</p>
<p style="text-align:left;"> to an approximate (mean field) EMF RBM,  with n fixed point iterations (ignoring the Onsager correction for now).  Of course, an MLP is supervised, and an RBM is unsupervised, so we need to associate the RBM hidden nodes with the MLP output function at the last layer (<img src="https://s0.wp.com/latex.php?latex=g%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="g^{&#92;mu}_{MLP}(&#92;theta) " title="g^{&#92;mu}_{MLP}(&#92;theta) " class="latex" />), prior to adding the MLP output node</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=g%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29%3Dg%5E%7B%5Cmu%7D_%7BRBM%7D%28%5Ctheta%29%3D%5Cmathbf%7Bh%7D%5Bn%5D%28x%5E%7B%5Cmu%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="g^{&#92;mu}_{MLP}(&#92;theta)=g^{&#92;mu}_{RBM}(&#92;theta)=&#92;mathbf{h}[n](x^{&#92;mu}) " title="g^{&#92;mu}_{MLP}(&#92;theta)=g^{&#92;mu}_{RBM}(&#92;theta)=&#92;mathbf{h}[n](x^{&#92;mu}) " class="latex" /></p>
<p>This leads naturally to the following conjecture:</p>
<p style="text-align:center;"><span style="color:#008000;"><em><strong>The EMF RBM and the BackProp Forward and Backward steps effectively do the same thing&#8211;minimize the Free Energy</strong></em></span></p>
<h4>Is this right ?</h4>
<p><span style="color:#ff0000;"><strong>This is a work in progress</strong></span></p>
<p>Formally, it is simple and compelling.  Is it the whole story&#8230;probably not.  It is merely an observation&#8211;food for thought.</p>
<p>So far, I have only removed the visible magnetizations <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D%5Bn%5D%28x%5E%7B%5Cmu%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v}[n](x^{&#92;mu}) " title="&#92;mathbf{v}[n](x^{&#92;mu}) " class="latex" /> to obtain the MLP layer function<img src="https://s0.wp.com/latex.php?latex=g%5E%7B%5Cmu%7D_%7BMLP%7D%28%5Ctheta%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="g^{&#92;mu}_{MLP}(&#92;theta) " title="g^{&#92;mu}_{MLP}(&#92;theta) " class="latex" /> as a function of the original visible units.  The unsupervised EMF RBM Free Energy, however, contains expressions in terms of both the hidden and visible magnetizations ( <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D%5Bn%5D%2C%5Cmathbf%7Bh%7D%5Bn%5D%3D+%5Cmathbf%7Bm_%7Bv%7D%7D%2C%5Cmathbf%7Bm_%7Bh%7D%7D++&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v}[n],&#92;mathbf{h}[n]= &#92;mathbf{m_{v}},&#92;mathbf{m_{h}}  " title="&#92;mathbf{v}[n],&#92;mathbf{h}[n]= &#92;mathbf{m_{v}},&#92;mathbf{m_{h}}  " class="latex" /> ).  To get a final expression, it is necessary to either</p>
<ul>
<li>unravel the network, like a variational auto encoder (VAE)</li>
<li>replace the visible magnetizations with the true labels, and introduce the softax loss</li>
</ul>
<p>The result itself should not be so surprising, since it has already been pointed out by <a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank">Kingma and Welling, Auto-Encoding Variational Bayes,</a> that a Bernoulli MLP is like a variational decoder.  And, of course, VAEs can be formulated with BackProp.</p>
<p>Nore importantly, It is unclear how good the RBM EMF really is.  Some<a href="https://github.com/lzhbrian/MCMC/blob/master/mcmc.pdf" target="_blank"> followup studies </a>indicate that second order is not as good as, say, AIS, for estimating the partition function.  I have coded <a href="https://github.com/charlesmartin14/emf-rbm">a python emf_rbm.py module using the scikit-learn interface</a>, and testing is underway.  I will blog this soon.</p>
<p>Note that the EMF RBM relies on the<a href="https://arxiv.org/pdf/0806.1147.pdf" target="_blank"> Legendre Transform</a>, which is like a <a href="https://calculatedcontent.com/2015/03/14/convex-relaxations-of-transductive-learning/" target="_blank">convex relaxation</a>.  Early results indicates that this does degrade the RBM solution compared to traditional Cd.  Maybe BackProp may be effective relaxing the convexity constraint by, say, relaxing the condition that the weights are tied between layers.</p>
<p>Still, I hope this can provide some insight.  And there are &#8230;</p>
<hr />
<h4>Implications</h4>
<p>Free Energy is a first class concept in Statistical Mechanics.  In machine learning, not always so much. It appears in much of Hinton&#8217;s work, and, as a starting point to deriving methods like Variational Auto Encoders and <a href="http://edwardlib.org/">Probabilistic Programing</a>.</p>
<p>But Free Energy minimization plays an important role in non-convex optimization as well.  Free energies are a Boltzmann average of the zero-Temperature Energy landscape, and, therefore, convert a non-convex surface into something at least less non-convex.</p>
<p>Indeed, in <a href="https://calculatedcontent.com/2017/01/05/foundations-mean-field-boltzmann-machines-1987/">one of the very first papers on mean field Boltzmann Machines (1987),</a> it is noted that</p>
<p><span style="color:#0000ff;"><em>&#8220;An important property of the effective [free] energy function E'(V,0,T) is that it has a smoother landscape than E(S) due to the extra terms. Hence, the probability of getting stuck in a local minima decreases.&#8221;</em></span></p>
<p>Moreover, in protein folding, we have even stronger effects, which can lead to a ruggedly convex, energy landscape.  This arises when the system runs out of configurational entropy (S), and energetic effects (E) dominate.</p>
<p><span style="color:#008000;">Most importantly, we want to understand, when does Deep Learning generalize well, and when does it overtrain ? </span></p>
<p><a href="https://arxiv.org/abs/1611.01838" target="_blank">LeCun has very recently pointed out</a> that Deep Nets fail when they run out of configuration entropy&#8211;<a href="https://www.youtube.com/watch?v=kIbKHIPbxiU" target="_blank">an argument I also have made from theoretical analysis using the Random Energy Model</a>.  So it is becoming more important to understand what the actual energy landscape of a deep net is, how to separate out the entropic and energetic terms, and how to characterize the configurational entropy.</p>
<p>Hopefully the small insight will be useful and lead to a further understanding of <a href="https://calculatedcontent.com/2015/03/25/why-does-deep-learning-work/">Why Deep Learning Works.</a></p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/charlesmartin14.wordpress.com/9923/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/charlesmartin14.wordpress.com/9923/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=calculatedcontent.com&#038;blog=32496692&#038;post=9923&#038;subd=charlesmartin14&#038;ref=&#038;feed=1" width="1" height="1" />
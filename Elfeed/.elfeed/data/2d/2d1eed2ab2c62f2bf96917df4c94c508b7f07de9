<p>Deep Learning is amazing.  But why is Deep Learning so successful?  Is Deep Learning just old-school Neural Networks on modern hardware?  Is it just that we have so much data now the methods work better?  Is Deep Learning just a really good at finding features. Researchers are working hard to sort this out.</p>
<p>Recently it has been shown that [1]</p>
<p style="text-align:center;"><strong><em><span style="color:#008000;">Unsupervised Deep Learning implements the Kadanoff Real Space Variational Renormalization Group (1975)</span></em></strong></p>
<p style="text-align:left;">This means the success of Deep Learning is intimately related to some very deep and subtle ideas from Theoretical Physics.  In this post we examine this.</p>
<h4>Unsupervised Deep Learning: AutoEncoder Flow Map</h4>
<p>An AutoEncoder is a Unsupervised Deep Learning algorithm that learns how to represent an complex image or other data structure <img src="https://s0.wp.com/latex.php?latex=X+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="X " title="X " class="latex" />.   There are several kinds of AutoEncoders; we care about so-called Neural Encoders&#8211;those using Deep Learning techniques to reconstruct the data:</p>
<p style="text-align:center;"><img data-attachment-id="7135" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/recon/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/recon.jpeg" data-orig-size="312,162" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="recon" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/recon.jpeg?w=300&#038;h=156" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/recon.jpeg?w=312" class="aligncenter size-medium wp-image-7135" src="https://charlesmartin14.files.wordpress.com/2015/03/recon.jpeg?w=300&#038;h=156" alt="recon" width="300" height="156" srcset="https://charlesmartin14.files.wordpress.com/2015/03/recon.jpeg?w=300&amp;h=156 300w, https://charlesmartin14.files.wordpress.com/2015/03/recon.jpeg?w=150&amp;h=78 150w, https://charlesmartin14.files.wordpress.com/2015/03/recon.jpeg 312w" sizes="(max-width: 300px) 100vw, 300px" /></p>
<p style="text-align:left;">The simplest Neural Encoder is a <a title="Restricted Boltzmann machine" href="http://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" target="_blank">Restricted Boltzman Machine</a> (RBM).  An RBM is non-linear, recursive, lossy function <img src="https://s0.wp.com/latex.php?latex=f%28X%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="f(X) " title="f(X) " class="latex" /> that maps the data <img src="https://s0.wp.com/latex.php?latex=X+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="X " title="X " class="latex" /> from visible nodes <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="{v} " title="{v} " class="latex" /> into hidden nodes <img src="https://s0.wp.com/latex.php?latex=%7Bh%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="{h} " title="{h} " class="latex" />:</p>
<p style="text-align:left;"><a href="https://charlesmartin14.files.wordpress.com/2015/03/rbm.png"><img data-attachment-id="7271" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/rbm-3/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/rbm.png" data-orig-size="350,212" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rbm" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/rbm.png?w=366&#038;h=223" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/rbm.png?w=350" class="aligncenter wp-image-7271" src="https://charlesmartin14.files.wordpress.com/2015/03/rbm.png?w=366&#038;h=223" alt="rbm" width="366" height="223" srcset="https://charlesmartin14.files.wordpress.com/2015/03/rbm.png 350w, https://charlesmartin14.files.wordpress.com/2015/03/rbm.png?w=150&amp;h=91 150w, https://charlesmartin14.files.wordpress.com/2015/03/rbm.png?w=300&amp;h=182 300w" sizes="(max-width: 366px) 100vw, 366px" /></a></p>
<p style="text-align:left;">The RBM is learned by selecting the optimal parameters <img src="https://s0.wp.com/latex.php?latex=%7Bb_%7Bv%7D%7D%2C%7Bc_%7Bh%7D%7D%2C%7Bw_%7Bv%2Ch%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="{b_{v}},{c_{h}},{w_{v,h}} " title="{b_{v}},{c_{h}},{w_{v,h}} " class="latex" />that minimize some measure of the reconstruction error (see Training RBMs, below)</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmin+%7C%5CVert+f%28X%29-X%5CVert+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;min |&#92;Vert f(X)-X&#92;Vert " title="&#92;min |&#92;Vert f(X)-X&#92;Vert " class="latex" /></p>
<p style="text-align:left;">RBMs and other Deep Learning algos are formulated using classical Statistical Mechanics.  And that is where it gets interesting!</p>
<h4 style="text-align:left;">Multi Scale Feature Learning</h4>
<p>In machine learning (ML), we map (visible) data into (hidden) features</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D%28X%29%5Crightarrow%5Cmathbf%7Bh%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v}(X)&#92;rightarrow&#92;mathbf{h} " title="&#92;mathbf{v}(X)&#92;rightarrow&#92;mathbf{h} " class="latex" /></p>
<p style="text-align:left;">The hidden units discover features at a <em>coarser grain level of scale</em></p>
<p style="text-align:left;"><a href="https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg"><img data-attachment-id="7297" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/unsupervised-filters/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg?w=348&#038;h=211" data-orig-size="224,136" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="unsupervised filters" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg?w=348&#038;h=211?w=224" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg?w=348&#038;h=211?w=224" class="aligncenter wp-image-7297" src="https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg?w=348&#038;h=211" alt="unsupervised filters" width="348" height="211" srcset="https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg 224w, https://charlesmartin14.files.wordpress.com/2015/03/unsupervised-filters.jpeg?w=150&amp;h=91 150w" sizes="(max-width: 348px) 100vw, 348px" /></a></p>
<p style="text-align:left;">With RBMs, when features are complex, we may stack them into a Deep Belief Network (DBM), so that we can learn at different levels of scale</p>
<p style="text-align:left;"><a href="https://charlesmartin14.files.wordpress.com/2015/03/dbn.png"><img data-attachment-id="7283" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/dbn/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/dbn.png?w=405&#038;h=229" data-orig-size="299,169" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dbn" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/dbn.png?w=405&#038;h=229?w=299" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/dbn.png?w=405&#038;h=229?w=299" class="aligncenter wp-image-7283" src="https://charlesmartin14.files.wordpress.com/2015/03/dbn.png?w=405&#038;h=229" alt="dbn" width="405" height="229" srcset="https://charlesmartin14.files.wordpress.com/2015/03/dbn.png 299w, https://charlesmartin14.files.wordpress.com/2015/03/dbn.png?w=150&amp;h=85 150w" sizes="(max-width: 405px) 100vw, 405px" /></a></p>
<p style="text-align:left;">and leads to multi-scale features in each layer</p>
<p style="text-align:left;"><a href="https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg"><img data-attachment-id="7298" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/dbn-features/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg?w=338&#038;h=338" data-orig-size="225,224" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="dbn-features" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg?w=338&#038;h=338?w=225" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg?w=338&#038;h=338?w=225" class="aligncenter wp-image-7298" src="https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg?w=338&#038;h=338" alt="dbn-features" width="338" height="338" srcset="https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg 225w, https://charlesmartin14.files.wordpress.com/2015/03/dbn-features.jpeg?w=150&amp;h=150 150w" sizes="(max-width: 338px) 100vw, 338px" /></a></p>
<p style="text-align:center;"><strong><span style="color:#3366ff;"><em>Deep Belief Networks are a Theory of Unsupervised MultiScale Feature Learning</em></span></strong></p>
<h4 style="text-align:left;">Fixed Points and Flow Maps</h4>
<p>We call <img src="https://s0.wp.com/latex.php?latex=f%28X%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="f(X) " title="f(X) " class="latex" /> a <em><strong>flow map</strong></em></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f%28X%29%5Crightarrow+X+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="f(X)&#92;rightarrow X " title="f(X)&#92;rightarrow X " class="latex" /></p>
<p style="text-align:left;">eIf we apply the flow map to the data repeatedly, (we hope) it converges to a fixed point</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clim%5Climits_%7Bn%7Df%5E%7Bn%7D%28f%5E%7Bn-1%7D%28%5Ccdots%28f%5E%7B1%7D%28f%5E%7B0%7D%28X%29%29%29%29%5Crightarrow+f_%7B%5Cinfty%7D%28X%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lim&#92;limits_{n}f^{n}(f^{n-1}(&#92;cdots(f^{1}(f^{0}(X))))&#92;rightarrow f_{&#92;infty}(X) " title="&#92;lim&#92;limits_{n}f^{n}(f^{n-1}(&#92;cdots(f^{1}(f^{0}(X))))&#92;rightarrow f_{&#92;infty}(X) " class="latex" /></p>
<p style="text-align:left;">Notice that we usually expect to apply the same map each time <img src="https://s0.wp.com/latex.php?latex=f%5E%7Bn%7D%28x%29%3Df%28x%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="f^{n}(x)=f(x) " title="f^{n}(x)=f(x) " class="latex" />, however, for a computational theory we may need more flexibility.</p>
<h5>Example: Linear Flow Map</h5>
<p>The simplest example of a flow map is the simple linear map</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=X%5Crightarrow+CX+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="X&#92;rightarrow CX " title="X&#92;rightarrow CX " class="latex" /></p>
<p style="text-align:left;">so that</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f%28X%29%5Csim+CX+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="f(X)&#92;sim CX " title="f(X)&#92;sim CX " class="latex" /></p>
<p style="text-align:left;">where C is a non-negative, low rank matrix</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmid+C%5Cmid%5C%2C%5Cll%5C%2C%5Cmid+X%5Cmid+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mid C&#92;mid&#92;,&#92;ll&#92;,&#92;mid X&#92;mid " title="&#92;mid C&#92;mid&#92;,&#92;ll&#92;,&#92;mid X&#92;mid " class="latex" /></p>
<p style="text-align:left;">We have seen this before: this leads to a <a title="Advances in Convex NMF: Linear Programming" href="https://charlesmartin14.wordpress.com/2013/05/06/advances-in-convex-nmf-part-1-linear-programming/" target="_blank">Convex form of NonNegative Matrix Factorization NMF</a></p>
<p><a href="https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg"><img data-attachment-id="7142" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/convex-fig1/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg" data-orig-size="988,774" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Convex-Fig1" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg?w=300&#038;h=235" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg?w=840" class="aligncenter size-medium wp-image-7142" src="https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg?w=300&#038;h=235" alt="Convex-Fig1" width="300" height="235" srcset="https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg?w=300&amp;h=235 300w, https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg?w=600&amp;h=470 600w, https://charlesmartin14.files.wordpress.com/2015/03/convex-fig1.jpg?w=150&amp;h=118 150w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Convex NMF applies when we can specify the feature space and where the data naturally clusters.  Here, there are a few instances that are archetypes that define the convex hull of the data.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cmathbf%7Bx%7D_%7Bc%7D%5C%7D%5Cin+X%5C%2C%2Cc%3D1%2C%5Ccdots%2C%5Cmid+C%5Cmid+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;{&#92;mathbf{x}_{c}&#92;}&#92;in X&#92;,,c=1,&#92;cdots,&#92;mid C&#92;mid " title="&#92;{&#92;mathbf{x}_{c}&#92;}&#92;in X&#92;,,c=1,&#92;cdots,&#92;mid C&#92;mid " class="latex" /></p>
<p style="text-align:left;">Amazingly, many clustering problems are provably convex&#8211;but that&#8217;s a story for another post.</p>
<h5>Example: Manifold Learning</h5>
<p>Near a fixed point, we commonly approximate the flow map by a linear operator</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f_%7B%5Cinfty%7D%28X%29+%5Csim%5Cmathbf%7BL%7D%28X%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="f_{&#92;infty}(X) &#92;sim&#92;mathbf{L}(X) " title="f_{&#92;infty}(X) &#92;sim&#92;mathbf{L}(X) " class="latex" /></p>
<p style="text-align:left;">This lets us capture the structure of the true data manifold, and is usually described by the low lying eigen-spectra of</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BL%7D%28X%29%3D%5Csum%5Climits_%7Bi%3D1%7D%5Clambda_%7Bi%7D%5Chat%7B%5Cmathbf%7Bv%7D%7D_%7Bi%7D%5Csim%5Clambda_%7B0%7D%5Chat%7B%5Cmathbf%7Bv%7D%7D_%7B0%7D%2B%5Clambda_%7B1%7D%5Chat%7B%5Cmathbf%7Bv%7D%7D_%7B1%7D%2B%5Ccdots&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{L}(X)=&#92;sum&#92;limits_{i=1}&#92;lambda_{i}&#92;hat{&#92;mathbf{v}}_{i}&#92;sim&#92;lambda_{0}&#92;hat{&#92;mathbf{v}}_{0}+&#92;lambda_{1}&#92;hat{&#92;mathbf{v}}_{1}+&#92;cdots" title="&#92;mathbf{L}(X)=&#92;sum&#92;limits_{i=1}&#92;lambda_{i}&#92;hat{&#92;mathbf{v}}_{i}&#92;sim&#92;lambda_{0}&#92;hat{&#92;mathbf{v}}_{0}+&#92;lambda_{1}&#92;hat{&#92;mathbf{v}}_{1}+&#92;cdots" class="latex" />.</p>
<p>In the same spirit,  Semi &amp; Unsupervised Manifold Learning, we model the data using a Laplacian operator <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BL%7D%28%5Csigma%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{L}(&#92;sigma) " title="&#92;mathbf{L}(&#92;sigma) " class="latex" />, usually parameterized by <em>a single scale parameter</em> <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;sigma " title="&#92;sigma " class="latex" />.</p>
<h4><a href="https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png"><img data-attachment-id="7139" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/manifold-2/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png" data-orig-size="614,372" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="manifold" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png?w=300&#038;h=182" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png?w=614" class="aligncenter size-medium wp-image-7139" src="https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png?w=300&#038;h=182" alt="manifold" width="300" height="182" srcset="https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png?w=300&amp;h=182 300w, https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png?w=600&amp;h=364 600w, https://charlesmartin14.files.wordpress.com/2015/03/manifold1.png?w=150&amp;h=91 150w" sizes="(max-width: 300px) 100vw, 300px" /></a></h4>
<p>These methods include <a title="spectral clustering: a quick overview" href="https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/" target="_blank">Spectral Clustering</a>, <a title="Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples" href="http://vikas.sindhwani.org/MR.pdf" target="_blank">Manifold Regularization</a> , <a title="LapSVM" href="http://www.dii.unisi.it/~melacci/lapsvmp/" target="_blank">Laplacian SVM</a>, etc.  Note that manifold learning methods, like the <a title="The Manifold Tangent Classifier" href="http://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf" target="_blank">Manifold Tanget Classifier,</a>  employ Contractive Auto Encoders and use several scale parameters to capture the local structure of the data manifold.</p>
<h4 style="text-align:left;">The Renormalization Group</h4>
<p>In chemistry and physics, we frequently encounter problems that require a multi-scale description.   We need this for critical points and phase transitions, for natural crashes like earthquakes and avalanches,  for polymers and other macromolecules, for strongly correlated electronic systems, for quantum field theory, and, now, for Deep Learning.</p>
<p>A unifying idea across these systems is the Renormalization Group (RG) Theory.</p>
<p style="text-align:center;"><em><strong><span style="color:#0000ff;">Renormalization Group Theory is both a conceptual framework on how to think about physics on multiple scales as well as a technical &amp; computational problem solving tool.</span></strong></em></p>
<p style="text-align:left;"><a href="https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg"><img data-attachment-id="7339" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/kenwilson/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg?w=840" data-orig-size="277,182" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kenwilson" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg?w=840?w=277" data-large-file="https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg?w=840?w=277" class=" size-full wp-image-7339 alignleft" src="https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg?w=840" alt="kenwilson" srcset="https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg 277w, https://charlesmartin14.files.wordpress.com/2015/04/kenwilson.jpeg?w=150 150w" sizes="(max-width: 277px) 100vw, 277px"   /></a>Ken Wilson won <a title="THE RENORMALIZATION GROUP AND CRITICAL PHENOMENA" href="http://www.nobelprize.org/nobel_prizes/physics/laureates/1982/wilson-lecture.pdf" target="_blank">the 1982 Nobel Prize in Physics</a> for the development and application of his Momentum Space RG theory to phase transitions.</p>
<p style="text-align:left;">We used RG theory<a title="The Bitcoin Crash and How Nature Works" href="https://charlesmartin14.wordpress.com/2015/01/16/the-bitcoin-crash-and-how-nature-works/" target="_blank"> to model the recent BitCoin crash as a phase transition</a>.</p>
<p style="text-align:left;">Wilson invented modern multi-scale modeling; the so-called Wilson Basis was an early form of Wavelets.  Wilson was also a big advocate of using supercomputers for solving problems.  Being a Nobel Laureate, he had great success promoting scientific computing.  It was thanks to him I had access to a<a title="Cray-YMP" href="http://en.wikipedia.org/wiki/Cray_Y-MP"> Cray Y-MP</a> when I was in high school because he was a professor at my undergrad, The Ohio State University.</p>
<p style="text-align:left;"> Here is the idea.  Consider a feature map  which transforms the data <img src="https://s0.wp.com/latex.php?latex=X+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="X " title="X " class="latex" /> to a different, more coarse grain scale</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=x%5Crightarrow%5Cphi_%7B%5Clambda%7D%28x%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="x&#92;rightarrow&#92;phi_{&#92;lambda}(x) " title="x&#92;rightarrow&#92;phi_{&#92;lambda}(x) " class="latex" /></p>
<p style="text-align:left;">The RG theory requires that <em>the Free Energy</em> <img src="https://s0.wp.com/latex.php?latex=F%28x%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F(x) " title="F(x) " class="latex" /> is rescaled, to reflect that</p>
<p style="text-align:center;"><span style="color:#ff6600;"><strong><em>the Free Energy is both Size-Extensive and Scale Invariant near a Critical Point</em></strong></span></p>
<p style="text-align:left;">This is not obvious &#8212; but it is essential to both having a conceptual understanding of complex, multi scale phenomena, and it is necessary to obtain very highly accurate numerical calculations.  In fact, being <a title="Size consistency and size extensivity" href="http://en.wikipedia.org/wiki/Size_consistency_and_size_extensivity" target="_blank">size extensive and/or size consistent</a> is absolutely necessary for highly accurate quantum chemistry calculations of strongly correlated systems.  So it is pretty amazing but perhaps not surprising that this is necessary for large scale deep learning calculations also!</p>
<h5 style="text-align:left;">The Fundamental Renormalization Group Equation (RGE)</h5>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%28x%29%3Dg%28x%29%2B%5Cdfrac%7B1%7D%7B%5Clambda%7D%5Cmathcal%7BF%7D%28%5Cphi_%7B%5Clambda%7D%28x%29%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{F}(x)=g(x)+&#92;dfrac{1}{&#92;lambda}&#92;mathcal{F}(&#92;phi_{&#92;lambda}(x)) " title="&#92;mathcal{F}(x)=g(x)+&#92;dfrac{1}{&#92;lambda}&#92;mathcal{F}(&#92;phi_{&#92;lambda}(x)) " class="latex" /></p>
<p>If we (can) apply the same map, <img src="https://s0.wp.com/latex.php?latex=F%28x%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="F(x) " title="F(x) " class="latex" />, repeatedly, we obtain a RG recursion relation, which is the starting point for most analytic work in theoretical physics.   It is usually difficult to obtain an exact solution to the RGE (although it is illuminating when possible [20]).</p>
<p>Many RG formulations both approximate the exact RGE and/or only include relevant variables. To describe a multiscale system, it is essential to distinguish between these relevant and irrelevant variables.</p>
<h5 style="text-align:left;">Example: Linear Rescaling</h5>
<p>Let&#8217;s say the feature map is a simple linear rescaling</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28x%29%3D%5Clambda+x+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;phi(x)=&#92;lambda x " title="&#92;phi(x)=&#92;lambda x " class="latex" /></p>
<p style="text-align:left;">We can obtain <a title="The Bitcoin Crash and How Nature Works" href="https://charlesmartin14.wordpress.com/2015/01/16/the-bitcoin-crash-and-how-nature-works/" target="_blank">a very elegant, approximate RG solution</a> where F(x) obeys a complex (or log-periodic) power law.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%28x%29%5Csim+x%5E%7B-%28%5Calpha%2Bi%5Cbeta%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{F}(x)&#92;sim x^{-(&#92;alpha+i&#92;beta)} " title="&#92;mathcal{F}(x)&#92;sim x^{-(&#92;alpha+i&#92;beta)} " class="latex" /></p>
<p style="text-align:left;">This behavior is thought to characterize Per-Bak style Self-Organized Criticality (SOC), which appears in many natural systems&#8211;<a title="A Fundamental Theory to Model the Mind" href="https://www.quantamagazine.org/20140403-a-fundamental-theory-to-model-the-mind/" target="_blank">and perhaps even in the brain itself.</a>   Which leads to the argument that perhaps Deep Learning and Real Learning work so well because they operate like a system just near a phase transition&#8211;also known as <a title="Sand Pile Model of the Mind Grows in Popularity" href="http://www.scientificamerican.com/article/sand-pile-model-of-the-mind-grows-in-popularity/" target="_blank">the Sand Pile Model-</a>-operating at a state between order and chaos.</p>
<h4 style="text-align:left;">the Kadanoff Variational Renormalization Group (1975)</h4>
<p style="text-align:left;"><a href="https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg"><img data-attachment-id="7371" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/kadanoff/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg?w=840" data-orig-size="259,194" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="kadanoff" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg?w=840?w=259" data-large-file="https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg?w=840?w=259" class=" size-full wp-image-7371 alignright" src="https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg?w=840" alt="kadanoff" srcset="https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg 259w, https://charlesmartin14.files.wordpress.com/2015/04/kadanoff.jpeg?w=150 150w" sizes="(max-width: 259px) 100vw, 259px"   /></a>Leo Kadanoff, now at the University of Chicago, invented some of the early ideas in Renormalization Group.  He is most famous for the Real Space formulation of RG, sometimes called the Block Spin approach.  He also developed an alternative approach, called the Variational Renormalization Group (VRG, 1975), which is, remarkably, what Unsupervised RBMs are implementing!</p>
<p style="text-align:left;">Let&#8217;s consider a traditional Neural Network&#8211;a Hopfield Associative Memory (HAM).  This is also known as an Ising model or a Spin Glass in statistical physics.</p>
<p style="text-align:left;">An HAM consists of only visible units; it stores memories explicitly and directly in them:</p>
<p style="text-align:center;"><a href="https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png"><img data-attachment-id="7342" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/hopfield/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png?w=840" data-orig-size="240,210" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="hopfield" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png?w=840?w=240" data-large-file="https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png?w=840?w=240" class="alignnone size-full wp-image-7342" src="https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png?w=840" alt="hopfield" srcset="https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png 240w, https://charlesmartin14.files.wordpress.com/2015/04/hopfield.png?w=150 150w" sizes="(max-width: 240px) 100vw, 240px"   /></a></p>
<p style="text-align:left;">We specify the Energy &#8212; called <strong>the Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H} " title="&#92;mathcal{H} " class="latex" /></strong> &#8212; for the nodes.  Note that all the nodes are visible.  We write</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7BHAM%7D%3D-%5Csum%5Climits_%7Bi%7DB_%7Bi%7Dv_%7Bi%7D-%5Csum%5Climits_%7Bi%7DJ_%7Bi%2Cj%7Dv_%7Bi%7Dv_%7Bj%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}^{HAM}=-&#92;sum&#92;limits_{i}B_{i}v_{i}-&#92;sum&#92;limits_{i}J_{i,j}v_{i}v_{j} " title="&#92;mathcal{H}^{HAM}=-&#92;sum&#92;limits_{i}B_{i}v_{i}-&#92;sum&#92;limits_{i}J_{i,j}v_{i}v_{j} " class="latex" /></p>
<p>The Hopfield model has only single <img src="https://s0.wp.com/latex.php?latex=B_%7Bi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="B_{i} " title="B_{i} " class="latex" /> and pair-wise <img src="https://s0.wp.com/latex.php?latex=J_%7Bi%2Cj%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="J_{i,j} " title="J_{i,j} " class="latex" /> interactions.</p>
<p style="text-align:left;">A general Hamiltonian might have many-body, multi-scale interactions:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%28v%29%3D-%5Csum%5Climits_%7Bi%7DK_%7Bi%7Dv_%7Bi%7D-%5Csum%5Climits_%7Bi%7DK_%7Bi%2Cj%7Dv_%7Bi%7Dv_%7Bj%7D-%5Csum%5Climits_%7Bi%2Cj%2Ck%7DK_%7Bi%2Cj%2Ck%7Dv_%7Bi%7Dv_%7Bj%7Dv_%7Bk%7D-%5Ccdots+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}(v)=-&#92;sum&#92;limits_{i}K_{i}v_{i}-&#92;sum&#92;limits_{i}K_{i,j}v_{i}v_{j}-&#92;sum&#92;limits_{i,j,k}K_{i,j,k}v_{i}v_{j}v_{k}-&#92;cdots " title="&#92;mathcal{H}(v)=-&#92;sum&#92;limits_{i}K_{i}v_{i}-&#92;sum&#92;limits_{i}K_{i,j}v_{i}v_{j}-&#92;sum&#92;limits_{i,j,k}K_{i,j,k}v_{i}v_{j}v_{k}-&#92;cdots " class="latex" /></p>
<p style="text-align:left;">The <span style="color:#00ccff;"><strong>Partition Function</strong></span> is given as</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D%3D%5Csum%5Climits_%7Bv%7De%5E%7B-%5Cmathcal%7BH%7D%28v%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{Z}=&#92;sum&#92;limits_{v}e^{-&#92;mathcal{H}(v)} " title="&#92;mathcal{Z}=&#92;sum&#92;limits_{v}e^{-&#92;mathcal{H}(v)} " class="latex" /></p>
<p style="text-align:left;">And the <span style="color:#00ccff;"><strong>Free Energy</strong></span> is</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%5E%7Bv%7D%3D-%5Cln%5Cmathcal%7BZ%7D%3D-%5Cln%5Csum%5Climits_%7Bv%7De%5E%7B-%5Cmathcal%7BH%7D%28v%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{F}^{v}=-&#92;ln&#92;mathcal{Z}=-&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;mathcal{H}(v)} " title="&#92;mathcal{F}^{v}=-&#92;ln&#92;mathcal{Z}=-&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;mathcal{H}(v)} " class="latex" /></p>
<p style="text-align:left;">The idea was to mimic how our neurons were thought to store memories&#8211;<a title="Memories May Not Live in Neurons’ Synapses" href="http://www.scientificamerican.com/article/memories-may-not-live-in-neurons-synapses" target="_blank">although perhaps our neurons do not even do this.</a></p>
<p style="text-align:left;">Either way, Hopfield Neural Networks have many problems; most notably they may learn spurious patterns that never appeared in the training set. So they are pretty bad memories.</p>
<p>Hinton created the modern RBM to overcome the problems of the Hopfield model.  He used hidden units to represent the features in the data&#8211;not to memorize the data examples directly.</p>
<p style="text-align:left;"><a href="https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg"><img data-attachment-id="7344" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/rbm2/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg?w=840" data-orig-size="284,177" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rbm2" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg?w=840?w=284" data-large-file="https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg?w=840?w=284" class=" size-full wp-image-7344 aligncenter" src="https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg?w=840" alt="rbm2" srcset="https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg 284w, https://charlesmartin14.files.wordpress.com/2015/04/rbm2.jpeg?w=150 150w" sizes="(max-width: 284px) 100vw, 284px"   /></a></p>
<p>An RBM is specified Energy function for both the visible and hidden units</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BE%7D%28v%2Ch%29%3D%5Cmathbf%7Bv%7D%5E%7Bt%7D%5Cmathbf%7Bb%7D%2B%5Cmathbf%7Bv%7D%5E%7Bt%7D%5Cmathbf%7BW%7D%5Cmathbf%7Bh%7D%2B%5Cmathbf%7Bc%7D%5E%7Bt%7D%5Cmathbf%7Bh%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{E}(v,h)=&#92;mathbf{v}^{t}&#92;mathbf{b}+&#92;mathbf{v}^{t}&#92;mathbf{W}&#92;mathbf{h}+&#92;mathbf{c}^{t}&#92;mathbf{h} " title="&#92;mathbf{E}(v,h)=&#92;mathbf{v}^{t}&#92;mathbf{b}+&#92;mathbf{v}^{t}&#92;mathbf{W}&#92;mathbf{h}+&#92;mathbf{c}^{t}&#92;mathbf{h} " class="latex" /></p>
<p style="text-align:left;">This also defines joint probability of simultaenously observing a configuration of hidden and visible spins</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=P%28v%2Ch%29%3D%5Cdfrac%7Be%5E%7B-%5Cmathbf%7BE%28v%2Ch%29%7D%7D%7D%7B%5Cmathcal%7BZ%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="P(v,h)=&#92;dfrac{e^{-&#92;mathbf{E(v,h)}}}{&#92;mathcal{Z}} " title="P(v,h)=&#92;dfrac{e^{-&#92;mathbf{E(v,h)}}}{&#92;mathcal{Z}} " class="latex" /></p>
<p style="text-align:left;">which is learned variationally, by minimizing the reconstruction error&#8230;or the cross entropy (KL divergence), plus some regularization (Dropout), using Greedy layer-wise unsupervised training, with the Contrastive Divergence (CD or PCD) algo, &#8230;</p>
<p>The specific details of an RBM Energy are not addressed by these general concepts; these details do not affect these arguments&#8211;although clearly they matter in practice !</p>
<h5></h5>
<p>It turns out that</p>
<p style="text-align:center;"><span style="color:#008000;"><strong><em>Introducing Hidden Units in a Neural Network is a Scale Renormalization.</em>  </strong></span></p>
<p style="text-align:left;">When changing scale, we obtain an <em>Effective Hamiltonian <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BH%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{&#92;mathcal{H}} " title="&#92;tilde{&#92;mathcal{H}} " class="latex" /></em> that acts on a the new feature space (i.e the hidden units)</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%28v%29%5Crightarrow%5Ctilde%7B%5Cmathcal%7BH%7D%7D%28h%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}(v)&#92;rightarrow&#92;tilde{&#92;mathcal{H}}(h) " title="&#92;mathcal{H}(v)&#92;rightarrow&#92;tilde{&#92;mathcal{H}}(h) " class="latex" /></p>
<p style="text-align:left;">or, in operator form</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BH%7D%7D%28h%29%3D%5Cmathbb%7BR%7D%5B%5Cmathcal%7BH%7D%28v%29%5D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{&#92;mathcal{H}}(h)=&#92;mathbb{R}[&#92;mathcal{H}(v)] " title="&#92;tilde{&#92;mathcal{H}}(h)=&#92;mathbb{R}[&#92;mathcal{H}(v)] " class="latex" /></p>
<p style="text-align:left;">This Effective Hamiltonian is not specified explicitly, but we know it can take the general form (<a title="Why does Deep Learning work?" href="https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/" target="_blank">of a spin funnel, actually</a>)</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BH%7D%7D%28h%29%3D-%5Csum%5Climits_%7Bi%7D%5Ctilde%7BK%7D_%7Bi%7Dh_%7Bi%7D-%5Csum%5Climits_%7Bi%7D%5Ctilde%7BK%7D_%7Bi%2Cj%7Dh_%7Bi%7Dh_%7Bj%7D-%5Csum%5Climits_%7Bi%2Cj%2Ck%7D%5Ctilde%7BK%7D_%7Bi%2Cj%2Ck%7Dh_%7Bi%7Dh_%7Bj%7Dh_%7Bk%7D%5Ccdots+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{&#92;mathcal{H}}(h)=-&#92;sum&#92;limits_{i}&#92;tilde{K}_{i}h_{i}-&#92;sum&#92;limits_{i}&#92;tilde{K}_{i,j}h_{i}h_{j}-&#92;sum&#92;limits_{i,j,k}&#92;tilde{K}_{i,j,k}h_{i}h_{j}h_{k}&#92;cdots " title="&#92;tilde{&#92;mathcal{H}}(h)=-&#92;sum&#92;limits_{i}&#92;tilde{K}_{i}h_{i}-&#92;sum&#92;limits_{i}&#92;tilde{K}_{i,j}h_{i}h_{j}-&#92;sum&#92;limits_{i,j,k}&#92;tilde{K}_{i,j,k}h_{i}h_{j}h_{k}&#92;cdots " class="latex" /></p>
<p style="text-align:left;">The RG transform preservers the Free Energy (when properly rescaled):</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%5E%7Bh%7D%5Csim%5Cmathcal%7BF%7D%5E%7Bv%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{F}^{h}&#92;sim&#92;mathcal{F}^{v} " title="&#92;mathcal{F}^{h}&#92;sim&#92;mathcal{F}^{v} " class="latex" /></p>
<p style="text-align:left;">where</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%5E%7Bh%7D%3D-%5Cln%5Csum%5Climits_%7Bv%7De%5E%7B%5Cmathcal%7BH%7D%28h%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{F}^{h}=-&#92;ln&#92;sum&#92;limits_{v}e^{&#92;mathcal{H}(h)} " title="&#92;mathcal{F}^{h}=-&#92;ln&#92;sum&#92;limits_{v}e^{&#92;mathcal{H}(h)} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D%5E%7Bv%7D%3D-%5Cln%5Csum%5Climits_%7Bv%7De%5E%7B-%5Ctilde%7B%5Cmathcal%7BH%7D%7D%28h%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{F}^{v}=-&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;tilde{&#92;mathcal{H}}(h)} " title="&#92;mathcal{F}^{v}=-&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;tilde{&#92;mathcal{H}}(h)} " class="latex" /></p>
<h5 style="text-align:left;">Critical Trajectories and Renormalized Manifolds</h5>
<p>The RG theory provides a way to iteratively update, or renormalize, the system Hamiltonian.  Each time we add a layer of hidden units (<strong>h1,</strong> <strong>h2</strong>, &#8230;), we have</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7BRG%7D%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh1%7D%29%3D%5Cmathbb%7BR%7D%5B%5Cmathcal%7BH%7D%28%5Cmathbf%7Bv%7D%29%5D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}^{RG}(&#92;mathbf{v},&#92;mathbf{h1})=&#92;mathbb{R}[&#92;mathcal{H}(&#92;mathbf{v})] " title="&#92;mathcal{H}^{RG}(&#92;mathbf{v},&#92;mathbf{h1})=&#92;mathbb{R}[&#92;mathcal{H}(&#92;mathbf{v})] " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7BRG%7D%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh1%7D%2C%5Cmathbf%7Bh2%7D%29%3D%5Cmathbb%7BR%7D%5B%5Cmathbb%7BR%7D%5B%5Cmathcal%7BH%7D%28%5Cmathbf%7Bv%7D%29%5D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}^{RG}(&#92;mathbf{v},&#92;mathbf{h1},&#92;mathbf{h2})=&#92;mathbb{R}[&#92;mathbb{R}[&#92;mathcal{H}(&#92;mathbf{v})] " title="&#92;mathcal{H}^{RG}(&#92;mathbf{v},&#92;mathbf{h1},&#92;mathbf{h2})=&#92;mathbb{R}[&#92;mathbb{R}[&#92;mathcal{H}(&#92;mathbf{v})] " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ccdots+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;cdots " title="&#92;cdots " class="latex" /></p>
<p style="text-align:left;">We imagine that the flow map is attracted to a <strong><em><span style="color:#ff9900;">Critical Trajectory</span> </em></strong>which naturally leads the algorithm to the fixed point.  At each step, when we apply another RG transform, we obtain a new, <span style="color:#ff00ff;"><strong><em>Renormalized Manifold</em></strong>,</span> each one closer to the optimal data manifold.</p>
<p><img data-attachment-id="7123" data-permalink="https://calculatedcontent.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/graph_fisher/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/03/graph_fisher.jpg" data-orig-size="2730,2851" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="graph_fisher" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/03/graph_fisher.jpg?w=287&#038;h=300" data-large-file="https://charlesmartin14.files.wordpress.com/2015/03/graph_fisher.jpg?w=840" class=" size-medium wp-image-7123 aligncenter" src="https://charlesmartin14.files.wordpress.com/2015/03/graph_fisher.jpg?w=287&#038;h=300" alt="graph_fisher" width="287" height="300" srcset="https://charlesmartin14.files.wordpress.com/2015/03/graph_fisher.jpg?w=287&amp;h=300 287w, https://charlesmartin14.files.wordpress.com/2015/03/graph_fisher.jpg?w=574&amp;h=600 574w, https://charlesmartin14.files.wordpress.com/2015/03/graph_fisher.jpg?w=144&amp;h=150 144w" sizes="(max-width: 287px) 100vw, 287px" /></p>
<p>Conceptually, the RG flow map is most useful when applied to critical phenomena&#8211;physical systems and/or simple models that undergo a phase transition.  And, as importantly, the small changes in the data should &#8216;wash away&#8217; as noise and not affect the macroscopic / critical phenomena. Many systems&#8211;<em>but not all</em>&#8211;display this.</p>
<p>Where Hopfield Nets fail to be useful here, RBMs and Deep Learning systems shine.</p>
<p style="text-align:left;">We now show that these RG transformations are achieved by stacking RBMs and solving the RBM inference problem!</p>
<p style="text-align:left;"><strong>Kadanoff&#8217;s Variational Renormalization Group</strong></p>
<p>As in many physics problems, we break the modeling problem into two parts:  one we know how to solve, and one we need to guess.</p>
<ol>
<li><strong><span style="color:#008000;"><em>we know</em></span></strong> the Hamiltonian at the most fine grained level of scale  <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%28v%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}(v) " title="&#92;mathcal{H}(v) " class="latex" /></li>
<li><strong><span style="color:#ff0000;"><em>we seek </em></span></strong>the correlation <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%7D%28v%2Ch%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{V}(v,h) " title="&#92;mathbf{V}(v,h) " class="latex" /> that couples to the next level scale</li>
</ol>
<p>The joint Hamiltonian, or Energy function, is then given by</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%28%5Cmathbf%7Bv%2Ch%7D%29%3D%5Cmathcal%7BH%7D%28%5Cmathbf%7Bv%7D%29%2B%5Cmathbf%7BV%28v%2Ch%29%7D&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}(&#92;mathbf{v,h})=&#92;mathcal{H}(&#92;mathbf{v})+&#92;mathbf{V(v,h)}" title="&#92;mathcal{H}(&#92;mathbf{v,h})=&#92;mathcal{H}(&#92;mathbf{v})+&#92;mathbf{V(v,h)}" class="latex" /></p>
<p style="text-align:left;">The Correlation <strong>V(v,h)</strong> is defined so that the partition function <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{Z} " title="&#92;mathcal{Z} " class="latex" /> is not changed</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Csum%5Climits_%7Bh%7De%5E%7B-%5Cmathbf%7BV%7D%5Cmathbf%7B%28v%2Ch%7D%29%7D%3D1+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;sum&#92;limits_{h}e^{-&#92;mathbf{V}&#92;mathbf{(v,h})}=1 " title="&#92;sum&#92;limits_{h}e^{-&#92;mathbf{V}&#92;mathbf{(v,h})}=1 " class="latex" /></p>
<p style="text-align:left;">This gives us</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D%3D%5Csum_%7Bv%7De%5E%7B-%5Cmathcal%7BH%7D%28v%29%7D%3D%5Csum%5Climits_%7Bv%7D%5Csum%5Climits_%7Bh%7De%5E%7B-%5Cmathbf%7BV%7D%28v%2Ch%29%7De%5E%7B-%5Cmathcal%7BH%7D%28v%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{Z}=&#92;sum_{v}e^{-&#92;mathcal{H}(v)}=&#92;sum&#92;limits_{v}&#92;sum&#92;limits_{h}e^{-&#92;mathbf{V}(v,h)}e^{-&#92;mathcal{H}(v)} " title="&#92;mathcal{Z}=&#92;sum_{v}e^{-&#92;mathcal{H}(v)}=&#92;sum&#92;limits_{v}&#92;sum&#92;limits_{h}e^{-&#92;mathbf{V}(v,h)}e^{-&#92;mathcal{H}(v)} " class="latex" /></p>
<p><small>(Sometimes the Correlation <strong>V</strong> is called a Transfer Operator <strong>T</strong>, where<strong> V(v,h)=-T(v,h)</strong> )</small></p>
<p class="p1">We may now define a renormalized effective Hamilonian <img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BH%7D%28h%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{&#92;mathcal{H}(h)} " title="&#92;tilde{&#92;mathcal{H}(h)} " class="latex" /> that acts only on the hidden nodes</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BH%7D%7D%28h%29%3D%5Cln%5Csum%5Climits_%7Bv%7De%5E%7B-%5Cmathbf%7BV%7D%28v%2Ch%29%7De%5E%7B-%5Cmathcal%7BH%7D%28v%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;tilde{&#92;mathcal{H}}(h)=&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;mathbf{V}(v,h)}e^{-&#92;mathcal{H}(v)} " title="&#92;tilde{&#92;mathcal{H}}(h)=&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;mathbf{V}(v,h)}e^{-&#92;mathcal{H}(v)} " class="latex" /></p>
<p>so that we may write</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D%3D%5Csum%5Climits_%7Bh%7De%5E%7B-%5Ctilde%7B%5Cmathcal%7BH%7D%7D%28h%29%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{Z}=&#92;sum&#92;limits_{h}e^{-&#92;tilde{&#92;mathcal{H}}(h)} " title="&#92;mathcal{Z}=&#92;sum&#92;limits_{h}e^{-&#92;tilde{&#92;mathcal{H}}(h)} " class="latex" /></p>
<p>Because the partition function does not change, the Exact RGE preserves the Free Energy (up to a scale change, we we subsume into <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{Z}) " title="&#92;mathcal{Z}) " class="latex" /></p>
<p class="p1" style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5CDelta%5Ctilde%7B%5Cmathcal%7BF%7D%7D%3D%5Ctilde%7B%5Cmathcal%7BF%7D%7D%5E%7Bh%7D-%5Cmathcal%7BF%7D%5E%7Bv%7D%3D0+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;Delta&#92;tilde{&#92;mathcal{F}}=&#92;tilde{&#92;mathcal{F}}^{h}-&#92;mathcal{F}^{v}=0 " title="&#92;Delta&#92;tilde{&#92;mathcal{F}}=&#92;tilde{&#92;mathcal{F}}^{h}-&#92;mathcal{F}^{v}=0 " class="latex" /></p>
<p class="p1" style="text-align:left;">We generally can not solve the exact RGE&#8211;but we can try to minimize this Free Energy difference.</p>
<p style="text-align:center;"><strong><span style="color:#008000;"><em>What Kadanoff showed, way back in 1975, is that we can accurately approximate the Exact Renormalization Group Equation by finding a lower bound using this formalism</em></span></strong></p>
<p style="text-align:left;">Deep learning appears to be a real-space variational RG technique, specifically applicable to very complex, inhomogenous systems where the detailed scale transformations have to be learned from the data</p>
<h4>RBMs expressed using Variational RG</h4>
<p>We will now show how to express RBMs using the VRG formalism and provide some intuition</p>
<p>In an RBM, we simply want to learn the Energy function directly; we don&#8217;t specify the Hamiltonian for the visible or hidden units explicitly, like we would in physics.  The RBM Energy is just</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BE%7D%5E%7BRBM%7D%28v%2Ch%29%3D%5Cmathcal%7BH%7D%5E%7BRBM%7D%28v%29-%5Cmathbf%7BV%7D%28v%2Ch%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{E}^{RBM}(v,h)=&#92;mathcal{H}^{RBM}(v)-&#92;mathbf{V}(v,h) " title="&#92;mathbf{E}^{RBM}(v,h)=&#92;mathcal{H}^{RBM}(v)-&#92;mathbf{V}(v,h) " class="latex" /></p>
<p style="text-align:left;">We identify the Hamiltonian for the hidden units as the Renormalized Effective Hamiltonian from the VRG theory</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BH%7D%5E%7BRBM%7D%28h%29%3D%5Chat%7B%5Cmathcal%7BH%7D%7D%28h%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{H}^{RBM}(h)=&#92;hat{&#92;mathcal{H}}(h) " title="&#92;mathbf{H}^{RBM}(h)=&#92;hat{&#92;mathcal{H}}(h) " class="latex" /></p>
<h5>RBM Hamiltonians / Marginal Probabilities</h5>
<p>To obtain RBM Hamiltonians for just the visible <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7BRBM%7D%28v%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}^{RBM}(v) " title="&#92;mathcal{H}^{RBM}(v) " class="latex" /> or hidden <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7BRBM%7D%28h%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}^{RBM}(h) " title="&#92;mathcal{H}^{RBM}(h) " class="latex" /> nodes, we need to integrate out the other nodes; that is, we need to find the marginal probabilities.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=P%28v%29%3D%5Csum%5Climits_%7Bh%7DP%28v%2Ch%29%3D%5Cdfrac%7Be%5E%7B-%5Cmathcal%7BH%7D%5E%7BRBM%7D%28v%29+%7D%7D%7B%5Cmathcal%7BZ%7D%7D%3D%5Cdfrac%7B1%7D%7B%5Cmathcal%7BZ%7D%7D+%5Csum%5Climits_%7Bh%7De%5E%7B-%5Cmathbf%7BE%28v%2Ch%29%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="P(v)=&#92;sum&#92;limits_{h}P(v,h)=&#92;dfrac{e^{-&#92;mathcal{H}^{RBM}(v) }}{&#92;mathcal{Z}}=&#92;dfrac{1}{&#92;mathcal{Z}} &#92;sum&#92;limits_{h}e^{-&#92;mathbf{E(v,h)}} " title="P(v)=&#92;sum&#92;limits_{h}P(v,h)=&#92;dfrac{e^{-&#92;mathcal{H}^{RBM}(v) }}{&#92;mathcal{Z}}=&#92;dfrac{1}{&#92;mathcal{Z}} &#92;sum&#92;limits_{h}e^{-&#92;mathbf{E(v,h)}} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7BRBM%7D%28v%29%3D-%5Cln%5Csum%5Climits_%7Bh%7De%5E%7B-%5Cmathbf%7BE%28v%2Ch%29%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}^{RBM}(v)=-&#92;ln&#92;sum&#92;limits_{h}e^{-&#92;mathbf{E(v,h)}} " title="&#92;mathcal{H}^{RBM}(v)=-&#92;ln&#92;sum&#92;limits_{h}e^{-&#92;mathbf{E(v,h)}} " class="latex" /></p>
<p style="text-align:left;">and</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=P%28h%29%3D%5Csum%5Climits_%7Bv%7DP%28v%2Ch%29%3D%5Cdfrac%7Be%5E%7B-%5Cmathcal%7BH%7D%5E%7BRBM%7D%28h%29+%7D%7D%7B%5Cmathcal%7BZ%7D%7D%3D%5Csum%5Climits_%7Bv%7D%5Cdfrac%7Be%5E%7B-%5Cmathbf%7BE%28v%2Ch%29%7D%7D%7D%7B%5Cmathcal%7BZ%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="P(h)=&#92;sum&#92;limits_{v}P(v,h)=&#92;dfrac{e^{-&#92;mathcal{H}^{RBM}(h) }}{&#92;mathcal{Z}}=&#92;sum&#92;limits_{v}&#92;dfrac{e^{-&#92;mathbf{E(v,h)}}}{&#92;mathcal{Z}} " title="P(h)=&#92;sum&#92;limits_{v}P(v,h)=&#92;dfrac{e^{-&#92;mathcal{H}^{RBM}(h) }}{&#92;mathcal{Z}}=&#92;sum&#92;limits_{v}&#92;dfrac{e^{-&#92;mathbf{E(v,h)}}}{&#92;mathcal{Z}} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BH%7D%5E%7BRBM%7D%28h%29%3D-%5Cln%5Csum%5Climits_%7Bv%7De%5E%7B-%5Cmathbf%7BE%28v%2Ch%29%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathcal{H}^{RBM}(h)=-&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;mathbf{E(v,h)}} " title="&#92;mathcal{H}^{RBM}(h)=-&#92;ln&#92;sum&#92;limits_{v}e^{-&#92;mathbf{E(v,h)}} " class="latex" /></p>
<h5>Training RBMs</h5>
<p>To train an RBM, we apply Contrastive Divergence (CD), or, perhaps today, Persistent Contrastive Divergence (PCD).  We can kindof think of this as slowly approximating</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%5Ctheta%7D%5Cln%5Cmathcal%7BZ%7D%28%5Ctheta%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta}&#92;ln&#92;mathcal{Z}(&#92;theta) " title="&#92;dfrac{&#92;partial}{&#92;partial&#92;theta}&#92;ln&#92;mathcal{Z}(&#92;theta) " class="latex" /></p>
<p>In practice, however, RBM training minimizes the associated Free Energy difference <img src="https://s0.wp.com/latex.php?latex=%5CDelta%5Cmathbf%7BF%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;Delta&#92;mathbf{F} " title="&#92;Delta&#92;mathbf{F} " class="latex" /> &#8230; or something akin to this&#8230;to avoid overfitting.</p>
<p>In the &#8220;Practical Guide to Training Restricted Boltzmann Machines&#8221;, Hinton explains how to train an RBM (circa 2011).  Section 6 addresses &#8220;Monitoring the overfitting&#8221;</p>
<p><em>&#8220;it is possible to directly monitor the overfitting by comparing the free energies of training data and held out validation data&#8230;If the model is not overfitting at all, the average free energy should be about the same on training and validation data&#8221;</em></p>
<h5>Other Objective Functions</h5>
<p><a href="http://arxiv.org/pdf/1301.6323v1.pdf" target="_blank">Modern variants of Real Space VRG</a> are not <em> &#8220;&#8216;forced&#8217; to minimize the global free energy&#8221;</em> and have attempted other approaches such as Tensor-SVD Renormalization.  Likeswise, some RBM / DBM approaches do likewise may minimize a different objective.</p>
<p>In some methods, we minimize the KL Divergence; this has a very natural analog in VRG language [1].</p>
<h4>Why Deep Learning Works: Lessons from Theoretical Physics</h4>
<p>The Renormalization Group Theory provides new insights as to why Deep Learning works so amazingly well.  It is not, however, a complete theory. Rather, it is framework for beginning to understand what is an incredibly powerful, modern, applied tool.  Enjoy!</p>
<h4 style="text-align:left;">References</h4>
<p>[1] <a title="An exact mapping between the Variational Renormalization Group and Deep Learning" href="http://arxiv.org/pdf/1410.3831v1.pdf" target="_blank">An exact mapping between the Variational Renormalization Group and Deep Learning</a>, 2014</p>
<p>[2] <a title="Variational Approximations for Renormalization Group Transformations" href="http://jfi.uchicago.edu/~leop/Oldies%20but%20Goodies/2-Variational%202-Approximations%20for%20Renormalization%20Group%20Transformations.pdf" target="_blank">Variational Approximations for Renormalization Group Transformations</a>, 1976</p>
<p>[3]  <a title="A Common Logic to Seeing Cats and Cosmos" href="https://www.quantamagazine.org/20141204-a-common-logic-to-seeing-cats-and-cosmos" target="_blank">A Common Logic to Seeing Cats and Cosmos</a></p>
<p>[4] <a title="WHY DOES UNSUPERVISED DEEP LEARNING WORK? - A PERSPECTIVE FROM GROUP THEORY" href="http://arxiv.org/pdf/1412.6621v3.pdf" target="_blank">WHY DOES UNSUPERVISED DEEP LEARNING WORK? &#8211; A PERSPECTIVE FROM GROUP THEORY</a>, 2015</p>
<p class="p1"><span class="s1">[5] <a title="A Fundamental Theory to Model the Mind" href="https://www.quantamagazine.org/20140403-a-fundamental-theory-to-model-the-mind/" target="_blank">A Fundamental Theory to Model the Mind</a></span></p>
<p class="p1">[6] <a title="A Practical Guide to Training Restricted Boltzmann Machines" href="https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf" target="_blank">A Practical Guide to Training Restricted Boltzmann Machines</a>, 2010</p>
<p class="p1">[7] <a title="On the importance of initialization and momentum in deep learning" href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf" target="_blank">On the importance of initialization and momentum in deep learning</a>, 2013</p>
<p class="p1">[8] <a title="Dropout: A Simple Way to Prevent Neural Networks from Overfitting" href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>, 2014</p>
<p class="p1">[9] <a title="Ken Wilson, A Scientific Appreciation" href="http://frankwilczek.com/2013/PNAS-2013-Wilczek-1312463110.pdf" target="_blank">Ken Wilson, A Scientific Appreciation</a></p>
<p class="p1">[10] http://www-math.unice.fr/~patras/CargeseConference/ACQFT09_JZinnJustin.pdf</p>
<p class="p1">[11] <a title="Training Products of Experts by Minimizing Contrastive Divergence" href="http://www.cs.toronto.edu/~fritz/absps/nccd.pdf" target="_blank">Training Products of Experts by Minimizing Contrastive Divergence</a>, 2002</p>
<p class="p1">[12] <a title="Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient" href="http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf" target="_blank">Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient</a>, 2008</p>
<p class="p1">[13] <a href="http://www.nonlin-processes-geophys.net/3/102/1996/npg-3-102-1996.pdf" rel="nofollow">http://www.nonlin-processes-geophys.net/3/102/1996/npg-3-102-1996.pdf</a></p>
<p class="p1">[14] <a title="THE RENORMALIZATION GROUP AND CRITICAL PHENOMENA" href="http://www.nobelprize.org/nobel_prizes/physics/laureates/1982/wilson-lecture.pdf" target="_blank">THE RENORMALIZATION GROUP AND CRITICAL PHENOMENA</a>, Ken Wilson Nobel Prize Lecture</p>
<p class="p1">[15] <a title="Scaling, universality, and renormalization: Three pillars of modern critical phenomena" href="http://cps-www.bu.edu/hes/articles/s99a.pdf" target="_blank">Scaling, universality, and renormalization: Three pillars of modern critical phenomena</a></p>
<p>[16] <a title="The Potential Energy of an Autoencode" href="http://www.iro.umontreal.ca/~memisevr/pubs/AEenergy.pdf" target="_blank">The Potential Energy of an Autoencoder</a>, 2014</p>
<p>[17] <a title="http://www.icml-2011.org/papers/455_icmlpaper.pdf" href="http://www.icml-2011.org/papers/455_icmlpaper.pdf" target="_blank"> Contractive Auto-Encoders: Explicit Invariance During Feature Extraction</a>, 2011</p>
<p>[18] <a title="Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion" href="http://jmlr.csail.mit.edu/papers/volume11/vincent10a/vincent10a.pdf" target="_blank">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</a>, 2010</p>
<p>[19] <a title="What is renormalization group theory?" href="http://www.quora.com/What-is-renormalization-group-theory" target="_blank">Quora:  What is Renormalization group theory?</a></p>
<p>[20] <span class="s1"><a href="http://journals.aps.org/prb/abstract/10.1103/PhysRevB.15.4476" target="_blank">Renormalization group and critical localization</a>, 1977</span></p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/charlesmartin14.wordpress.com/6633/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/charlesmartin14.wordpress.com/6633/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=calculatedcontent.com&#038;blog=32496692&#038;post=6633&#038;subd=charlesmartin14&#038;ref=&#038;feed=1" width="1" height="1" />
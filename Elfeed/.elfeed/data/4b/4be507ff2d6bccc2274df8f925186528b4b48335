<p>I am starting a new project to try and reproduce some core deep learning papers in <a href="https://www.tensorflow.org/">TensorFlow</a> from some of the big names.</p>
<p style="text-align:left;"><span style="color:#000000;"><strong>The motivation:   </strong></span><span style="color:#000000;">to understand how to build very deep networks and why they do (or don&#8217;t) work</span><em><span style="color:#000000;">.</span></em></p>
<p>There are several papers that caught my eye, starting with</p>
<ul>
<li><a href="https://arxiv.org/pdf/1003.0358.pdf">Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition</a> (2010)
<ul>
<li>I can find no implementation or data</li>
</ul>
</li>
<li>
<p class="p1"><span class="s1"><a href="https://arxiv.org/pdf/1511.03643.pdf">Unifying Distillation and Privileged Information</a> (2015)  </span></p>
<ul>
<li>Also called student-teacher learning</li>
<li>there is <a href="https://github.com/lopezpaz/distillation_privileged_information">an implementation</a>, but it is unclear what data was used</li>
</ul>
</li>
</ul>
<p>These papers set the foundation for looking at much larger, deeper networks such as</p>
<ul>
<li><a href="http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf">ResNet (Deep Residual Learning)</a>
<ul>
<li>there are several TensorFlow implementations.  I don&#8217;t know which is best</li>
</ul>
</li>
<li><a href="http://arxiv.org/pdf/1505.00387.pdf">Highway Networks</a>
<ul>
<li>see <a href="https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa#.mrn1wpz1s">Jim Flemming&#8217;s post</a> on a TensorFlow implementation</li>
</ul>
</li>
<li>and <a href="https://arxiv.org/abs/1605.07648">FractalNet</a>.
<ul>
<li>an implementation is needed</li>
</ul>
</li>
</ul>
<p>FractalNet&#8217;s are particularly interesting since they suggest that very deep networks do not need student-teacher learning, and, instead, can be self similar.  (which is related to very recent work on the <a href="https://www.youtube.com/watch?v=7KCWcx-YIRI">Statistical Physics of Deep Learning</a>, and the <a href="https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/">Renormalization Group analogy</a>).</p>
<p>IMHO,  it is not enough just to implement the code; the results have to be excellent as well. I am not impressed with the results I have seen so far, and I would like to flush out what is really going on.</p>
<h3>Big Deep Simple Nets</h3>
<p>The 2010 paper still appears to be 1 of the top 10 results on MNIST:</p>
<p><a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html</a></p>
<p>The idea is simple. They claim to get state-of-the-art accuracy on MNIST using a 5-layer MLP, but running a large number of epochs with just SGD, a decaying learning rate, <em>and an augmented data set.</em></p>
<p>The key idea is that the augmented data set can provide, in practice, an infinite amount of training data.  And having infinite data means that we never have to worry about overtraining because we have too many adjustable parameters, and therefore any reasonable size network will do the trick if we just run it long enough.</p>
<p>In other words, there is no convolution gap,  no need for early stopping, or really no regularization at all.</p>
<p>This sounds dubious to me, but I wanted to see for myself.  Also, perhaps I am missing some subtle detail.  Did they clip gradients somewhere ?   Is the activation function central ?  Do we need to tune the learning rate decay ?</p>
<p>I  have <a href="https://github.com/CalculatedContent/big_deep_simple_mlp">initial notebooks on github</a>,  and would welcome feedback and contributions, plus ideas for other papers to reproduce.</p>
<p>I am trying to repeat this experiment using Tensorflow and 2 kinds of augmented data sets:</p>
<ul>
<li><a href="http://leon.bottou.org/projects/infimnist">InfiMNIST</a> (2006) &#8211; provides nearly 1B deformations of MNIST</li>
<li><a href="http://www2.compute.dtu.dk/~sohau/augmentations/">AlignMNIST </a>(2016) &#8211; provides 75-150 epochs of deformed MNIST</li>
</ul>
<p>(and let me say a special personal thanks to <a class="navbar-brand" href="http://www2.compute.dtu.dk/~sohau/">Søren Hauberg</a> for providing this recent data set)</p>
<p>I would like to try other methods, such as the <a href="http://keras.io/preprocessing/image/">Keras Data Augmentation library </a>(see below), or even the recent <a href="https://arxiv.org/pdf/1606.03498v1.pdf">data generation library coming out of OpenAI</a>.</p>
<p>Current results are up for</p>
<ul>
<li><a href="https://github.com/CalculatedContent/big_deep_simple_mlp/blob/master/2-Layer-MLP-AlignMNiST.ipynb" target="_blank">2 Layer AlignMNIST</a>   75 epochs</li>
<li><a href="https://github.com/CalculatedContent/big_deep_simple_mlp/blob/master/5-Layer-MLP-AlignMNiST.ipynb" target="_blank">5 LayerAlignMNIST</a>   75 epochs</li>
<li><a href="https://github.com/CalculatedContent/big_deep_simple_mlp/blob/master/2-Layer-MLP-InfiMNiST.ipynb" target="_blank">2 Layer InfiMNIST</a>   500 epochs</li>
<li>5<a href="https://github.com/CalculatedContent/big_deep_simple_mlp/blob/master/2-Layer-MLP-InfiMNiST.ipynb" target="_blank"> Layer InfiMNIST</a>   500 epochs</li>
</ul>
<p>The initial results indicate that AlignMNIST is much better that InfiMNIST for this simple MLP, although I still do not see the extremely high, top-10 accuracy reported.</p>
<p>Furthermore, the 5-Layer InfiMNIST actually diverges after ~100 epochs.  So we still need early stopping, even with an <em>infinite</em> amount of data.</p>
<p>It may be interesting try using the Keras ImageDataGenerator class, described in this related blog on &#8220;<a href="http://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" target="_blank">building powerful image classification models using very little data</a>&#8221;</p>
<p>Also note that <a href="https://arxiv.org/pdf/1606.03498v1.pdf">the OpenAI group as released a new paper</a> and <a href="https://github.com/openai/improved-gan">code for creating data</a> used in generative adversarial networks (GANs).</p>
<p>I will periodically update this blog as new data comes in, and I have the time to implement these newer techniques.</p>
<p>Next, we will check in the log files and discuss the tensorboard results.</p>
<p>Comments, criticisms, and contributions are very welcome.</p>
<p>(<a href="https://gitter.im/CalculatedContent/big_deep_simple_mlp?utm_source=share-link&amp;utm_medium=link&amp;utm_campaign=share-link">chat on gitter</a> )</p>
<p>&nbsp;</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/charlesmartin14.wordpress.com/8913/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/charlesmartin14.wordpress.com/8913/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=calculatedcontent.com&#038;blog=32496692&#038;post=8913&#038;subd=charlesmartin14&#038;ref=&#038;feed=1" width="1" height="1" />
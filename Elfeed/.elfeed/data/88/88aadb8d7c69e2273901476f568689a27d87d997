<p><em>Update: this content (and more) is now available as <a href="https://arxiv.org/abs/1610.03585">a more thorough abstract
on arXiv</a>, co-authored with my OpenAI colleague Igor Mordatch</em>.</p>

<p>I&rsquo;ve been really pleased with the response to my last post, <a href="http://www.foldl.me/2016/solving-language/"><em>On &ldquo;solving
language.&rdquo;</em></a> While I certainly wasn&rsquo;t saying anything revolutionary, it
does seem that I managed to capture some very common sentiment floating around
in the AI community today. I think the post has served as a clear checkpoint
for me and for people with similar interests: it&rsquo;s time to focus on language
in a situated, interactive setting!</p>

<p>Since that time in mid-August, I&rsquo;ve been working on a paradigm for simulating
situated language acquisition. This post will give a brief overview of the
motivating ideas, and I&rsquo;ll follow up shortly with more concrete details on some
experiments I&rsquo;ve been doing recently.</p>

<p><small>(Before I get started: this space is rapidly increasing in activity,
        which is certainly a good thing for science! Facebook Research just
        released their <a href="https://github.com/facebookresearch/CommAI-env">Environment for Communication-based AI</a>, and there
        have been murmurs of other similar environments around the Internets.)
</small></p>

<h2 id="the-paradigm">The paradigm</h2>

<p>One of the key points of <a href="http://www.foldl.me/2016/solving-language/"><em>&ldquo;Solving language&rdquo;</em></a> was that natural language
dialogue is necessarily situated in some grounded context. We use language (and
other tools) to accomplish real-world goals, which are themselves often not
linguistic. The reference-game example in that post gave one instance of
linguistic behavior that was strongly tied to nonlinguistic world knowledge —
something we can&rsquo;t solve as a language problem in isolation.</p>

<p>If we&rsquo;re interested in building language agents which can eventually cooperate
with us via language in similarly grounded contexts, then the learning tasks we
design should reflect this goal.</p>

<p>I&rsquo;ve followed this idea through to design a general paradigm for situated
language acquisition. In this paradigm, cooperative agents teach or learn a
language in order to accomplish some nonlinguistic goal. Here are the details:</p>

<ol>
  <li>A <em>child</em> agent lives in some grounded world and has some goal which is
<strong>nonlinguistic</strong> (e.g. reach a goal region, get food, etc.).</li>
  <li>The child has only partial observations of its environment, and can take
only a subset of the necessary actions to reach its goal.</li>
  <li>A <em>parent</em> agent also exists in this world. The parent speaks some fixed
language and wants to cooperate with the child (to help it reach its goal).</li>
  <li>The parent has full observations from the environment, and can take actions
which the child cannot take on its own.</li>
  <li>The child and parent can communicate via a language channel.</li>
</ol>

<p>The environment is designed such that the child cannot accomplish the goal on
its own; it must employ the help of its parent. The child acquires language
<strong>as a side effect of accomplishing its grounded goal</strong>: it is the most
efficient (or perhaps the only efficient) mechanism for reaching its main goal.</p>

<h2 id="philosophizing">Philosophizing</h2>

<p>To clearly restate: a critical and distinguishing factor of this framework is
that the child acquires language only as a side effect of striving for some
grounded, nonlinguistic goal.</p>

<p>The environment is designed in particular to avoid <strong>reifying</strong> &ldquo;language.&rdquo; I
think it is misleading to see language as some sort of unitary <em>thing</em> to be
solved — as just one of a few isolated tools in the toolbox of cognition that
need to be picked up on the way to general artificial intelligence.</p>

<p><a href="https://en.wikipedia.org/wiki/Language-game_(philosophy)"><strong>Language is defined by its use.</strong></a> Language-enabled agents are not identified
their next-word prediction perplexity or their part-of-speech tag confusion
matrix, but by their ability to cooperate with other agents through language.
We shouldn&rsquo;t expect the latter to magically emerge from hill-climbing on any of
the former.</p>

<p>As I&rsquo;ll show in my next post, it&rsquo;s within our reach to design simple environments
that let us directly hill-climb on this objective of cooperation through language.
Stay tuned!<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>And please get in touch! I always enjoy hearing new ideas from my readers. (All four of you. ;) )&nbsp;<a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div><img src="http://feeds.feedburner.com/~r/foldl/rss/~4/Cxmz_YaNDsI" height="1" width="1" alt=""/>
<p>Training a novel network on the <a href="http://image-net.org/challenges/LSVRC/2012/index">ImageNet dataset</a> can be tricky. Here are some guidelines to make your model train faster and help you design better models.</p>

<p>For a more in-depth analysis and comparison of all the networks record-winners on ImageNet, please see our <a href="https://arxiv.org/abs/1605.07678">recent article</a>. One representative figure from this article is here:</p>

<p><img src="/assets/nets/acc_vs_net_vs_ops.svg" alt="" /></p>

<p>This work is based on the model in our recent paper <a href="https://arxiv.org/abs/1606.02147">ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</a>. ENet network model is given <a href="https://github.com/e-lab/ENet-training/blob/master/train/models/encoder.lua">here</a>.</p>

<p>An easy way to train a neural network model on ImageNet is to use <a href="http://torch.ch/">Torch7</a> and <a href="https://github.com/soumith/imagenet-multiGPU.torch">this training script</a> from <a href="https://github.com/soumith">Soumith Chintala</a>. Another great training script, also deriving from this one, is <a href="https://github.com/facebook/fb.resnet.torch">here</a>.</p>

<p>Regardless, ENet needs to be modified to run on ImageNet images of 224x224 size. This is done by modifying a downsampling bottleneck function at the end of the last 2 modules (in the for loop). This is ENet V2:</p>

<div class="language-lua highlighter-rouge"><pre class="highlight"><code><span class="kd">local</span> <span class="n">initial_block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ConcatTable</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
   <span class="n">initial_block</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialConvolution</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
   <span class="n">initial_block</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialMaxPooling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">initial_block</span><span class="p">)</span> <span class="c1">-- size of 112x112</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">JoinTable</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="c1">-- can't use Concat, because SpatialConvolution needs contiguous gradOutput</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">SpatialBatchNormalization</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">PReLU</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- size of 56x56</span>
   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">4</span> <span class="k">do</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
   <span class="k">end</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- size of 28x28</span>
   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">2</span> <span class="k">do</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">dbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">wbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xdbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xxdbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">wbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xxxdbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- size of 14x14, then 7x7</span>
   <span class="k">end</span>
   <span class="c1">-- global average pooling 1x1</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialAveragePooling</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre>
</div>

<p>We used Soumith training script invoked with the following command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>th main.lua -data /media/SuperSSD/ILSVRC2012 -backend cudnn -netType enet -nGPU 4 -batchSize 128 -nDonkeys 12
</code></pre>
</div>

<p>To justify these choices: we have 4 GPUs in our machine and 6 processors with 12 total threads, so we use 12 Donkeys to load data. That makes a big difference in loading data and overall time. Then you will get the following results:</p>

<p><img src="/assets/enet/v23.png" alt="" /></p>

<p>As you can see ENet V2 trains slowly, because the training script use a fairly conservative learning rate (LR) and weight decay (WD).</p>

<div class="language-lua highlighter-rouge"><pre class="highlight"><code>    <span class="kd">local</span> <span class="n">regimes</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1">-- start, end,    LR,   WD,</span>
        <span class="p">{</span>  <span class="mi">1</span><span class="p">,</span>     <span class="mi">18</span><span class="p">,</span>   <span class="mf">1e-2</span><span class="p">,</span>   <span class="mf">5e-4</span><span class="p">,</span> <span class="p">},</span>
        <span class="p">{</span> <span class="mi">19</span><span class="p">,</span>     <span class="mi">29</span><span class="p">,</span>   <span class="mf">5e-3</span><span class="p">,</span>   <span class="mf">5e-4</span>  <span class="p">},</span>
        <span class="p">{</span> <span class="mi">30</span><span class="p">,</span>     <span class="mi">43</span><span class="p">,</span>   <span class="mf">1e-3</span><span class="p">,</span>   <span class="mi">0</span> <span class="p">},</span>
        <span class="p">{</span> <span class="mi">44</span><span class="p">,</span>     <span class="mi">52</span><span class="p">,</span>   <span class="mf">5e-4</span><span class="p">,</span>   <span class="mi">0</span> <span class="p">},</span>
        <span class="p">{</span> <span class="mi">53</span><span class="p">,</span>    <span class="mf">1e8</span><span class="p">,</span>   <span class="mf">1e-4</span><span class="p">,</span>   <span class="mi">0</span> <span class="p">},</span>
    <span class="p">}</span>
</code></pre>
</div>

<p>See how long training is flat in the 1st and 2nd regime! Too much wasted time.</p>

<p>So for ENet V3, we decided to modify the training regime to go faster:</p>

<div class="language-lua highlighter-rouge"><pre class="highlight"><code>    <span class="kd">local</span> <span class="n">regimes</span> <span class="o">=</span> <span class="p">{</span>
        <span class="c1">-- start, end,    LR,   WD,</span>
        <span class="p">{</span>  <span class="mi">1</span><span class="p">,</span>     <span class="mi">10</span><span class="p">,</span>   <span class="mf">1e-2</span><span class="p">,</span>   <span class="mf">5e-4</span><span class="p">,</span> <span class="p">},</span>
        <span class="p">{</span> <span class="mi">11</span><span class="p">,</span>     <span class="mi">15</span><span class="p">,</span>   <span class="mf">5e-3</span><span class="p">,</span>   <span class="mf">5e-4</span>  <span class="p">},</span>
        <span class="p">{</span> <span class="mi">16</span><span class="p">,</span>     <span class="mi">20</span><span class="p">,</span>   <span class="mf">1e-3</span><span class="p">,</span>   <span class="mi">0</span> <span class="p">},</span>
        <span class="p">{</span> <span class="mi">21</span><span class="p">,</span>     <span class="mi">30</span><span class="p">,</span>   <span class="mf">5e-4</span><span class="p">,</span>   <span class="mi">0</span> <span class="p">},</span>
        <span class="p">{</span> <span class="mi">31</span><span class="p">,</span>    <span class="mf">1e8</span><span class="p">,</span>   <span class="mf">1e-4</span><span class="p">,</span>   <span class="mi">0</span> <span class="p">},</span>
    <span class="p">}</span>
</code></pre>
</div>

<p>Also ENet V3 was modified to have more output features: 512 like ResNet 18 and 34. Here is ENet V3 model:</p>

<div class="language-lua highlighter-rouge"><pre class="highlight"><code>   <span class="kd">local</span> <span class="n">initial_block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ConcatTable</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
   <span class="n">initial_block</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialConvolution</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
   <span class="n">initial_block</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialMaxPooling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">initial_block</span><span class="p">)</span> <span class="c1">-- size of 112x112</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">JoinTable</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">SpatialBatchNormalization</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">PReLU</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- size of 56x56</span>
   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">4</span> <span class="k">do</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
   <span class="k">end</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- size of 28x28</span>
   
   <span class="c1">-- pass 1:</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">dbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">wbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xdbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xxdbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">wbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xxxdbottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- size of 14x14</span>

   <span class="c1">--pass 2:</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">dbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">wbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xdbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xxdbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">wbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">xxxdbottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- size of 7x7</span>


   <span class="c1">-- global average pooling 1x1</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialAveragePooling</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre>
</div>

<p>As a result (purple top left plots), you can see ENet V3 training faster, about 2x faster! Also it does a bit better in performance, given the model now has more output features!</p>

<p>Then we read <a href="https://arxiv.org/abs/1606.02228">this paper</a> suggesting that linear learning rate updates may be better. So we tried this in ENet V6, basically identical to V3. Here are the results (green plots):</p>

<p><img src="/assets/enet/v236.png" alt="" /></p>

<p>The function to update the weight is given below:</p>

<div class="language-lua highlighter-rouge"><pre class="highlight"><code><span class="kd">local</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span>
<span class="kd">local</span> <span class="k">function</span> <span class="nf">paramsForEpoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">opt</span><span class="p">.</span><span class="n">LR</span> <span class="o">~=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">then</span> <span class="c1">-- if manually specified</span>
         <span class="n">lr</span> <span class="o">=</span> <span class="n">opt</span><span class="p">.</span><span class="n">LR</span>
         <span class="k">return</span> <span class="p">{</span> <span class="p">}</span>
      <span class="k">elseif</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">then</span>
         <span class="n">lr</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">1</span>
         <span class="k">return</span> <span class="p">{}</span>
      <span class="k">else</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">epoch</span><span class="o">/</span><span class="n">opt</span><span class="p">.</span><span class="n">nEpochs</span><span class="p">)</span> <span class="c1">--  math.pow( 0.9, epoch - 1)</span>
      <span class="k">end</span>
     <span class="kd">local</span> <span class="n">regimes</span> <span class="o">=</span> <span class="p">{</span>
         <span class="c1">-- start, end,     WD,</span>
         <span class="p">{</span>  <span class="mi">1</span><span class="p">,</span>     <span class="mi">18</span><span class="p">,</span>   <span class="mf">5e-4</span><span class="p">,</span> <span class="p">},</span>
         <span class="p">{</span> <span class="mi">19</span><span class="p">,</span>     <span class="mi">29</span><span class="p">,</span>  <span class="mf">5e-4</span>  <span class="p">},</span>
         <span class="p">{</span> <span class="mi">30</span><span class="p">,</span>     <span class="mi">43</span><span class="p">,</span>    <span class="mi">0</span> <span class="p">},</span>
         <span class="p">{</span> <span class="mi">44</span><span class="p">,</span>     <span class="mi">52</span><span class="p">,</span>   <span class="mi">0</span> <span class="p">},</span>
         <span class="p">{</span> <span class="mi">53</span><span class="p">,</span>    <span class="mf">1e8</span><span class="p">,</span>  <span class="mi">0</span> <span class="p">},</span>
     <span class="p">}</span>
     <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="k">in</span> <span class="nb">ipairs</span><span class="p">(</span><span class="n">regimes</span><span class="p">)</span> <span class="k">do</span>
         <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">&lt;=</span> <span class="n">row</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">then</span>
             <span class="k">return</span> <span class="p">{</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="n">lr</span> <span class="p">,</span> <span class="n">weightDecay</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="p">},</span> <span class="kc">true</span>
         <span class="k">end</span>
     <span class="k">end</span>
 <span class="k">end</span>
</code></pre>
</div>

<p>This is great, but as you can see not much different that the previous regimes used in V3.</p>

<p>Then we noticed that all ENet are ResNet-like network models, and so we looked at <a href="https://github.com/facebook/fb.resnet.torch">this FB training script</a>. Here they used a linear LR and a fixed WD of 1e-4. Adopting this and testing on ENet V7 gave us the red and orange plots:</p>

<p><img src="/assets/enet/v2367.png" alt="" /></p>

<p>This gave us the best results, and now it trains in ~ 10 epochs, which is 4x faster than what we started with. We used this learning rate function:</p>

<div class="language-lua highlighter-rouge"><pre class="highlight"><code><span class="kd">local</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span>
<span class="kd">local</span> <span class="k">function</span> <span class="nf">paramsForEpoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">opt</span><span class="p">.</span><span class="n">LR</span> <span class="o">~=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">then</span> <span class="c1">-- if manually specified</span>
         <span class="n">lr</span> <span class="o">=</span> <span class="n">opt</span><span class="p">.</span><span class="n">LR</span>
         <span class="k">return</span> <span class="p">{</span> <span class="p">}</span>
      <span class="k">elseif</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">then</span>
         <span class="n">lr</span> <span class="o">=</span> <span class="mi">0</span><span class="p">.</span><span class="mi">1</span>
         <span class="k">return</span> <span class="p">{</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="n">lr</span><span class="p">,</span> <span class="n">weightDecay</span><span class="o">=</span><span class="mf">1e-4</span> <span class="p">}</span>
      <span class="k">else</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="nb">math.pow</span><span class="p">(</span> <span class="mi">0</span><span class="p">.</span><span class="mi">9</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span> <span class="n">learningRate</span> <span class="o">=</span> <span class="n">lr</span><span class="p">,</span> <span class="n">weightDecay</span><span class="o">=</span><span class="mf">1e-4</span> <span class="p">},</span> <span class="kc">true</span>
      <span class="k">end</span>
 <span class="k">end</span>
</code></pre>
</div>

<p>This learning rate update was recommended by SangPil Kim.</p>

<p>ENet V7 is a bit different. It removed all dilated and asymmetric convolutions and instead uses ResNet-like modules. ENet V7 model is here:</p>

<div class="language-lua highlighter-rouge"><pre class="highlight"><code>   <span class="kd">local</span> <span class="n">initial_block</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ConcatTable</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
   <span class="n">initial_block</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialConvolution</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
   <span class="n">initial_block</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialMaxPooling</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">initial_block</span><span class="p">)</span> <span class="c1">-- 112x112</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">JoinTable</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">SpatialBatchNormalization</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">PReLU</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- 56x56</span>

   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">5</span> <span class="k">do</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
   <span class="k">end</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- 28x28</span>

   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">5</span> <span class="k">do</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
   <span class="k">end</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- 14x14</span>

   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">5</span> <span class="k">do</span>
      <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
   <span class="k">end</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">bottleneck</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="kc">true</span><span class="p">))</span> <span class="c1">-- 7x7</span>

   <span class="c1">-- global average pooling 1x1</span>
   <span class="n">features</span><span class="p">:</span><span class="n">add</span><span class="p">(</span><span class="n">cudnn</span><span class="p">.</span><span class="n">SpatialAveragePooling</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</code></pre>
</div>

<p>Changing WD from 1e-4 to 0 after 7 epochs did not give better results.</p>

<h1 id="a-balance-of-power">a balance of power</h1>

<p>After a few experiments, ENet V12 gave really good results. Here is the model:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>   local initial_block = nn.ConcatTable(2)
   initial_block:add(cudnn.SpatialConvolution(3, 13, 3, 3, 2, 2, 1, 1))
   initial_block:add(cudnn.SpatialMaxPooling(2, 2, 2, 2))

   features:add(initial_block) -- 112x112
   features:add(nn.JoinTable(2))
   features:add(nn.SpatialBatchNormalization(16, 1e-3))
   features:add(nn.PReLU(16))

   -- 1st block
   features:add(ibottleneck(16, 64, true)) -- 56x56
   features:add(bottleneck(64, 128))
   features:add(bottleneck(128, 128))
   
   -- 2nd block: dilation of 2
   features:add(bottleneck(128, 256, true)) -- 28x28
   features:add(bottleneck(256, 256))
   features:add(dbottleneck(256, 256))

   -- 3rd block: dilation 4
   features:add(bottleneck(256, 512, true)) -- 14x14
   features:add(bottleneck(512, 512))
   features:add(xdbottleneck(512, 512))

   -- 4th block, dilation 8
   features:add(bottleneck(512, 1024, true)) -- 7x7
   features:add(bottleneck(1024, 1024))
   features:add(xxdbottleneck(1024, 1024))

   -- global average pooling 1x1
   features:add(cudnn.SpatialAveragePooling(7, 7, 1, 1, 0, 0))
</code></pre>
</div>

<p>With the training regime listed below, it reached 65.1% in 17 epochs:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>local function paramsForEpoch(epoch)
      if opt.LR ~= 0.0 and epoch == 1 then -- if manually specified
         lr = opt.LR
         return { }
      elseif epoch == 1 then
         lr = 0.1
         return { learningRate = lr, weightDecay=1e-4 }
      else
        lr = lr * math.pow( 0.95, epoch - 1)
        if epoch &gt; 7 then wd = 0 else wd = 1e-4 end
        return { learningRate = lr, weightDecay=wd }, true
      end
 end
</code></pre>
</div>

<p>Notice this is the same function as for V7, but with a slower 0.95 instead of 0.9 as decay exponent to the learning rate.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>local function paramsForEpoch(epoch)
      if opt.LR ~= 0.0 and epoch == 1 then -- if manually specified
         lr = opt.LR
         return { }
      elseif epoch == 1 then
         lr = 0.1
         return { learningRate = lr, weightDecay=1e-4 }
      elseif epoch &gt; 15 then
         lr = lr * math.pow( 0.95, epoch - 15) 
         wd = 0 
         return { learningRate = lr, weightDecay=wd }, true
      else 
         wd = 1e-4
         return { learningRate = lr, weightDecay=wd }, true
      end
 end
</code></pre>
</div>

<p>We experimented with ENEt V12 with slowing down learning again, to see if it could give better results. And it did! ENet V12 reached 68.4% accuracy in approximately 30 epochs.</p>

<h1 id="more-experiments">more experiments</h1>

<p>To experiment more on ENet, we decided to see the effect of dilation, dropout using the fast training mode in order to save time.</p>

<ul>
  <li>
    <p>no dilated convolutions resulted in 64.2% in 18 epochs. This is ENet V13:</p>

    <p>```
     local initial_block = nn.ConcatTable(2)
     initial_block:add(cudnn.SpatialConvolution(3, 13, 3, 3, 2, 2, 1, 1))
     initial_block:add(cudnn.SpatialMaxPooling(2, 2, 2, 2))</p>

    <div class="highlighter-rouge"><pre class="highlight"><code> features:add(initial_block)  -- 112x112
 features:add(nn.JoinTable(2))
 features:add(nn.SpatialBatchNormalization(16, 1e-3))
 features:add(nn.PReLU(16))

 -- 1st block
 features:add(ibottleneck(16, 64, true)) -- 56x56
 features:add(bottleneck(64, 128))
 features:add(bottleneck(128, 128))

 -- 2nd block
 features:add(bottleneck(128, 256, true)) -- 28x28
 features:add(bottleneck(256, 256))
 features:add(bottleneck(256, 256))

 -- 3rd block
 features:add(bottleneck(256, 512, true)) -- 14x14
 features:add(bottleneck(512, 512))
 features:add(bottleneck(512, 512))

 -- 4th block
 features:add(bottleneck(512, 1024, true)) -- 7x7
 features:add(bottleneck(1024, 1024))
 features:add(bottleneck(1024, 1024))

 -- global average pooling 1x1
 features:add(cudnn.SpatialAveragePooling(7, 7, 1, 1, 0, 0))   ```
</code></pre>
    </div>
  </li>
  <li>
    <p>no dropout used in bottleneck resulted in 64.7% in 18 epochs for ENet V14.</p>
  </li>
  <li>
    <p>The initial bock of ENet concatenates the input and a filtered version of the input:</p>

    <p><code class="highlighter-rouge">lua
  local initial_block = nn.ConcatTable(2)
     initial_block:add(cudnn.SpatialConvolution(3, 13, 3, 3, 2, 2, 1, 1))
     initial_block:add(cudnn.SpatialMaxPooling(2, 2, 2, 2))
  ...
 </code></p>

    <p>Adding more features (29 instead of 13) to the convolution did not have any improvements on accuracy. 64.7% with 29 versus 64.9 with 13 features in ENEt V17-18 (not reported here because similar to V13)</p>

    <p>Notice this block is different from ResNet, where instead they use this kind of initial block:</p>

    <p><code class="highlighter-rouge">lua
     features:add(cudnn.SpatialConvolution(3, 64, 7, 7, 2, 2, 3, 3))
     features:add(cudnn.SpatialMaxPooling(3, 3, 2, 2))
     features:add(nn.SpatialBatchNormalization(64, 1e-3))
     features:add(nn.ReLU(64))
 </code></p>

    <p>ENet initial block is much more efficient: 13 convolutions of 3x3 compared to 64 convolutions of 7x7, or 26x times more operations.</p>
  </li>
</ul>

<p>Compare these results to the V12 fast training mode reaching 65.1% in 17 epochs. Here are the learning profiles of V12:</p>

<p><img src="/assets/enet/v12.png" alt="" /></p>

<h1 id="comparison-to-resnet">Comparison to ResNet</h1>

<p>The best ENet gave a 68.4% (V12) accuracy on test set. ResNet 18 is 69.5% and ResNet34 is 73.2 as reported <a href="https://github.com/facebook/fb.resnet.torch">here</a>.</p>

<p>ResNet 18 uses a total of 3.6 G-Ops on a 224x224 input image. ENet 1.6 G-ops on the same image. And on a 960x540 input image ResNet 18 uses 38 G-Ops, while ENet uses 16.9 G-ops on the same image.</p>

<p>This means ENet is 2.24x more efficient that ResNet 18.</p>

<p><img src="/assets/nets/acc_dens_vs_net.svg" alt="" /></p>

<p>As you can see in this figure ENet has the highest accuracy per parameter used of any neural network out there!</p>


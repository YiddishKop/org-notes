<p>Restricted Boltzmann Machines (RBMs) are like the H atom of Deep Learning.</p>
<p>They are basically a solved problem, and while of academic interest, not really used in complex modeling problems.  <a href="https://www.quora.com/Is-Unsupervised-pre-training-and-Greedy-layer-wise-pre-training-methods-considered-obsolete-for-modern-Deep-Learning/answer/Alexander-Ororbia">They were, upto 10 years ago, used for pretraining deep supervised net</a>s.  Today, we can train very deep, supervised nets directly.</p>
<p style="text-align:center;"><em><strong><span style="color:#333399;">RBMs are the foundation of unsupervised deep learning&#8211;</span></strong></em></p>
<p style="text-align:center;"><em><strong><span style="color:#333399;">an unsolved problem.</span> </strong></em></p>
<p> RBMs appear to outperform Variational Auto Encoders (VAEs) on simple data sets like the <a href="http://cims.nyu.edu/~brenden/LakeEtAl2015Science.pdf">Omniglot set&#8211;a data set developed for one shot learning</a>, and <a href="https://arxiv.org/pdf/1603.05106.pdf">used in deep learning research</a>.</p>
<figure data-shortcode="caption" id="attachment_9875" style="width: 310px" class="wp-caption aligncenter"><a href="https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-11-15-50-pm/#main" rel="attachment wp-att-9875"><img data-attachment-id="9875" data-permalink="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-11-15-50-pm/" data-orig-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-11-15-50-pm.png" data-orig-size="336,272" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2016-10-21-at-11-15-50-pm" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-11-15-50-pm.png?w=300&#038;h=243" data-large-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-11-15-50-pm.png?w=336" class="size-medium wp-image-9875" src="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-11-15-50-pm.png?w=300&#038;h=243" alt="Omniglot Dataset" width="300" height="243" srcset="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-11-15-50-pm.png?w=300&amp;h=243 300w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-11-15-50-pm.png?w=150&amp;h=121 150w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-11-15-50-pm.png 336w" sizes="(max-width: 300px) 100vw, 300px" /></a><figcaption class="wp-caption-text">Omniglot Dataset</figcaption></figure>
<p>RBM research continues in areas like <a href="http://www.david-reitter.com/pub/ororbia_deep_hybrid_ecml_2015.pdf">semi-supervised learning with deep hybrid architectures</a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4725829/">Temperature dependence</a>,  <a href="https://arxiv.org/pdf/1502.02476.pdf">infinitely deep RBMs</a>, etc.</p>
<p>Many of basic concepts of Deep Learning are found in RBMs.</p>
<p style="text-align:center;"><span style="color:#333399;"><em><strong>Sometimes clients ask, &#8220;how is Physical Chemistry related to Deep Learning ?&#8221;</strong></em></span></p>
<p>In this post, I am going to discuss a recent advanced in RBM theory based on ideas from theoretical condensed matter physics and physical chemistry,</p>
<p style="text-align:center;"><strong>the Extended Mean Field Restricted Boltzmann Machine: EMF_RBM</strong></p>
<p style="text-align:center;"><strong>(see: <a href="https://arxiv.org/pdf/1506.02914.pdf">Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy</a> ) </strong></p>
<p>[Along the way, we will encounter several Nobel Laureates, including the physicists David J Thouless (2016) and Philip W. Anderson (1977), and the physical chemist Lars Onsager (1968).]</p>
<p>RBMs are pretty simple, and easily implemented from scratch.  The original EMF_RBM is in Julia; I have ported EMF_RBM to python, in the style of t<a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html" target="_blank">he scikit-learn BernoulliRBM package</a>.</p>
<h3>Show me the code</h3>
<p><a href="https://github.com/charlesmartin14/emf-rbm/blob/master/EMF_RBM_Test.ipynb">https://github.com/charlesmartin14/emf-rbm/blob/master/EMF_RBM_Test.ipynb</a></p>
<h3>Theory</h3>
<p>We examined RBMs in the last post <a href="https://charlesmartin14.wordpress.com/2016/09/10/on-cheap-learning-partition-functions-and-rbms/" target="_blank">on Cheap Learning: Partition Functions and RBMs</a>. I will build upon that here, within the context of statistical mechanics.</p>
<p>RBMs are defined by the Energy function</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=E%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%3D-%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7BW%7D%5Cmathbf%7Bh%7D-%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7Ba%7D-%5Cmathbf%7Bb%7D%5E%7BT%7D%5Cmathbf%7Bh%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="E(&#92;mathbf{v},&#92;mathbf{h})=-&#92;mathbf{v}^{T}&#92;mathbf{W}&#92;mathbf{h}-&#92;mathbf{v}^{T}&#92;mathbf{a}-&#92;mathbf{b}^{T}&#92;mathbf{h} " title="E(&#92;mathbf{v},&#92;mathbf{h})=-&#92;mathbf{v}^{T}&#92;mathbf{W}&#92;mathbf{h}-&#92;mathbf{v}^{T}&#92;mathbf{a}-&#92;mathbf{b}^{T}&#92;mathbf{h} " class="latex" /></p>
<p style="text-align:left;">To train an RBM, we minimize the log likelihood ,</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%3D%3D-F%5E%7Bc%7D%28%5Cmathbf%7Bv%7D%29-F%28%5Cmathbf%7Bv%2Ch%7D%29+%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathcal{L}==-F^{c}(&#92;mathbf{v})-F(&#92;mathbf{v,h})  " title="&#92;mathcal{L}==-F^{c}(&#92;mathbf{v})-F(&#92;mathbf{v,h})  " class="latex" /></p>
<p>the sum of the clamped and (actual) Free Energies, where</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%5E%7Bc%7D%3Dln%5C%3B%5Cunderset%7B%5Cmathbf%7Bh%7D%7D%7B%5Csum%7D%5C%3BE%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F^{c}=ln&#92;;&#92;underset{&#92;mathbf{h}}{&#92;sum}&#92;;E(&#92;mathbf{v},&#92;mathbf{h}) " title="F^{c}=ln&#92;;&#92;underset{&#92;mathbf{h}}{&#92;sum}&#92;;E(&#92;mathbf{v},&#92;mathbf{h}) " class="latex" /></p>
<p>and</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%3D%5Cln%5C%3BZ%28%5Cmathbf%7Bv%2Ch%7D%29%3Dln%5C%3B%5Cunderset%7B%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%7D%7B%5Csum%7D%5C%3BE%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29%5C%3B%5C%3B%2C+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F=&#92;ln&#92;;Z(&#92;mathbf{v,h})=ln&#92;;&#92;underset{&#92;mathbf{v},&#92;mathbf{h}}{&#92;sum}&#92;;E(&#92;mathbf{v},&#92;mathbf{h})&#92;;&#92;;, " title="F=&#92;ln&#92;;Z(&#92;mathbf{v,h})=ln&#92;;&#92;underset{&#92;mathbf{v},&#92;mathbf{h}}{&#92;sum}&#92;;E(&#92;mathbf{v},&#92;mathbf{h})&#92;;&#92;;, " class="latex" /></p>
<p style="text-align:left;">The sums range over a space of  <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%7B2%5E%7BN%7D%7D%29%5C%3B%5C%3B+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathcal{O}({2^{N}})&#92;;&#92;; " title="&#92;mathcal{O}({2^{N}})&#92;;&#92;; " class="latex" />,</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D%5Cin%5B0%2C1%5D%5E%7BN_v%7D%2C%5Cmathbf%7Bh%7D%5Cin%5B0%2C1%5D%5E%7BN_h%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v}&#92;in[0,1]^{N_v},&#92;mathbf{h}&#92;in[0,1]^{N_h} " title="&#92;mathbf{v}&#92;in[0,1]^{N_v},&#92;mathbf{h}&#92;in[0,1]^{N_h} " class="latex" /></p>
<p style="text-align:left;">which is intractable in most cases.</p>
<p style="text-align:center;"><strong><span style="color:#ff0000;">Training an RBM requires computing the log Free Energy; this is hard.</span></strong></p>
<p>When training an RBM,  we</p>
<ol>
<li>take a few &#8216;steps toward equilibration&#8217;, to approximate <strong>F</strong></li>
<li>take a gradient step, <img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial+F%7D%7B%5Cpartial+w_%7Bi%2Cj%7D%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial F}{&#92;partial w_{i,j}} " title="&#92;dfrac{&#92;partial F}{&#92;partial w_{i,j}} " class="latex" />, to get <strong>W, a, b</strong></li>
<li>regularize <strong>W </strong>(i.e. by weight decay)</li>
<li>update <strong>W, a, b </strong></li>
<li>repeat until some stopping criteria is met</li>
</ol>
<p>We don&#8217;t include label information, although a trained RBM can provide features for a down-stream classifier.</p>
<h4 style="text-align:left;">Extended Mean Field Theory:</h4>
<p style="text-align:left;">The Extended Mean Field (EMF)  RBM is a straightforward application of known statistical mechanics theories.</p>
<p style="text-align:left;">There are, literally, thousands of papers on spin glasses.</p>
<p style="text-align:center;"><strong><span style="color:#008000;"><em>The EMF RBM is a great example of how to operationalize spin glass theory.</em></span></strong></p>
<h4>Mean Field Theory</h4>
<p>The Restricted Boltzmann Machine has a very simple Energy function, which makes it very easy to factorize the partition function Z , <a href="https://www.youtube.com/watch?v=lekCh_i32iE" target="_blank">explained by Hugo Larochelle</a>, to obtain the conditional probabilities</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28h_%7Bi%7D%3D1%7Cv%29%3D%5Csigma%28b_%7Bi%7D%2B%5Cmathbf%7BW%7D_%7Bi%7D%5Cmathbf%7Bh%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(h_{i}=1|v)=&#92;sigma(b_{i}+&#92;mathbf{W}_{i}&#92;mathbf{h}) " title="p(h_{i}=1|v)=&#92;sigma(b_{i}+&#92;mathbf{W}_{i}&#92;mathbf{h}) " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=p%28v_%7Bi%7D%3D1%7Ch%29%3D%5Csigma%28a_%7Bi%7D%2B%5Cmathbf%7BW%7D_%7Bi%7D%5Cmathbf%7Bv%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(v_{i}=1|h)=&#92;sigma(a_{i}+&#92;mathbf{W}_{i}&#92;mathbf{v}) " title="p(v_{i}=1|h)=&#92;sigma(a_{i}+&#92;mathbf{W}_{i}&#92;mathbf{v}) " class="latex" /></p>
<p>The conditional probabilities let us apply Gibbs Sampling, which is simply</p>
<ul>
<li>hold <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v} " title="&#92;mathbf{v} " class="latex" /> fixed, sample <img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bh%7D%7C%5Cmathbf%7Bv%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{h}|&#92;mathbf{v}) " title="p(&#92;mathbf{h}|&#92;mathbf{v}) " class="latex" /></li>
<li>hold <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bh%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{h} " title="&#92;mathbf{h} " class="latex" /> fixed, sample <img src="https://s0.wp.com/latex.php?latex=p%28%5Cmathbf%7Bh%7D%7C%5Cmathbf%7Bh%7D%29&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="p(&#92;mathbf{h}|&#92;mathbf{h})" title="p(&#92;mathbf{h}|&#92;mathbf{h})" class="latex" /></li>
<li>repeat for 1 or more <em>equlibiration</em> steps</li>
</ul>
<p>In statistical mechanics, this is called a mean field theory.  This means that the Free Energy (in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v} " title="&#92;mathbf{v} " class="latex" />) can be written as a simple linear average over the hidden units</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%5E%7BRBM%7D%3D%5Cmathbf%7Ba%7D%5E%7BT%7D%5Cmathbf%7Bv%7D%2B%5Cmathbf%7Bb%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%2B%5Cmathbf%7Bv%7D%5E%7BT%7D%28%5Cmathbf%7BWh%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F^{RBM}=&#92;mathbf{a}^{T}&#92;mathbf{v}+&#92;mathbf{b}^{T}&#92;mathbf{h}+&#92;mathbf{v}^{T}(&#92;mathbf{Wh}) " title="F^{RBM}=&#92;mathbf{a}^{T}&#92;mathbf{v}+&#92;mathbf{b}^{T}&#92;mathbf{h}+&#92;mathbf{v}^{T}(&#92;mathbf{Wh}) " class="latex" />.</p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BWh%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{Wh} " title="&#92;mathbf{Wh} " class="latex" /> is the <em>mean field</em> of the hidden units.</p>
<p>At high Temp., for a spin glass, a mean field model seems very sensible because the spins (i.e. activations) become uncorrelated.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clangle+s_%7Bi%7D%5Ccdots+s_%7Bj%7D%5Crangle%5Cunderset%7BT%5Crightarrow%5Cinfty%7D%7B%5Crightarrow%7D%5Clangle+s_%7Bi%7D%5Crangle%5Ccdots%5Clangle+s_%7Bj%7D%5Crangle+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;langle s_{i}&#92;cdots s_{j}&#92;rangle&#92;underset{T&#92;rightarrow&#92;infty}{&#92;rightarrow}&#92;langle s_{i}&#92;rangle&#92;cdots&#92;langle s_{j}&#92;rangle " title="&#92;langle s_{i}&#92;cdots s_{j}&#92;rangle&#92;underset{T&#92;rightarrow&#92;infty}{&#92;rightarrow}&#92;langle s_{i}&#92;rangle&#92;cdots&#92;langle s_{j}&#92;rangle " class="latex" /></p>
<p style="text-align:center;"><a href="http://statweb.stanford.edu/~souravc/talk_spin.pdf" target="_blank">This is non-trivial to prove</a></p>
<p>Theoreticians use mean field models like <a href="https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/" target="_blank">the p-spin spherical spin glass to study deep learning</a> because of their simplicity.  Computationally, we frequently need more.</p>
<p>How we can go beyond mean field theory ?</p>
<h4>The Onsager Correction</h4>
<p><img class="alignleft" src="https://i0.wp.com/www.nobelprize.org/nobel_prizes/chemistry/laureates/1968/onsager.jpg" /></p>
<p>Onsager was awarded <a href="http://www.nobelprize.org/nobel_prizes/chemistry/laureates/1968/" target="_blank">1968 the Nobel Prize in Chemistry</a> for the development of the <a href="https://en.wikipedia.org/wiki/Onsager_reciprocal_relations">Onsager Reciprocal Relations</a>, sometimes called the &#8216;4th law of Thermodynamics&#8217;</p>
<p>The Onsager relations provides the theory to treat thermodynamic systems that are in a <strong><em>quasi-stationary, local equilibrium</em></strong>.</p>
<p>Onsager was the first to show how to relate the correlations in the fluctuations to <a href="https://charlesmartin14.wordpress.com/2013/08/01/causality-correlation-and-brownian-motion/" target="_blank">the linear response</a>.  And by tying a sequence of quasi-stationary systems together, we can describe an irreversible process&#8230;</p>
<p>..like learning.  And this is exactly what we need to train an RBM.</p>
<p>In an RBM, the fluctuations are variations in the hidden and visible nodes.</p>
<p><a href="https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/rbm-nodes/#main" rel="attachment wp-att-9315"><img data-attachment-id="9315" data-permalink="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/rbm-nodes/" data-orig-file="https://charlesmartin14.files.wordpress.com/2016/10/rbm-nodes.png?w=840" data-orig-size="162,89" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="rbm-nodes" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2016/10/rbm-nodes.png?w=840?w=162" data-large-file="https://charlesmartin14.files.wordpress.com/2016/10/rbm-nodes.png?w=840?w=162" class="aligncenter size-full wp-image-9315" src="https://charlesmartin14.files.wordpress.com/2016/10/rbm-nodes.png?w=840" alt="rbm-nodes" srcset="https://charlesmartin14.files.wordpress.com/2016/10/rbm-nodes.png 162w, https://charlesmartin14.files.wordpress.com/2016/10/rbm-nodes.png?w=150 150w" sizes="(max-width: 162px) 100vw, 162px"   /></a></p>
<p>In a <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html" target="_blank">BernoulliRBM</a>, the activations can be 0 or 1, so the <span style="color:#000000;"><em>fluctuation vectors</em> </span>are</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7Bh%7D-%5Cmathbf%7B0%7D%29%2C+%28%5Cmathbf%7Bh%7D-%5Cmathbf%7B1%7D%29+%5C%3B%5C%3B+and%5C%3B%5C%3B%C2%A0%28%5Cmathbf%7Bv%7D-%5Cmathbf%7B0%7D%29%2C%28%5Cmathbf%7Bv%7D-%5Cmathbf%7B1%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{h}-&#92;mathbf{0}), (&#92;mathbf{h}-&#92;mathbf{1}) &#92;;&#92;; and&#92;;&#92;; (&#92;mathbf{v}-&#92;mathbf{0}),(&#92;mathbf{v}-&#92;mathbf{1}) " title="(&#92;mathbf{h}-&#92;mathbf{0}), (&#92;mathbf{h}-&#92;mathbf{1}) &#92;;&#92;; and&#92;;&#92;; (&#92;mathbf{v}-&#92;mathbf{0}),(&#92;mathbf{v}-&#92;mathbf{1}) " class="latex" /></p>
<p>The simplest correction to the mean field Free Energy, at each step in training, are the correlations in these fluctuations:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5B%28%5Cmathbf%7Bh%7D-%5Cmathbf%7B0%7D%29%5E%7BT%7D%28%5Cmathbf%7Bh%7D-%5Cmathbf%7B1%7D%29%5E%7BT%7D%5Cmathbf%7BW%7D%5E%7B2%7D%C2%A0%28%5Cmathbf%7Bv%7D-%5Cmathbf%7B0%7D%29%28%5Cmathbf%7Bv%7D-%5Cmathbf%7B1%7D%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;left[(&#92;mathbf{h}-&#92;mathbf{0})^{T}(&#92;mathbf{h}-&#92;mathbf{1})^{T}&#92;mathbf{W}^{2} (&#92;mathbf{v}-&#92;mathbf{0})(&#92;mathbf{v}-&#92;mathbf{1})&#92;right] " title="&#92;left[(&#92;mathbf{h}-&#92;mathbf{0})^{T}(&#92;mathbf{h}-&#92;mathbf{1})^{T}&#92;mathbf{W}^{2} (&#92;mathbf{v}-&#92;mathbf{0})(&#92;mathbf{v}-&#92;mathbf{1})&#92;right] " class="latex" /></p>
<p>where <strong>W</strong> is the Energy weight matrix.</p>
<p style="text-align:center;"><span style="color:#ff0000;"><em>Unlike normal RBMs, here is we work in an Interaction Ensemble, so the hidden and visible units become hidden and visible magnetizations:  </em></span></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bv%7D%5Cleftrightarrow%5Cmathbf%7Bm%7D%5E%7Bv%7D%2C%5C%3B%5Cmathbf%7Bh%7D%5Cleftrightarrow%5Cmathbf%7Bm%7D%5E%7Bh%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{v}&#92;leftrightarrow&#92;mathbf{m}^{v},&#92;;&#92;mathbf{h}&#92;leftrightarrow&#92;mathbf{m}^{h} " title="&#92;mathbf{v}&#92;leftrightarrow&#92;mathbf{m}^{v},&#92;;&#92;mathbf{h}&#92;leftrightarrow&#92;mathbf{m}^{h} " class="latex" /></p>
<p style="text-align:center;"><span style="color:#ff0000;"><em>To simplify (or confuse?) the presentations here, I don&#8217;t write magnetizations (until the Appendix).</em></span></p>
<p>The corrections make sense under the stationarity constraints, that the Extended Mean Field RBM Free Energy (<img src="https://s0.wp.com/latex.php?latex=F%5E%7BEMF%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F^{EMF} " title="F^{EMF} " class="latex" />) is at a critical point</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5B%5Cdfrac%7BdF%5E%7BEMF%7D%7D%7Bd%5Cmathbf%7Bv%7D%7D%2C%5Cdfrac%7BdF%5E%7BEMF%7D%7D%7Bd%5Cmathbf%7Bh%7D%7D%5Cright%5D%3D0+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;left[&#92;dfrac{dF^{EMF}}{d&#92;mathbf{v}},&#92;dfrac{dF^{EMF}}{d&#92;mathbf{h}}&#92;right]=0 " title="&#92;left[&#92;dfrac{dF^{EMF}}{d&#92;mathbf{v}},&#92;dfrac{dF^{EMF}}{d&#92;mathbf{h}}&#92;right]=0 " class="latex" />,</p>
<p>That is, small changes in the activations <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7Bv%7D%2C%5Cmathbf%7Bh%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{v},&#92;mathbf{h}) " title="(&#92;mathbf{v},&#92;mathbf{h}) " class="latex" /> do not change the Free Energy.</p>
<p>We will show that we can write</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%5E%7BEMF%7D%3DS%2B%5Cbeta%5C%3BF%5E%7BMF%7D%2B%5Cfrac%7B1%7D%7B2%7D%5Cbeta%5E%7B2%7DF%5E%7BOnsager%7D%2B%5Ccdots+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F^{EMF}=S+&#92;beta&#92;;F^{MF}+&#92;frac{1}{2}&#92;beta^{2}F^{Onsager}+&#92;cdots " title="F^{EMF}=S+&#92;beta&#92;;F^{MF}+&#92;frac{1}{2}&#92;beta^{2}F^{Onsager}+&#92;cdots " class="latex" /></p>
<p>as a Taylor series in <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" />, the inverse Temperature, where <img src="https://s0.wp.com/latex.php?latex=S%C2%A0&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="S " title="S " class="latex" /> is the Entropy</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=S%3D%5Cmathbf%7Bv%7D%5C%3Bln%28%5Cmathbf%7Bv%7D%29%2B%28%5Cmathbf%7B1%7D-%5Cmathbf%7Bv%7D%29%5C%3Bln%28%5Cmathbf%7B1%7D-%5Cmathbf%7Bv%7D%29%2B%5Cmathbf%7Bh%7D%5C%3Bln%28%5Cmathbf%7Bh%7D%29%2B%28%5Cmathbf%7B1%7D-%5Cmathbf%7Bh%7D%29%5C%3Bln%28%5Cmathbf%7B1%7D-%5Cmathbf%7Bh%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="S=&#92;mathbf{v}&#92;;ln(&#92;mathbf{v})+(&#92;mathbf{1}-&#92;mathbf{v})&#92;;ln(&#92;mathbf{1}-&#92;mathbf{v})+&#92;mathbf{h}&#92;;ln(&#92;mathbf{h})+(&#92;mathbf{1}-&#92;mathbf{h})&#92;;ln(&#92;mathbf{1}-&#92;mathbf{h}) " title="S=&#92;mathbf{v}&#92;;ln(&#92;mathbf{v})+(&#92;mathbf{1}-&#92;mathbf{v})&#92;;ln(&#92;mathbf{1}-&#92;mathbf{v})+&#92;mathbf{h}&#92;;ln(&#92;mathbf{h})+(&#92;mathbf{1}-&#92;mathbf{h})&#92;;ln(&#92;mathbf{1}-&#92;mathbf{h}) " class="latex" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=F%5E%7BMF%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F^{MF} " title="F^{MF} " class="latex" /> is the standard, mean field RBM Free energy</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%3D%5Cmathbf%7Ba%7D%5E%7BT%7D%5Cmathbf%7Bv%7D%2B%5Cmathbf%7Bb%7D%5E%7BT%7D%5Cmathbf%7Bh%7D%2B%5Cmathbf%7Bv%7D%5E%7BT%7D%5Cmathbf%7BW%7D%5Cmathbf%7Bh%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F=&#92;mathbf{a}^{T}&#92;mathbf{v}+&#92;mathbf{b}^{T}&#92;mathbf{h}+&#92;mathbf{v}^{T}&#92;mathbf{W}&#92;mathbf{h} " title="F=&#92;mathbf{a}^{T}&#92;mathbf{v}+&#92;mathbf{b}^{T}&#92;mathbf{h}+&#92;mathbf{v}^{T}&#92;mathbf{W}&#92;mathbf{h} " class="latex" />,</p>
<p style="text-align:left;">and <img src="https://s0.wp.com/latex.php?latex=F%5E%7BOnsager%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F^{Onsager} " title="F^{Onsager} " class="latex" /> is the Onsager correction</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=F%5E%7BOnsager%7D%3D%5Cleft%5B%28%5Cmathbf%7Bh%7D-%5Cmathbf%7Bh%7D%5E%7B2%7D%29%5E%7BT%7D%5Cmathbf%7BW%7D%5E%7B2%7D%C2%A0%28%5Cmathbf%7Bv%7D-%5Cmathbf%7Bv%7D%5E%7B2%7D%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F^{Onsager}=&#92;left[(&#92;mathbf{h}-&#92;mathbf{h}^{2})^{T}&#92;mathbf{W}^{2} (&#92;mathbf{v}-&#92;mathbf{v}^{2})&#92;right] " title="F^{Onsager}=&#92;left[(&#92;mathbf{h}-&#92;mathbf{h}^{2})^{T}&#92;mathbf{W}^{2} (&#92;mathbf{v}-&#92;mathbf{v}^{2})&#92;right] " class="latex" />.</p>
<p style="text-align:left;">Given the expressions for the Free Energy, we must now evaluate it.</p>
<h4>Thouless-Anderson-Palmer TAP Theory</h4>
<p>The Taylor series above is a result of the TAP theory &#8212; the Thouless-Anderson-Palmer approach developed for spin glasses.</p>
<p style="text-align:left;"><img data-attachment-id="9743" data-permalink="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/vortices/" data-orig-file="https://charlesmartin14.files.wordpress.com/2016/10/vortices.gif?w=372&#038;h=186" data-orig-size="1060,530" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="vortices" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2016/10/vortices.gif?w=372&#038;h=186?w=300" data-large-file="https://charlesmartin14.files.wordpress.com/2016/10/vortices.gif?w=372&#038;h=186?w=840" class="aligncenter wp-image-9743" src="https://charlesmartin14.files.wordpress.com/2016/10/vortices.gif?w=372&#038;h=186" alt="vortices.gif" width="372" height="186" /></p>
<p> The TAP theory is outlined in the Appendix; here it is noted that</p>
<p><a href="https://www.nobelprize.org/nobel_prizes/physics/laureates/2016/" target="_blank">Thouless just shared the 2016 Nobel Prize in Physics </a><a href="https://www.nobelprize.org/nobel_prizes/physics/laureates/2016/" target="_blank">(for his work in topological phase transitions)</a></p>
<h4>Temperature Dependence and Weight Decay</h4>
<p style="text-align:left;">Being a series in inverse Temperature <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" />, the theory applies at low <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" />, or high Temperature.   For fixed <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" />, this also corresponds to small weights <strong>W. </strong></p>
<p style="text-align:left;">Specifically, the expansion applies at Temperatures above the glass transition&#8211;<a href="https://www.youtube.com/watch?v=kIbKHIPbxiU" target="_blank">a concept which I describe in a recent video blog</a>.</p>
<p style="text-align:left;">Here, to implement the EMF_RBM, we set</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cbeta%3D1%5C%3B%5C%3B+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta=1&#92;;&#92;; " title="&#92;beta=1&#92;;&#92;; " class="latex" />,</p>
<p style="text-align:left;">and, instead, apply weight decay to keep the weights <strong>W</strong> from exploding</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BdW%7D%5Crightarrow%5Cmathbf%7BdW%7D%2B%5Cdelta%5CVert%5Cmathbf%7BW%7D%5CVert+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{dW}&#92;rightarrow&#92;mathbf{dW}+&#92;delta&#92;Vert&#92;mathbf{W}&#92;Vert " title="&#92;mathbf{dW}&#92;rightarrow&#92;mathbf{dW}+&#92;delta&#92;Vert&#92;mathbf{W}&#92;Vert " class="latex" /></p>
<p style="text-align:left;">where <img src="https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7BW%7D%5CVert+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;Vert&#92;mathbf{W}&#92;Vert " title="&#92;Vert&#92;mathbf{W}&#92;Vert " class="latex" /> may be an L1 or L2 norm.</p>
<p style="text-align:center;"><strong><span style="color:#008000;">Weight Decay acts to keep the Temperature high.</span></strong></p>
<h4>RBMs with explicit Temperature</h4>
<p>Early RBM computational models were formulated using statistical mechanics  (see the Appendix) language, and so included a Temperature parameter, and were solved using techniques like simulated annealing and the (mean field) TAP equations (described below).</p>
<p>Adding  Temperature allowed the system to &#8216;jump&#8217; out of the spurious local minima.  So any usable model required a non-zero Temp, and/or some scheme to avoid local minima that generalized poorly.  (See:  <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf" target="_blank">Learning Deep Architectures for AI,</a> by Bengio)</p>
<p>These older approaches did not work well &#8211;then &#8212; so Hinton proposed the Contrastive Divergence (CD) algorithm.  Note that researchers struggled for some years to &#8216;explain&#8217; what optimization problem CD actually solves.</p>
<p>More that recent work on <a href="http://Temperature based Restricted Boltzmann Machines" target="_blank">Temperature Bases RBMs</a> also suggests that higher T solutions perform better, and that</p>
<p style="text-align:center;"><strong><span style="color:#008000;"> <em>&#8220;temperature is an essential parameter controlling the selectivity of the firing neurons in the hidden layer.&#8221;</em></span></strong></p>
<h4></h4>
<h4>Training without Sampling:  Fixed Point equations</h4>
<p>Standard RBM training approximates the (unconstrained) Free Energy, F=ln Z, in the mean field approximation, using (one or more steps of) Gibbs Sampling.  This is usually implemented as Contrastive Divergence (CD), or Persistent Contrastive Divergence (PCD).</p>
<p>Using techniques of statistical mechanics, however, it is possible to train an RBM directly, without sampling, by solving a set of deterministic fixed point equations.</p>
<p>Indeed, this approach clarifies how to view an RBM as solving a (<em>determinisitic)</em> fixed point equation of the form</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=f%28%5Cmathbf%7BX%7D%29%5Crightarrow%5Cmathbf%7BX%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="f(&#92;mathbf{X})&#92;rightarrow&#92;mathbf{X} " title="f(&#92;mathbf{X})&#92;rightarrow&#92;mathbf{X} " class="latex" /></p>
<p style="text-align:left;">Consider each step, at at fixed (<img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Ba%7D%2C+%5Cmathbf%7Bb%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{W}, &#92;mathbf{a}, &#92;mathbf{b} " title="&#92;mathbf{W}, &#92;mathbf{a}, &#92;mathbf{b} " class="latex" />), as a <em>Quasi-Stationary </em>system, which is close to equilibrium, but we don&#8217;t need to evaluate <strong>ln Z(v,h) </strong>exactly.</p>
<p style="text-align:left;">We can use the stationary conditions to derive a pair of coupled, non-linear equations</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=v_%7Bi%7D%5Csimeq%5Csigma%5Cleft%5Ba_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dh_%7Bj%7D-%28v_%7Bi%7D-%5Cfrac%7B1%7D%7B2%7D%29w%5E%7B2%7D_%7Bi%2Cj%7D%28h_%7Bj%7D-%28h_%7Bj%7D%29%5E%7B2%7D%29%2B%5Ccdots%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="v_{i}&#92;simeq&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}h_{j}-(v_{i}-&#92;frac{1}{2})w^{2}_{i,j}(h_{j}-(h_{j})^{2})+&#92;cdots&#92;right] " title="v_{i}&#92;simeq&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}h_{j}-(v_{i}-&#92;frac{1}{2})w^{2}_{i,j}(h_{j}-(h_{j})^{2})+&#92;cdots&#92;right] " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%5Csimeq%5Csigma%5Cleft%5Bb_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dv_%7Bj%7D-%28h_%7Bi%7D-%5Cfrac%7B1%7D%7B2%7D%29w%5E%7B2%7D_%7Bi%2Cj%7D%28v_%7Bj%7D-%28v_%7Bj%7D%29%5E%7B2%7D%29%2B%5Ccdots%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="h_{i}&#92;simeq&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}v_{j}-(h_{i}-&#92;frac{1}{2})w^{2}_{i,j}(v_{j}-(v_{j})^{2})+&#92;cdots&#92;right] " title="h_{i}&#92;simeq&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}v_{j}-(h_{i}-&#92;frac{1}{2})w^{2}_{i,j}(v_{j}-(v_{j})^{2})+&#92;cdots&#92;right] " class="latex" /></p>
<p style="text-align:left;">They extend the standard formula of sigmoid linear activations <img src="https://s0.wp.com/latex.php?latex=%5Csigma+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;sigma " title="&#92;sigma " class="latex" /> with additional, non-linear, inter-layer interactions.</p>
<p style="text-align:left;">They differs significantly from (simple) Deep Learning activation functions because the activation for each layer explicitly includes information from other layers.</p>
<p style="text-align:left;">This extension couples the mean (<img src="https://s0.wp.com/latex.php?latex=v_%7Bi%7D-%5Cfrac%7B1%7D%7B2%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="v_{i}-&#92;frac{1}{2} " title="v_{i}-&#92;frac{1}{2} " class="latex" />) and total fluctuations (<img src="https://s0.wp.com/latex.php?latex=v_%7Bj%7D-%28v_%7Bj%7D%29%5E%7B2%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="v_{j}-(v_{j})^{2} " title="v_{j}-(v_{j})^{2} " class="latex" />) between layers.  Higher order correlations could also be included, even to infinite order, using techniques from field theory.</p>
<p style="text-align:left;">We can not satisfy both equations simultaneously, but we can satisfy each condition <img src="https://s0.wp.com/latex.php?latex=%5Cleft%5B%5Cdfrac%7BdF%5E%7BEMF%7D%7D%7Bd%5Cmathbf%7Bv%7D%7D%2C%5Cdfrac%7BdF%5E%7BEMF%7D%7D%7Bd%5Cmathbf%7Bh%7D%7D%5Cright%5D%3D0+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;left[&#92;dfrac{dF^{EMF}}{d&#92;mathbf{v}},&#92;dfrac{dF^{EMF}}{d&#92;mathbf{h}}&#92;right]=0 " title="&#92;left[&#92;dfrac{dF^{EMF}}{d&#92;mathbf{v}},&#92;dfrac{dF^{EMF}}{d&#92;mathbf{h}}&#92;right]=0 " class="latex" /> individually, letting us write a set of recursion relations</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=v_%7Bi%7D%5Bt%2B1%5D%5Cleftarrow%5Csigma%5Cleft%5Ba_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dh_%7Bj%7D%5Bt%5D-%28v_%7Bi%7D%5Bt%5D-%5Cfrac%7B1%7D%7B2%7D%29w%5E%7B2%7D_%7Bi%2Cj%7D%28h_%7Bj%7D%5Bt%5D-%28h_%7Bj%7D%5Bt%5D%29%5E%7B2%7D%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="v_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}h_{j}[t]-(v_{i}[t]-&#92;frac{1}{2})w^{2}_{i,j}(h_{j}[t]-(h_{j}[t])^{2})&#92;right] " title="v_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}h_{j}[t]-(v_{i}[t]-&#92;frac{1}{2})w^{2}_{i,j}(h_{j}[t]-(h_{j}[t])^{2})&#92;right] " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=h_%7Bi%7D%5Bt%2B1%5D%5Cleftarrow%5Csigma%5Cleft%5Bb_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dv_%7Bj%7D%5Bt%2B1%5D-%28h_%7Bi%7D%5Bt%2B1%5D-%5Cfrac%7B1%7D%7B2%7D%29w%5E%7B2%7D_%7Bi%2Cj%7D%28v_%7Bj%7D%5Bt%2B1%5D-%28v_%7Bj%7D%5Bt%2B1%5D%29%5E%7B2%7D%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="h_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}v_{j}[t+1]-(h_{i}[t+1]-&#92;frac{1}{2})w^{2}_{i,j}(v_{j}[t+1]-(v_{j}[t+1])^{2})&#92;right] " title="h_{i}[t+1]&#92;leftarrow&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}v_{j}[t+1]-(h_{i}[t+1]-&#92;frac{1}{2})w^{2}_{i,j}(v_{j}[t+1]-(v_{j}[t+1])^{2})&#92;right] " class="latex" /></p>
<p>These fixed point equations converge to the stationary solution, leading to a local equilibrium.  Like Gibbs Sampling, however,  we only need a few iterations (say t=3 to 5). Unlike Sampling, however, the EMF RBM is deterministic.</p>
<h4>Implementation</h4>
<ul>
<li>The original implementation of the EMF-RBM is in the<a href="https://github.com/sphinxteam/Boltzmann.jl"> Sphinx Team Julia package Boltzmann.jl</a></li>
<li>I have ported the second order EMF-RBM to python, in the style of the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html">scikit learn BernoulliRBM</a> package.  The python code is available at</li>
</ul>
<p style="text-align:center;"><a href="https://github.com/charlesmartin14/emf-rbm/">https://github.com/charlesmartin14/emf-rbm/</a></p>
<p style="text-align:left;">If there is enough interest, I can do a pull request on sklearn to include it.</p>
<p style="text-align:left;">The next blog post will demonstrate how the python code in action.</p>
<h3>Appendix</h3>
<h4>the Statistical Mechanics of Inference</h4>
<h5>Early work on Hopfield Associative Memories</h5>
<p>Most older physicists will remember the Hopfield model.  <a href="https://nms.kcl.ac.uk/reimer.kuehn/CS03/Sompolinsky_PhysicsToday.pdf">They peaked in 1986</a>, although interesting work continued into the late 90s (when I was a post doc).</p>
<p>Originally, Boltzmann machines were introduced as a way to avoid spurious local minima while including &#8216;hidden&#8217; features into Hopfield Associative Memories (HAM).</p>
<p>Hopfield himself was a theoretical chemist, and his simple model HAMs were of  great interest to theoretical chemists and physicists.</p>
<p><a href="https://www.youtube.com/watch?v=k31Ox9hOh7M" target="_blank">Hinton explains Hopfield nets in his on-line lectures on Deep Learning.</a></p>
<p>The Hopfield Model is a kind of spin glass, which acts like a &#8216;memory&#8217; that can recognize &#8216;stored patterns&#8217;.  It was originally developed as a <strong><em>quasi-stationary</em></strong> solution of more complex, dynamical models of neuronal firing patterns (see <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=7&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjH19Wsys_PAhVI7GMKHfmwB4cQFghHMAY&amp;url=http%3A%2F%2Fmathematical-neuroscience.springeropen.com%2Ftrack%2Fpdf%2F10.1186%2Fs13408-015-0034-5%3Fsite%3Dmathematical-neuroscience.springeropen.com&amp;usg=AFQjCNFfpyFJ6rV8ohnEBnojcHlJv0A3gw&amp;sig2=qVt_LA_yhSf5mMnUp6wePQ" target="_blank">the Cowan-Wilson model</a>).</p>
<p>Early theoretical work on HAMs studied analytic approximations to ln Z to compute their capacity (<img src="https://s0.wp.com/latex.php?latex=%5Crho%2FN+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;rho/N " title="&#92;rho/N " class="latex" />), and their phase diagram.  The capacity is simply the number of patterns  <img src="https://s0.wp.com/latex.php?latex=%5Crho+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;rho " title="&#92;rho " class="latex" /> a network of size N can memorize without getting confused.</p>
<figure data-shortcode="caption" id="attachment_9629" style="width: 316px" class="wp-caption aligncenter"><img data-attachment-id="9629" data-permalink="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/g35/" data-orig-file="https://charlesmartin14.files.wordpress.com/2016/10/g35.gif?w=316&#038;h=185" data-orig-size="376,220" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="g35" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2016/10/g35.gif?w=316&#038;h=185?w=300" data-large-file="https://charlesmartin14.files.wordpress.com/2016/10/g35.gif?w=316&#038;h=185?w=376" class=" wp-image-9629 aligncenter" src="https://charlesmartin14.files.wordpress.com/2016/10/g35.gif?w=316&#038;h=185" alt="g35" width="316" height="185" /><figcaption class="wp-caption-text">Phase diagram of a Hopfield Network</figcaption></figure>
<p>The Hopfield model was traditionally run at T=0.</p>
<p>Looking at the T=0 line, at extremely low capacity, the system has<em> stable mixed states</em> that correspond to &#8216;frozen&#8217; memories.  But this is very low capacity, and generally unusable.  Also, when the capacity too large, <img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7Bp%7D%7BN%7D%5Cge+0.138+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{p}{N}&#92;ge 0.138 " title="&#92;dfrac{p}{N}&#92;ge 0.138 " class="latex" />, (which is really not that large),  the system abruptly breaks down completely.</p>
<p>There is a small window of capacity, <img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7Bp%7D%7BN%7D%5Cle%7E0.05+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{p}{N}&#92;le~0.05 " title="&#92;dfrac{p}{N}&#92;le~0.05 " class="latex" />,with <em>stable pattern equilibria, </em>dominated by frozen out, spin glass states.   The problem is, for any realistic system, with correlated data, the system is dominated by spurious local minima which look like low energy spin glass states.</p>
<p>So the Hopfield model suggested that</p>
<p style="text-align:center;"><span style="color:#008000;"><strong>glass states can be useful minima, but</strong></span></p>
<p style="text-align:center;"><span style="color:#ff0000;"><strong>we want to avoid low energy  (spurious) glassy states.</strong></span></p>
<p><a href="https://arxiv.org/abs/1105.2790" target="_blank">One can try to derive direct mapping between Hopfield Nets and RBMs</a> (under reasonable assumptions). Then the RBM capacity is proportional to the number of  Hidden nodes.    After that, the analogies stop.</p>
<p>The intuition about RBMs is different since (effectively) they operate at a non-zero Temperature.  Additionally, it is unclear to this blogger if the proper description of deep learning is a mean field spin glass, with many useful local minima, or a strongly correlated system, which may behave very differently, and more like a funneled energy landscape.</p>
<p><strong>Thouless-Anderson-Palmer Theory of Spin Glasses</strong></p>
<p>The TAP theory is one of the classic analytic tools used to study spin glasses and <a href="https://arxiv.org/abs/cond-mat/9705015">even the Hopfield Model</a>.</p>
<p>We will derive the EMF RBM method following the <a href="http://www.tandfonline.com/doi/abs/10.1080/14786437708235992"><strong>Thouless-Anderson-Palmer (TAP) </strong></a>approach to spin glasses.</p>
<p>On a side note he TAP method introduces us to 2 more Nobel Laureates:</p>
<ul>
<li>David J Thouless, 2016 Nobel Prize in Physics</li>
<li>Phillip W Anderson, 1977 Nobel Prize in Physics</li>
</ul>
<p>The TAP theory, published in 1977, presented a formal approach to study the thermodynamics of mean field spin glasses.  In particular, the TAP theory provides an expression for the average spin</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clangle+s_%7Bi%7D%5Crangle%3Dtanh%28C%2B%5Cbeta%5C%3BMeanField%29-%5Cbeta%5E%7B2%7D%5C%3BOnsager%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;langle s_{i}&#92;rangle=tanh(C+&#92;beta&#92;;MeanField)-&#92;beta^{2}&#92;;Onsager) " title="&#92;langle s_{i}&#92;rangle=tanh(C+&#92;beta&#92;;MeanField)-&#92;beta^{2}&#92;;Onsager) " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=tanh%28%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="tanh() " title="tanh() " class="latex" /> is like an activation function, C is a constant, and the MeanField and Onsager terms are like our terms above.</p>
<p>In 1977, they argued that the TAP approach would hold for all Temperatures (and external fields), although it was only <em>proven</em> until 25 years later by <a href="http://www.springer.com/la/book/9783540003564">Talagrand</a>.  It is these <em>relatively</em> new, rigorous approaches that are <em>cited</em> by Deep Learning researchers like <a href="https://arxiv.org/abs/1412.0233">LeCun</a> , <a href="http://vision.ucla.edu/~pratikac/pub/chaudhari.cs269.16.pdf">Chaudhari</a>, etc.    But many of the cited results have been suggested using the TAP approach.  In particular, the structure of the Energy Landscape, has been understood looking at <a href="https://arxiv.org/pdf/cond-mat/9710272.pdf">the stationary points of the TAP free energy</a>.</p>
<p>More importantly, the TAP approach can be <em><strong>operationalized</strong></em>, as a new RBM solver.</p>
<h4>The TAP Equations</h4>
<p>We start with the RBM Free Energy <img src="https://s0.wp.com/latex.php?latex=F+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="F " title="F " class="latex" />.</p>
<p>Introduce an &#8216;external field&#8217; <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bq%7D%3D%5C%7Bq_%7Bi%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{q}=&#92;{q_{i}&#92;} " title="&#92;mathbf{q}=&#92;{q_{i}&#92;} " class="latex" /> which couples to the spins, adding a linear term to the Free Energy</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cbeta+F%5B%5Cmathbf%7Bq%7D%5D%3D%5Cln%5C%3B%5Csum%5Climits_%7B%5Cmathbf%7Bs%7D%7D%5Cexp%28-%5Cbeta%5B+E%28%5Cmathbf%7Bs%7D%29-%5Cmathbf%7Bq%7D%5E%7BT%7D%5Cmathbf%7Bs%7D%5D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta F[&#92;mathbf{q}]=&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{s}}&#92;exp(-&#92;beta[ E(&#92;mathbf{s})-&#92;mathbf{q}^{T}&#92;mathbf{s}]) " title="&#92;beta F[&#92;mathbf{q}]=&#92;ln&#92;;&#92;sum&#92;limits_{&#92;mathbf{s}}&#92;exp(-&#92;beta[ E(&#92;mathbf{s})-&#92;mathbf{q}^{T}&#92;mathbf{s}]) " class="latex" /></p>
<p>Physically, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bq%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{q} " title="&#92;mathbf{q} " class="latex" /> would be an external magnetic field <em>which drives the system out-of-equilibrium</em>.</p>
<p>As is standard in statistical mechanics, we take the Legendre Transform,  in terms of a set of conjugate variables <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bm%7D%3D%5C%7Bm_%7Bij%7D%5C%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{m}=&#92;{m_{ij}&#92;} " title="&#92;mathbf{m}=&#92;{m_{ij}&#92;} " class="latex" />.  These are the <em>magnetizations </em>of each spin under the applied field,  and describe how the spins behave outside of equilibrium</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=-%5Cbeta%5C%3B%5CGamma%5B%5Cmathbf%7Bm%7D%5D%3D-%5Cbeta%5C%3B%5Cunderset%7B%5Cmathbf%7Bq%7D%7D%7B%5Cmax%7D%5Cleft%5BF%5B%5Cmathbf%7Bq%7D%5D%2B%5Cmathbf%7Bm%7D%5E%7BT%7D%5Cmathbf%7Bq%7D%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="-&#92;beta&#92;;&#92;Gamma[&#92;mathbf{m}]=-&#92;beta&#92;;&#92;underset{&#92;mathbf{q}}{&#92;max}&#92;left[F[&#92;mathbf{q}]+&#92;mathbf{m}^{T}&#92;mathbf{q}&#92;right] " title="-&#92;beta&#92;;&#92;Gamma[&#92;mathbf{m}]=-&#92;beta&#92;;&#92;underset{&#92;mathbf{q}}{&#92;max}&#92;left[F[&#92;mathbf{q}]+&#92;mathbf{m}^{T}&#92;mathbf{q}&#92;right] " class="latex" /></p>
<p style="text-align:left;">The transform which effectively defines a new <span style="color:#008000;"><em>interaction</em> </span><em><span style="color:#008000;">ensemble</span> <img src="https://s0.wp.com/latex.php?latex=%5CGamma%5B%5Cmathbf%7Bm%7D%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;Gamma[&#92;mathbf{m}] " title="&#92;Gamma[&#92;mathbf{m}] " class="latex" /></em>.  We now set <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bq%7D%3D0+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{q}=0 " title="&#92;mathbf{q}=0 " class="latex" />, and note</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=-%5Cbeta+F%3D%5Cbeta%5C%3BF%5B%5Cmathbf%7Bq%7D%3D0%5D%3D%5Cbeta%5C%3B%5Cunderset%7B%5Cmathbf%7Bq%7D%7D%7B%5Cmin%7D%5Cleft%5B%5CGamma%5B%5Cmathbf%7Bm%7D%5D%5Cright%5D%3D-%5Cbeta%5C%3B%5CGamma%5B%5Cmathbf%7Bm%7D%5E%7B%2A%7D%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="-&#92;beta F=&#92;beta&#92;;F[&#92;mathbf{q}=0]=&#92;beta&#92;;&#92;underset{&#92;mathbf{q}}{&#92;min}&#92;left[&#92;Gamma[&#92;mathbf{m}]&#92;right]=-&#92;beta&#92;;&#92;Gamma[&#92;mathbf{m}^{*}] " title="-&#92;beta F=&#92;beta&#92;;F[&#92;mathbf{q}=0]=&#92;beta&#92;;&#92;underset{&#92;mathbf{q}}{&#92;min}&#92;left[&#92;Gamma[&#92;mathbf{m}]&#92;right]=-&#92;beta&#92;;&#92;Gamma[&#92;mathbf{m}^{*}] " class="latex" /></p>
<p style="text-align:left;">Define an<span style="color:#800000;"><em> interaction Free Energy</em></span> to describe the interaction ensemble</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=A%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%3D%5Cbeta%5C%3B%5CGamma%5B%5Cmathbf%7Bm%7D%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A(&#92;beta,&#92;mathbf{m})=&#92;beta&#92;;&#92;Gamma[&#92;mathbf{m}] " title="A(&#92;beta,&#92;mathbf{m})=&#92;beta&#92;;&#92;Gamma[&#92;mathbf{m}] " class="latex" /></p>
<p style="text-align:left;">which equals the original Free Energy when <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bq%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{q} " title="&#92;mathbf{q} " class="latex" />.</p>
<p>Note that because we have visible and hidden spins (or nodes), we will identify <em>magnetizations</em> for each</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bm%7D%3D%5B%5Cmathbf%7Bm%5E%7Bv%7D%2Cm%5E%7Bh%7D%7D%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{m}=[&#92;mathbf{m^{v},m^{h}}] " title="&#92;mathbf{m}=[&#92;mathbf{m^{v},m^{h}}] " class="latex" /></p>
<p style="text-align:left;">Now, recall we want to avoid the glassy phase; this means we keep the Temperature high.   Or <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" /> low.</p>
<p style="text-align:left;">We form a low Taylor series expansion <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" />  in the new ensemble</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=A%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%3DA%280%2C%5Cmathbf%7Bm%7D%29%2B%5Cbeta%5Cdfrac%7B%5Cpartial+A%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%7D%7B%5Cpartial%5Cbeta%7D%5CBig%5Cvert_%7B%5Cbeta%3D0%7D%2B%5Cdfrac%7B%5Cbeta%7D%7B2%7D%5Cdfrac%7B%5Cpartial%5E%7B2%7D+A%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%7D%7B%5Cpartial%5Cbeta%5E%7B2%7D%7D%5CBig%5Cvert_%7B%5Cbeta%3D0%7D%2B%5Ccdots+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A(&#92;beta,&#92;mathbf{m})=A(0,&#92;mathbf{m})+&#92;beta&#92;dfrac{&#92;partial A(&#92;beta,&#92;mathbf{m})}{&#92;partial&#92;beta}&#92;Big&#92;vert_{&#92;beta=0}+&#92;dfrac{&#92;beta}{2}&#92;dfrac{&#92;partial^{2} A(&#92;beta,&#92;mathbf{m})}{&#92;partial&#92;beta^{2}}&#92;Big&#92;vert_{&#92;beta=0}+&#92;cdots " title="A(&#92;beta,&#92;mathbf{m})=A(0,&#92;mathbf{m})+&#92;beta&#92;dfrac{&#92;partial A(&#92;beta,&#92;mathbf{m})}{&#92;partial&#92;beta}&#92;Big&#92;vert_{&#92;beta=0}+&#92;dfrac{&#92;beta}{2}&#92;dfrac{&#92;partial^{2} A(&#92;beta,&#92;mathbf{m})}{&#92;partial&#92;beta^{2}}&#92;Big&#92;vert_{&#92;beta=0}+&#92;cdots " class="latex" /></p>
<p style="text-align:left;">which, at low order in <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" />, we expect to be reasonably accurate even away from equilibrium, at least at high Temp.</p>
<p style="text-align:left;">This leads to an order-by-order expansion for the Free Energy <img src="https://s0.wp.com/latex.php?latex=A%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A(&#92;beta,&#92;mathbf{m}) " title="A(&#92;beta,&#92;mathbf{m}) " class="latex" />.  The first order (<img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cbeta%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathcal{O}(&#92;beta) " title="&#92;mathcal{O}(&#92;beta) " class="latex" />) correction is the mean field term.  The second order term (<img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cbeta%5E%7B2%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathcal{O}(&#92;beta^{2}) " title="&#92;mathcal{O}(&#92;beta^{2}) " class="latex" />) is the Onsager correction.</p>
<h5>Second Order Free Energy</h5>
<p>Upto <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cbeta%5E%7B2%7D%29&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathcal{O}(&#92;beta^{2})" title="&#92;mathcal{O}(&#92;beta^{2})" class="latex" />, we have</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=A%280%2C%5Cmathbf%7Bm%5E%7Bv%7D%7D%2C%5Cmathbf%7Bm%5E%7Bh%7D%7D%29%3DS%28%5Cmathbf%7Bm%5E%7Bv%7D%7D%29%2BS%28%5Cmathbf%7Bm%5E%7Bh%7D%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A(0,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=S(&#92;mathbf{m^{v}})+S(&#92;mathbf{m^{h}}) " title="A(0,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=S(&#92;mathbf{m^{v}})+S(&#92;mathbf{m^{h}}) " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%5Cbeta%7DA%28%5Cbeta%2C%5Cmathbf%7Bm%5E%7Bv%7D%7D%2C%5Cmathbf%7Bm%5E%7Bh%7D%7D%29%3D%5Cmathbf%7Ba%5E%7BT%7Dm%5E%7Bv%7D%7D%2B%5Cmathbf%7Bb%5E%7BT%7Dm%5E%7Bh%7D%7D%2B%5Cmathbf%7B%28m%5E%7Bv%7D%29%5E%7BT%7DWm%5E%7Bh%7D%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial}{&#92;partial&#92;beta}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;mathbf{a^{T}m^{v}}+&#92;mathbf{b^{T}m^{h}}+&#92;mathbf{(m^{v})^{T}Wm^{h}} " title="&#92;dfrac{&#92;partial}{&#92;partial&#92;beta}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;mathbf{a^{T}m^{v}}+&#92;mathbf{b^{T}m^{h}}+&#92;mathbf{(m^{v})^{T}Wm^{h}} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%5E%7B2%7D%7D%7B%5Cpartial%5Cbeta%5E%7B2%7D%7DA%28%5Cbeta%2C%5Cmathbf%7Bm%5E%7Bv%7D%7D%2C%5Cmathbf%7Bm%5E%7Bh%7D%7D%29%3D%5Cfrac%7B1%7D%7B2%7D%5Cmathbf%7B%28m%5E%7Bv%7D-%28m%5E%7Bv%7D%29%5E%7B2%7D%29%5E%7BT%7DW%5E%7B2%7D%28m%5E%7Bh%7D-%28m%5E%7Bh%7D%29%5E%7B2%7D%29%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial^{2}}{&#92;partial&#92;beta^{2}}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;frac{1}{2}&#92;mathbf{(m^{v}-(m^{v})^{2})^{T}W^{2}(m^{h}-(m^{h})^{2})} " title="&#92;dfrac{&#92;partial^{2}}{&#92;partial&#92;beta^{2}}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;frac{1}{2}&#92;mathbf{(m^{v}-(m^{v})^{2})^{T}W^{2}(m^{h}-(m^{h})^{2})} " class="latex" /></p>
<p style="text-align:left;">or</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=A%280%2C%5Cmathbf%7Bm%5E%7Bv%7D%7D%29%3D%5Cunderset%7Bi%7D%7B%5Csum%7Dm%5E%7Bv%7D%5C%3Bln%5C%3Bm%5E%7Bv%7D%2B%281-m%5E%7Bv%7D%29ln%281-m%5E%7Bv%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A(0,&#92;mathbf{m^{v}})=&#92;underset{i}{&#92;sum}m^{v}&#92;;ln&#92;;m^{v}+(1-m^{v})ln(1-m^{v}) " title="A(0,&#92;mathbf{m^{v}})=&#92;underset{i}{&#92;sum}m^{v}&#92;;ln&#92;;m^{v}+(1-m^{v})ln(1-m^{v}) " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=A%280%2C%5Cmathbf%7Bm%5E%7Bh%7D%7D%29%3D%5Cunderset%7Bi%7D%7B%5Csum%7Dm%5E%7Bh%7D%5C%3Bln%5C%3Bm%5E%7Bh%7D%2B%281-m%5E%7Bh%7D%29ln%281-m%5E%7Bh%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A(0,&#92;mathbf{m^{h}})=&#92;underset{i}{&#92;sum}m^{h}&#92;;ln&#92;;m^{h}+(1-m^{h})ln(1-m^{h}) " title="A(0,&#92;mathbf{m^{h}})=&#92;underset{i}{&#92;sum}m^{h}&#92;;ln&#92;;m^{h}+(1-m^{h})ln(1-m^{h}) " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%7D%7B%5Cpartial%5Cbeta%7DA%28%5Cbeta%2C%5Cmathbf%7Bm%5E%7Bv%7D%7D%2C%5Cmathbf%7Bm%5E%7Bh%7D%7D%29%3D%5Cunderset%7Bi%7D%7B%5Csum%7Da_%7Bi%7Dm%5E%7Bv%7D_%7Bi%7D%2B%5Cunderset%7Bi%7D%7B%5Csum%7Db_%7Bi%7Dm%5E%7Bh%7D_%7Bi%7D%2B%5Cunderset%7Bi%2Cj%7D%7B%5Csum%7Dm%5E%7Bv%7D_%7Bi%7Dw_%7Bi%2Cj%7Dm%5E%7Bh%7D_%7Bj%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial}{&#92;partial&#92;beta}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;underset{i}{&#92;sum}a_{i}m^{v}_{i}+&#92;underset{i}{&#92;sum}b_{i}m^{h}_{i}+&#92;underset{i,j}{&#92;sum}m^{v}_{i}w_{i,j}m^{h}_{j} " title="&#92;dfrac{&#92;partial}{&#92;partial&#92;beta}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;underset{i}{&#92;sum}a_{i}m^{v}_{i}+&#92;underset{i}{&#92;sum}b_{i}m^{h}_{i}+&#92;underset{i,j}{&#92;sum}m^{v}_{i}w_{i,j}m^{h}_{j} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5Cpartial%5E%7B2%7D%7D%7B%5Cpartial%5Cbeta%5E%7B2%7D%7DA%28%5Cbeta%2C%5Cmathbf%7Bm%5E%7Bv%7D%7D%2C%5Cmathbf%7Bm%5E%7Bh%7D%7D%29%3D%5Cfrac%7B1%7D%7B2%7D%5Cunderset%7Bi%2Cj%7D%7B%5Csum%7D%28m%5E%7Bv%7D_%7Bi%7D-%28m%5E%7Bv%7D%29%5E%7B2%7D_%7Bi%7D%29w%5E%7B2%7D_%7Bi%2Cj%7D%28m%5E%7Bh%7D_%7Bj%7D-%28m%5E%7Bh%7D%29%5E%7B2%7D_%7Bj%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;partial^{2}}{&#92;partial&#92;beta^{2}}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;frac{1}{2}&#92;underset{i,j}{&#92;sum}(m^{v}_{i}-(m^{v})^{2}_{i})w^{2}_{i,j}(m^{h}_{j}-(m^{h})^{2}_{j}) " title="&#92;dfrac{&#92;partial^{2}}{&#92;partial&#92;beta^{2}}A(&#92;beta,&#92;mathbf{m^{v}},&#92;mathbf{m^{h}})=&#92;frac{1}{2}&#92;underset{i,j}{&#92;sum}(m^{v}_{i}-(m^{v})^{2}_{i})w^{2}_{i,j}(m^{h}_{j}-(m^{h})^{2}_{j}) " class="latex" /></p>
<h5>Stationary Condition</h5>
<p>Rather than assume equilibrium, we assume that, at each step during inference &#8211;at fixed (<img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BW%7D%2C+%5Cmathbf%7Ba%7D%2C+%5Cmathbf%7Bb%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathbf{W}, &#92;mathbf{a}, &#92;mathbf{b} " title="&#92;mathbf{W}, &#92;mathbf{a}, &#92;mathbf{b} " class="latex" />&#8211;the system satisfies a <em>quasi-stationary</em> condition.  Each step reaches a a local saddle point in phase space, s.t.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7BdA%7D%7Bd%5Cmathbf%7Bm%7D%7D%3D%5Cleft%5B%5Cdfrac%7BdA%7D%7Bd%5Cmathbf%7Bm%5E%7Bv%7D%7D%7D%2C%5Cdfrac%7BdA%7D%7Bd%5Cmathbf%7Bm%5E%7Bh%7D%7D%7D%5Cright%5D%3D0+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;dfrac{dA}{d&#92;mathbf{m}}=&#92;left[&#92;dfrac{dA}{d&#92;mathbf{m^{v}}},&#92;dfrac{dA}{d&#92;mathbf{m^{h}}}&#92;right]=0 " title="&#92;dfrac{dA}{d&#92;mathbf{m}}=&#92;left[&#92;dfrac{dA}{d&#92;mathbf{m^{v}}},&#92;dfrac{dA}{d&#92;mathbf{m^{h}}}&#92;right]=0 " class="latex" /></p>
<h5>Stationary Magnetizations</h5>
<p>Applying the stationary conditions lets us write coupled equations for the individual magnetizations that effectively define the (second order), high Temp, quasi-equilibrium states</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=m%5E%7Bv%7D_%7Bi%7D%5Csimeq%5Csigma%5Cleft%5Ba_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dm%5E%7Bh%7D_%7Bj%7D-%28m%5E%7Bv%7D_%7Bi%7D-%5Cfrac%7B1%7D%7B2%7D%29w%5E%7B2%7D_%7Bi%2Cj%7D%28m%5E%7Bh%7D_%7Bj%7D-%28m%5E%7Bh%7D_%7Bj%7D%29%5E%7B2%7D%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="m^{v}_{i}&#92;simeq&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}m^{h}_{j}-(m^{v}_{i}-&#92;frac{1}{2})w^{2}_{i,j}(m^{h}_{j}-(m^{h}_{j})^{2})&#92;right] " title="m^{v}_{i}&#92;simeq&#92;sigma&#92;left[a_{i}+&#92;underset{j}{&#92;sum}w_{i,j}m^{h}_{j}-(m^{v}_{i}-&#92;frac{1}{2})w^{2}_{i,j}(m^{h}_{j}-(m^{h}_{j})^{2})&#92;right] " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=m%5E%7Bh%7D_%7Bi%7D%5Csimeq%5Csigma%5Cleft%5Bb_%7Bi%7D%2B%5Cunderset%7Bj%7D%7B%5Csum%7Dw_%7Bi%2Cj%7Dm%5E%7Bv%7D_%7Bj%7D-%28m%5E%7Bh%7D_%7Bi%7D-%5Cfrac%7B1%7D%7B2%7D%29w%5E%7B2%7D_%7Bi%2Cj%7D%28m%5E%7Bv%7D_%7Bj%7D-%28m%5E%7Bv%7D_%7Bj%7D%29%5E%7B2%7D%29%5Cright%5D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="m^{h}_{i}&#92;simeq&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}m^{v}_{j}-(m^{h}_{i}-&#92;frac{1}{2})w^{2}_{i,j}(m^{v}_{j}-(m^{v}_{j})^{2})&#92;right] " title="m^{h}_{i}&#92;simeq&#92;sigma&#92;left[b_{i}+&#92;underset{j}{&#92;sum}w_{i,j}m^{v}_{j}-(m^{h}_{i}-&#92;frac{1}{2})w^{2}_{i,j}(m^{v}_{j}-(m^{v}_{j})^{2})&#92;right] " class="latex" /></p>
<p>Notice that the <img src="https://s0.wp.com/latex.php?latex=m%5E%7Bv%7D_%7Bi%7D%2Cm%5E%7Bh%7D_%7Bi%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="m^{v}_{i},m^{h}_{i} " title="m^{v}_{i},m^{h}_{i} " class="latex" /> resemble the RBM conditional probabilities; in fact, at first order in <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" />, they are the same.</p>
<h5>Mean Field Theory</h5>
<p style="text-align:left;"><img src="https://s0.wp.com/latex.php?latex=%5Cbeta%3D1+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;beta=1 " title="&#92;beta=1 " class="latex" /> gives a mean field theory, and</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=P%28h%7Cv%29%3Dm%5E%7Bh%7D_%7Bi%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="P(h|v)=m^{h}_{i} " title="P(h|v)=m^{h}_{i} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=P%28v%7Ch%29%3Dm%5E%7Bv%7D_%7Bi%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="P(v|h)=m^{v}_{i} " title="P(v|h)=m^{v}_{i} " class="latex" /></p>
<p style="text-align:left;">And in the late 90&#8217;s, mean field TAP theory was attempted, unsuccessfully,  to create an RBM solver.</p>
<h5>Fixed Point Equations</h5>
<p>At second order, the magnetizations <img src="https://s0.wp.com/latex.php?latex=m%5E%7Bv%7D_%7Bi%7D%2Cm%5E%7Bh%7D_%7Bi%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="m^{v}_{i},m^{h}_{i} " title="m^{v}_{i},m^{h}_{i} " class="latex" /> are coupled through the Onsager corrections.  To solve them, we can write down the fixed point equations, shown above.</p>
<h3>Higher Order Corrections</h3>
<p>We can include higher order correction the Free Energy <img src="https://s0.wp.com/latex.php?latex=A+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A " title="A " class="latex" /> by including more terms in the Taylor Series. This is called a <a href="https://nerdwisdom.com/tag/plefka/">Plefka expansion</a>.  The terms can be represented using diagrams</p>
<figure style="width: 310px" class="wp-caption aligncenter"><img class="size-medium" src="https://nerdwisdom.files.wordpress.com/2007/10/expansion001.jpg?w=300&#038;h=239" alt="Plefka Expansion for the Free Energy"   /><figcaption class="wp-caption-text">Plefka (Diagrammatic) Expansion for the Free Energy</figcaption></figure>
<p>Plefka derived these terms in 1982, although it appears he only published up to the Onsager correction; a recent paper shows how to obtain all high order terms.</p>
<p>The Diagrammatic expansion appears not to have been fully worked out, and is only sketched above.</p>
<p>I can think of at least 3 ways to include these higher terms:</p>
<ul>
<li><strong>Include all terms at a higher order in <img src="https://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;beta " title="&#92;beta " class="latex" /> .</strong>   The <a href="https://github.com/sphinxteam/Boltzmann.jl">Sphinx Boltzmann.jl </a>packages includes all terms up to <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BO%7D%28%5Cbeta%5E%7B3%7D%29+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="&#92;mathcal{O}(&#92;beta^{3}) " title="&#92;mathcal{O}(&#92;beta^{3}) " class="latex" /> .  This changes both the Free Energy and the associated fixed point equations for the magnetizations.</li>
</ul>
<figure data-shortcode="caption" id="attachment_9850" style="width: 310px" class="wp-caption aligncenter"><a href="https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-8-39-56-am/#main" rel="attachment wp-att-9850"><img data-attachment-id="9850" data-permalink="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-8-39-56-am/" data-orig-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-39-56-am.png" data-orig-size="586,110" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2016-10-21-at-8-39-56-am" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-39-56-am.png?w=300&#038;h=56" data-large-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-39-56-am.png?w=586" class="size-medium wp-image-9850" src="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-39-56-am.png?w=300&#038;h=56" alt="Third Order Corrections" width="300" height="56" srcset="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-39-56-am.png?w=300&amp;h=56 300w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-39-56-am.png?w=150&amp;h=28 150w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-39-56-am.png 586w" sizes="(max-width: 300px) 100vw, 300px" /></a><figcaption class="wp-caption-text">Third Order Corrections</figcaption></figure>
<figure data-shortcode="caption" id="attachment_9852" style="width: 310px" class="wp-caption aligncenter"><a href="https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-8-42-41-am/#main" rel="attachment wp-att-9852"><img data-attachment-id="9852" data-permalink="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-8-42-41-am/" data-orig-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-42-41-am.png" data-orig-size="670,124" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2016-10-21-at-8-42-41-am" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-42-41-am.png?w=300&#038;h=56" data-large-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-42-41-am.png?w=670" class="size-medium wp-image-9852" src="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-42-41-am.png?w=300&#038;h=56" alt="Fourth Order Corrections" width="300" height="56" srcset="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-42-41-am.png?w=300&amp;h=56 300w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-42-41-am.png?w=600&amp;h=112 600w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-42-41-am.png?w=150&amp;h=28 150w" sizes="(max-width: 300px) 100vw, 300px" /></a><figcaption class="wp-caption-text">Fourth Order Corrections</figcaption></figure>
<ul>
<li><strong>Resum all diagrams of a certain kind.</strong> This is an old-school<em> renormalization </em>procedure<i>, </i>and it would include an <img src="https://s0.wp.com/latex.php?latex=%5Cinfty+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;infty " title="&#92;infty " class="latex" /> number of terms.   This requires working out the analytic expressions for some class of diagrams, such as all circle (RPA-like) diagrams
<figure data-shortcode="caption" id="attachment_9853" style="width: 146px" class="wp-caption aligncenter"><a href="https://charlesmartin14.wordpress.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-8-43-19-am/#main" rel="attachment wp-att-9853"><img data-attachment-id="9853" data-permalink="https://calculatedcontent.com/2016/10/21/improving-rbms-with-physical-chemistry/screen-shot-2016-10-21-at-8-43-19-am/" data-orig-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-43-19-am.png" data-orig-size="218,482" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screen-shot-2016-10-21-at-8-43-19-am" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-43-19-am.png?w=136&#038;h=300" data-large-file="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-43-19-am.png?w=218" class="size-medium wp-image-9853" src="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-43-19-am.png?w=136&#038;h=300" alt="All RPA-like Diagrams, to infinite order" width="136" height="300" srcset="https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-43-19-am.png?w=136&amp;h=300 136w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-43-19-am.png?w=68&amp;h=150 68w, https://charlesmartin14.files.wordpress.com/2016/10/screen-shot-2016-10-21-at-8-43-19-am.png 218w" sizes="(max-width: 136px) 100vw, 136px" /></a><figcaption class="wp-caption-text">all circle diagrams</figcaption></figure>
<p>This is similar, in some sense, to the <a href="https://arxiv.org/abs/1502.02476">infinite RBM by Larochelle,</a> which uses an resummation trick to include an infinite number of  Hidden nodes.</li>
<li><strong>Use a low order <a href="http://mathworld.wolfram.com/PadeApproximant.html">Pade Approximant</a></strong>, to <em>improve</em> the Taylor Series.   The idea is to include higher order terms implicitly by expanding the Free Energy as a ratio of polynomial functions  P, Q</li>
</ul>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=A%5E%7BP%7D%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%3D%5Cdfrac%7BP%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%7D%7BQ%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%7D+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A^{P}(&#92;beta,&#92;mathbf{m})=&#92;dfrac{P(&#92;beta,&#92;mathbf{m})}{Q(&#92;beta,&#92;mathbf{m})} " title="A^{P}(&#92;beta,&#92;mathbf{m})=&#92;dfrac{P(&#92;beta,&#92;mathbf{m})}{Q(&#92;beta,&#92;mathbf{m})} " class="latex" />  , where</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=A%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29+-A%5E%7BP%7D%28%5Cbeta%2C%5Cmathbf%7Bm%7D%29%3D0+&#038;bg=ffffff&#038;fg=303030&#038;s=0" alt="A(&#92;beta,&#92;mathbf{m}) -A^{P}(&#92;beta,&#92;mathbf{m})=0 " title="A(&#92;beta,&#92;mathbf{m}) -A^{P}(&#92;beta,&#92;mathbf{m})=0 " class="latex" />,</p>
<p style="text-align:left;">Obviously there are lots of interesting things to try.</p>
<h3>Beyond Binary Spins; Real-Valued RBMs</h3>
<p>The current <a href="https://github.com/charlesmartin14/emf-rbm/">python EMF_RBM</a> only treats binary data, just like the<a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html"> scikit learn BernoulliRBM</a>.  So for say MNIST, we have to use the <a href="https://www.reddit.com/r/MachineLearning/comments/340261/binary_mnist_in_literature/?st=iukn0wbb&amp;sh=bb90376e">binarized MNIST</a>.</p>
<p>There is some advantage to using<a href="https://arxiv.org/abs/1602.02830"> Binarized Neural Networks on GPUs</a>.</p>
<p>Still, a non-binary RBM may be useful.  <a href="https://arxiv.org/pdf/1606.03956.pdf">Tremel et. al. have suggested how to use real-valued data in the EMF_RBM</a>, although <a href="http://www.itwist16.es.aau.dk/digitalAssets/223/223256_trameleric-itwist2016.pdf">in the context of Compressed Sensing Using Generalized Boltzmann Machines</a>.</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/charlesmartin14.wordpress.com/9029/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/charlesmartin14.wordpress.com/9029/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=calculatedcontent.com&#038;blog=32496692&#038;post=9029&#038;subd=charlesmartin14&#038;ref=&#038;feed=1" width="1" height="1" />



&lt;p&gt;
&lt;a href="http://matlab.cheme.cmu.edu/2012/01/28/are-two-averages-different/" &gt;Matlab post&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
Adapted from &lt;a href="http://stattrek.com/ap-statistics-4/unpaired-means.aspx" &gt;http://stattrek.com/ap-statistics-4/unpaired-means.aspx&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
Class A had 30 students who received an average test score of 78, with standard deviation of 10. Class B had 25 students an average test score of 85, with a standard deviation of 15. We want to know if the difference in these averages is statistically relevant. Note that we only have estimates of the true average and standard deviation for each class, and there is uncertainty in those estimates. As a result, we are unsure if the averages are really different. It could have just been luck that a few students in class B did better.
&lt;/p&gt;

&lt;p&gt;
The hypothesis:
&lt;/p&gt;

&lt;p&gt;
the true averages are the same. We need to perform a two-sample t-test of the hypothesis that \(\mu_1 - \mu_2 = 0\) (this is often called the null hypothesis). we use a two-tailed test because we do not care if the difference is positive or negative, either way means the averages are not the same.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;

&lt;pre class="src src-python"&gt;&lt;span style="color: #8b0000;"&gt;import&lt;/span&gt; numpy &lt;span style="color: #8b0000;"&gt;as&lt;/span&gt; np

n1 = 30  &lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;students in class A&lt;/span&gt;
x1 = 78.0  &lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;average grade in class A&lt;/span&gt;
s1 = 10.0  &lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;std dev of exam grade in class A&lt;/span&gt;

n2 = 25  &lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;students in class B&lt;/span&gt;
x2 = 85.0  &lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;average grade in class B&lt;/span&gt;
s2 = 15.0  &lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;std dev of exam grade in class B&lt;/span&gt;

&lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;the standard error of the difference between the two averages. &lt;/span&gt;
SE = np.sqrt(s1**2 / n1 + s2**2 / n2)

&lt;span style="color: #ff0000; font-weight: bold;"&gt;# &lt;/span&gt;&lt;span style="color: #ff0000; font-weight: bold;"&gt;compute DOF&lt;/span&gt;
DF = (n1 - 1) + (n2 - 1)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
see the discussion at &lt;a href="http://stattrek.com/Help/Glossary.aspx?Target=Two-sample%20t-test" &gt;http://stattrek.com/Help/Glossary.aspx?Target=Two-sample%20t-test&lt;/a&gt; for a more complex definition of degrees of freedom. Here we simply subtract one from each sample size to account for the estimation of the average of each sample.
&lt;/p&gt;


&lt;p&gt;
compute the t-score for our data
&lt;/p&gt;

&lt;p&gt;
The difference between two averages determined from small sample numbers follows the t-distribution. the t-score is the difference between the difference of the means and the hypothesized difference of the means, normalized by the standard error. we compute the absolute value of the t-score to make sure it is positive for convenience later.
&lt;/p&gt;
&lt;div class="org-src-container"&gt;

&lt;pre class="src src-python"&gt;tscore = np.abs(((x1 - x2) - 0) / SE)
&lt;span style="color: #8b0000;"&gt;print&lt;/span&gt; tscore
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
1.99323179108
&lt;/pre&gt;

&lt;p&gt;
Interpretation
&lt;/p&gt;

&lt;p&gt;
A way to approach determinining if the difference is significant or not is to ask, does our computed average fall within a confidence range of the hypothesized value (zero)? If it does, then we can attribute the difference to statistical variations at that confidence level. If it does not, we can say that statistical variations do not account for the difference at that confidence level, and hence the averages must be different.
&lt;/p&gt;

&lt;p&gt;
Let us compute the t-value that corresponds to a 95% confidence level for a mean of zero with the degrees of freedom computed earlier. This means that 95% of the t-scores we expect to get will fall within \(\pm\) t95.
&lt;/p&gt;


&lt;div class="org-src-container"&gt;

&lt;pre class="src src-python"&gt;&lt;span style="color: #8b0000;"&gt;from&lt;/span&gt; scipy.stats.distributions &lt;span style="color: #8b0000;"&gt;import&lt;/span&gt;  t

ci = 0.95;
alpha = 1 - ci;
t95 = t.ppf(1.0 - alpha/2.0, DF)

&lt;span style="color: #8b0000;"&gt;print&lt;/span&gt; t95
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
&amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; 2.00574599354
&lt;/pre&gt;

&lt;p&gt;
since tscore &amp;lt; t95, we conclude that at the 95% confidence level we cannot say these averages are statistically different because our computed t-score falls in the expected range of deviations. Note that our t-score is very close to the 95% limit. Let us consider a smaller confidence interval.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;

&lt;pre class="src src-python"&gt;ci = 0.94
alpha = 1 - ci;
t95 = t.ppf(1.0 - alpha/2.0, DF)

&lt;span style="color: #8b0000;"&gt;print&lt;/span&gt; t95
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
&amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; &amp;gt;&amp;gt;&amp;gt; 1.92191364181
&lt;/pre&gt;

&lt;p&gt;
at the 94% confidence level, however, tscore &amp;gt; t94, which means we can say with 94% confidence that the two averages are different; class B performed better than class A did. Alternatively, there is only about a 6% chance we are wrong about that statement.
another way to get there
&lt;/p&gt;

&lt;p&gt;
An alternative way to get the confidence that the averages are different is to directly compute it from the cumulative t-distribution function. We compute the difference between all the t-values less than tscore and the t-values less than -tscore, which is the fraction of measurements that are between them. You can see here that we are practically 95% sure that the averages are different.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;

&lt;pre class="src src-python"&gt;f = t.cdf(tscore, DF) - t.cdf(-tscore, DF)
&lt;span style="color: #8b0000;"&gt;print&lt;/span&gt; f
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
0.948605075732
&lt;/pre&gt;
&lt;p&gt;Copyright (C) 2013 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;&lt;p&gt;&lt;a href="/org/2013/02/18/Are-averages-different.org"&gt;org-mode source&lt;/a&gt;&lt;p&gt;
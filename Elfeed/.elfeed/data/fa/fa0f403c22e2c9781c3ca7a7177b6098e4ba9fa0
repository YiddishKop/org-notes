<p style="text-align:left;"><strong><span style="color:#ff0000;">Another Holiday Blog:</span>  <span style="color:#008000;">Feedback and Questions are very welcome.</span></strong></p>
<p style="text-align:left;">I have a client who is using Naive Bayes pretty successfully, and the subject has come up as to &#8216;why do we need <em>fancy</em> machine learning methods?&#8217;   A typical question I am asked is:</p>
<p style="text-align:center;"><span style="color:#800000;"><strong><em>Why do we need Regularization ?</em></strong></span></p>
<p style="text-align:center;"><span style="color:#ff6600;"><strong><em>What happens if we just turn the regularizer off ?  </em></strong></span></p>
<p style="text-align:center;"><strong><span style="color:#ff9900;"><em>Can we just set it to a very small, default value ?</em></span></strong></p>
<p style="text-align:left;"><i></i>After all, we can just measure what customers are doing exactly.  Why not just reshow the most popular items, ads, etc to them. Can we just run a Regression?  Or Naive Bayes ?  Or just use the raw empirical estimates?</p>
<p style="text-align:center;"><a href="http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf" target="_blank">Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?</a></p>
<p>Sure, if you are simply estimating historical frequencies,  n-gram statistics, etc, then a simple method like Naive Bayes is fine. But to predict the future..</p>
<p style="text-align:center;"><strong><span style="color:#800080;">the Reason we need Regularization is for Generalization</span>.  </strong></p>
<p>After all, we don&#8217;t just want to make the same recommendations over and over.</p>
<p><img data-attachment-id="8430" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/chuck/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/chuck.jpg?w=100&#038;h=122" data-orig-size="236,289" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="chuck" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/chuck.jpg?w=100&#038;h=122?w=236" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/chuck.jpg?w=100&#038;h=122?w=236" class=" wp-image-8430 alignleft" src="https://charlesmartin14.files.wordpress.com/2015/12/chuck.jpg?w=100&#038;h=122" alt="chuck" width="100" height="122" srcset="https://charlesmartin14.files.wordpress.com/2015/12/chuck.jpg?w=100&amp;h=122 100w, https://charlesmartin14.files.wordpress.com/2015/12/chuck.jpg?w=200&amp;h=244 200w, https://charlesmartin14.files.wordpress.com/2015/12/chuck.jpg?w=122&amp;h=150 122w" sizes="(max-width: 100px) 100vw, 100px" />Unless you are just in love with ad re-targeting (sic).   We want to predict on things we have never seen.</p>
<p>And we need to correct for <a href="http://www.cs.cornell.edu/people/tj/publications/swaminathan_joachims_15a.pdf" target="_blank">presentation bias</a> when collecting data on things we have seen.</p>
<p>Most importantly, we need methods that don&#8217;t break down unexpectedly.</p>
<p style="text-align:left;">In this post, we will see why Regularization is subtle and non-trivial even in seemingly simple linear models like the classic<a href="https://en.wikipedia.org/wiki/Tikhonov_regularization"> Tikhonov Regularization</a>.   And something that is almost never discussed in modern classes.</p>
<p style="text-align:center;"><span style="color:#0000ff;"><strong><em>We demonstrate </em></strong></span><span style="color:#0000ff;"><strong><em>that &#8216;strong&#8217;  overtraining is accompanied by a Phase Transition&#8211;and optimal solutions lies just below this.</em></strong></span></p>
<p>The example is actually motivated by something I saw recently in <a href="http://calculationconsulting.com" target="_blank">my consulting practice</a>&#8211;and it was not obvious at first.</p>
<p>&nbsp;</p>
<p><img class="aligncenter" src="https://i0.wp.com/static5.businessinsider.com/image/4fe317f969bedd812e000015/big-bang-theory-whiteboard-math.jpg" alt="" width="400" height="300" /></p>
<p><em>Still,  please realize&#8211;we are about to discuss a pathological case hidden in the dark corners of Tikhonov Regularization&#8211;a curiosity for mathematicians.  </em></p>
<p style="text-align:center;">See <a href="https://www.quora.com/How-is-singular-value-decomposition-used-in-nlp">this Quora post</a> on SVD in NLP for some additional motivations.</p>
<p>Let&#8217;s begin when Vapnik [1], Smale [2], and the other great minds of the field begin:</p>
<p style="text-align:left;"><strong>(Regularized) Linear Regression:</strong></p>
<p style="text-align:left;">The most basic method in all statistics is linear regression</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D%3D%5Cmathbf%7By%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}&#92;mathbf{w}=&#92;mathbf{y} " title="&#92;mathbf{X}&#92;mathbf{w}=&#92;mathbf{y} " class="latex" />.</p>
<p style="text-align:left;"><a href="https://charlesmartin14.wordpress.com/?attachment_id=8273#main" rel=" rel=&quot;attachment wp-att-8273&quot;"><img data-attachment-id="8273" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/gauss_banknote/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/gauss_banknote.png" data-orig-size="400,265" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Gauss_banknote" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/gauss_banknote.png?w=110&#038;h=73" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/gauss_banknote.png?w=400" class="wp-image-8273 alignleft" src="https://charlesmartin14.files.wordpress.com/2015/12/gauss_banknote.png?w=110&#038;h=73" alt="Gauss_banknote" width="110" height="73" srcset="https://charlesmartin14.files.wordpress.com/2015/12/gauss_banknote.png?w=110&amp;h=73 110w, https://charlesmartin14.files.wordpress.com/2015/12/gauss_banknote.png?w=220&amp;h=146 220w, https://charlesmartin14.files.wordpress.com/2015/12/gauss_banknote.png?w=150&amp;h=99 150w" sizes="(max-width: 110px) 100vw, 110px" /></a></p>
<p style="text-align:left;">It is attributed to Gauss,<a href="https://www.quora.com/Who-is-the-most-badass-mathematician-ever" target="_blank"> one of the greatest mathematicians ever</a>.</p>
<p style="text-align:left;">One solution is to assume Gaussian noise and minimize the error.</p>
<p style="text-align:left;">The solution can also be constructed using the Moore-Penrose PseudoInverse.</p>
<p style="text-align:left;">If we multiply by  <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}^{&#92;dagger}" title="&#92;mathbf{X}^{&#92;dagger}" class="latex" /> to the left on both sides, we obtain</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D%3D%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7By%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}&#92;mathbf{w}=&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " title="&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}&#92;mathbf{w}=&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " class="latex" /></p>
<p style="text-align:left;">The formal solution is</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7Bw%7D%7D%3D%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D%29%5E%7B-1%7D%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7By%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;hat{w}}=(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X})^{-1}&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " title="&#92;mathbf{&#92;hat{w}}=(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X})^{-1}&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " class="latex" /></p>
<p style="text-align:left;">which is only valid when we can actually invert <em>the data covariance matrix</em> <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X} " title="&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X} " class="latex" />.</p>
<p><em>We say the problem is well-posed when <img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D%29%5E%7B-1%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X})^{-1} " title="(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X})^{-1} " class="latex" /></em></p>
<ol>
<li><em>    exists,   </em></li>
<li><em>  is unique, and </em></li>
<li><em>is stable. </em></li>
</ol>
<p>Otherwise we say it is singular, or ill-posed [1].</p>
<p style="text-align:left;">In nearly all practical methods, we would not compute the inverse directly, but use some iterative technique to solve for <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7Bw%7D%7D&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;hat{w}}" title="&#92;mathbf{&#92;hat{w}}" class="latex" />. In nearly all practical cases, even when the matrix seems invertible, or even well posed, it may have numerical instabilities, either due to the data or the numerical solver.  Vapnik refers to these as <em>stochastically ill posed problems [4]</em></p>
<p style="text-align:center;"><span style="color:#ff0000;"><strong><em>Frequently X<sup>T</sup>X can not be inverted..or should not be inverted..directly.</em></strong></span></p>
<h4 style="text-align:left;">Tikhonov Regularization</h4>
<p>A  trivial solution is to simply &#8216;regularize&#8217; the matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X} " title="&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X} " class="latex" /> by adding a small, non-zero value to the diagonal:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D%29%5E%7B-1%7D%5Crightarrow%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D-%5Clambda%5Cmathbf%7BI%7D%29%5E%7B-1%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X})^{-1}&#92;rightarrow(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})^{-1} " title="(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X})^{-1}&#92;rightarrow(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})^{-1} " class="latex" />.</p>
<p>Naively, this seems like it would dampen instabilities and allow for a robust numerical solution.  And it does&#8230;in <em>most</em> cases.</p>
<p>If you want to sound like you are doing some fancy math, give it a Russian name; we call this <em>Tikhonov Regularization</em> [4]:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7Bw%7D%7D%28%5Clambda%29%3D%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D-%5Clambda%5Cmathbf%7BI%7D%29%5E%7B-1%7D%29%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7By%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;hat{w}}(&#92;lambda)=(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})^{-1})&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " title="&#92;mathbf{&#92;hat{w}}(&#92;lambda)=(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})^{-1})&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " class="latex" /></p>
<p style="text-align:left;"><img data-attachment-id="8446" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/crossvalidation/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png" data-orig-size="389,536" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="crossvalidation" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png?w=137&#038;h=188" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png?w=389" class="wp-image-8446 alignright" src="https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png?w=137&#038;h=188" alt="crossvalidation" width="137" height="188" srcset="https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png?w=137&amp;h=188 137w, https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png?w=274&amp;h=376 274w, https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png?w=109&amp;h=150 109w, https://charlesmartin14.files.wordpress.com/2015/12/crossvalidation.png?w=218&amp;h=300 218w" sizes="(max-width: 137px) 100vw, 137px" />This is (also) called Ridge Regression in many common packages such as <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" target="_blank">scikit learn</a>.</p>
<p style="text-align:left;">The parameter <img src="https://s0.wp.com/latex.php?latex=%5Clambda+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda " title="&#92;lambda " class="latex" /> is the regularization parameter values, usually found with Cross Validation (CV).  While a data science best practice, CV can fail !</p>
<p style="text-align:left;">Grid searching <img src="https://s0.wp.com/latex.php?latex=%5Clambda+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda " title="&#92;lambda " class="latex" />, we can obtain <a href="https://www.sintef.no/globalassets/project/evitameeting/2005/lcurve.pdf" target="_blank">the infamous L-curve</a>:</p>
<p style="text-align:left;"><a href="https://charlesmartin14.wordpress.com/?attachment_id=8630#main" rel=" rel=&quot;attachment wp-att-8630&quot;"><img data-attachment-id="8630" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/screen-shot-2015-12-27-at-10-02-54-pm/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-10-02-54-pm.png" data-orig-size="1278,1310" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2015-12-27 at 10.02.54 PM" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-10-02-54-pm.png?w=293&#038;h=300" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-10-02-54-pm.png?w=840" class="size-medium wp-image-8630 aligncenter" src="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-10-02-54-pm.png?w=293&#038;h=300" alt="Screen Shot 2015-12-27 at 10.02.54 PM" width="293" height="300" srcset="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-10-02-54-pm.png?w=293&amp;h=300 293w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-10-02-54-pm.png?w=586&amp;h=600 586w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-10-02-54-pm.png?w=146&amp;h=150 146w" sizes="(max-width: 293px) 100vw, 293px" /></a></p>
<p style="text-align:left;">The L-curve is a log-log plot of the the norm of the regularized solution vs. the norm of the residual.  The best <img src="https://s0.wp.com/latex.php?latex=%5Clambda+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda " title="&#92;lambda " class="latex" /> lies somewhere along the bottom of the L, but we can&#8217;t really tell where (<strong><span style="color:#ff6600;"><em>even with cross validation</em></span></strong>).</p>
<p style="text-align:center;"><span style="color:#ff0000;"><em><strong>This challenge of regularization is absent from basic machine learning classes?</strong></em></span></p>
<p style="text-align:left;">The optimal <img src="https://s0.wp.com/latex.php?latex=%5Clambda+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda " title="&#92;lambda " class="latex" /> is actually hard to find.   Cross Validation (CV) only works in ideal cases.  And we need a CV metric, like <img src="https://s0.wp.com/latex.php?latex=R%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="R^{2} " title="R^{2} " class="latex" />, which also only works in ideal cases.</p>
<p style="text-align:left;">We might naively imagine <img src="https://s0.wp.com/latex.php?latex=%5Clambda+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda " title="&#92;lambda " class="latex" /> can just be very small&#8211;in effect, turning off the regularizer.  This is seen in the curve above, where the solution norm diverges.</p>
<p style="text-align:left;"><span style="color:#000000;text-align:left;line-height:1.5;">The regularization fails in these 2 notable cases, when</span></p>
<ol>
<li style="text-align:left;"> the model errors are correlated,  which fools simple cross validation</li>
<li style="text-align:left;"><span style="color:#000000;text-align:left;line-height:1.5;"> <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Crightarrow+0+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda&#92;rightarrow 0 " title="&#92;lambda&#92;rightarrow 0 " class="latex" /> </span><strong style="color:#000000;text-align:left;line-height:1.5;">and</strong><span style="color:#000000;text-align:left;line-height:1.5;"> the number of features ~ the number of training instances, which leads to a phase transition.</span><span style="color:#000000;text-align:left;line-height:1.5;"> </span></li>
</ol>
<p>Today we will examine this curious, spurious behavior in case 2 (and look briefly at 1)</p>
<h4>Regimes of Applications</h4>
<p><a href="https://charlesmartin14.wordpress.com/?attachment_id=8285#main" rel=" rel=&quot;attachment wp-att-8285&quot;"><img data-attachment-id="8285" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/strangelove/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/strangelove.jpg" data-orig-size="1600,917" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="strangelove" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/strangelove.jpg?w=118&#038;h=68" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/strangelove.jpg?w=840" class="wp-image-8285 alignleft" src="https://charlesmartin14.files.wordpress.com/2015/12/strangelove.jpg?w=118&#038;h=68" alt="strangelove" width="118" height="68" srcset="https://charlesmartin14.files.wordpress.com/2015/12/strangelove.jpg?w=118&amp;h=68 118w, https://charlesmartin14.files.wordpress.com/2015/12/strangelove.jpg?w=236&amp;h=136 236w, https://charlesmartin14.files.wordpress.com/2015/12/strangelove.jpg?w=150&amp;h=86 150w" sizes="(max-width: 118px) 100vw, 118px" /></a></p>
<p>As von Neumann said once, <em><span style="color:#993300;">&#8220;With four parameters I can fit an elephant, and with five I can make him wiggle his trunk&#8221;</span></em></p>
<p>To understand where the regularization can fail, we need to distinguish between the cases in which Ridge Regression is commonly used.</p>
<p>Say we have <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f} " title="N_{f} " class="latex" /> features, and <img src="https://s0.wp.com/latex.php?latex=N_%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{I} " title="N_{I} " class="latex" /> instances.  We distinguish between the <span style="color:#008000;"><strong>High Bias</strong></span> and <span style="color:#008000;"><strong>High Variance</strong> </span>regimes.</p>
<p>&nbsp;</p>
<p><img class="aligncenter" src="https://i0.wp.com/antianti.org/wp-content/uploads/2010/12/highVarianceHighBias.png" alt="" width="521" height="258" /></p>
<h5>High Variance: <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%5Cgg+N_%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}&#92;gg N_{I} " title="N_{f}&#92;gg N_{I} " class="latex" /></h5>
<p>This regime is <span style="color:#008000;"><strong>overdetermined,</strong></span> complicated models that are subject to overtraining.   We find multiple solutions which satisfy our constraints.</p>
<p>Most big data machine learning problems lie here.  We might have 100M documents and 100K words.  Or 1M images and simply all pixels as the (base) features.</p>
<p><img class="aligncenter" src="https://i2.wp.com/artsandsciences.sc.edu/sites/default/files/dilbert_1.jpg" alt="" width="830" height="260" /><img alt="" /></p>
<p><span style="line-height:1.5;">Regularization lets us pick the best of solution which is the most smooth (L2 norm), or the most sparse (L1 norm), or maybe even the one with the least presentation bias (i.e. using Variance regularization to implement <a href="http://arxiv.org/pdf/1502.02362v2.pdf" target="_blank">Counterfactural Risk Minimization</a>) </span></p>
<h5>High Bias regime: <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%5Cll+N_%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}&#92;ll N_{I} " title="N_{f}&#92;ll N_{I} " class="latex" /></h5>
<p>This is the <strong>Underdetermined </strong>regime, and any solution we find, regularized or not, will most likely  generalize poorly.</p>
<p>When we have more features than instances, there is no solution at all (let alone a unique one)</p>
<p><a href="https://charlesmartin14.wordpress.com/?attachment_id=8503#main" rel=" rel=&quot;attachment wp-att-8503&quot;"><img data-attachment-id="8503" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/matrix/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg" data-orig-size="400,400" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="matrix" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg?w=180&#038;h=181" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg?w=400" class=" wp-image-8503 aligncenter" src="https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg?w=180&#038;h=181" alt="matrix" width="180" height="181" srcset="https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg?w=180&amp;h=181 180w, https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg?w=360&amp;h=360 360w, https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg?w=150&amp;h=150 150w, https://charlesmartin14.files.wordpress.com/2015/12/matrix.jpg?w=300&amp;h=300 300w" sizes="(max-width: 180px) 100vw, 180px" /></a></p>
<p><span style="line-height:1.5;">Still, we can pick a regularizer, and the effect is similar to dimensional reduction. </span><span style="line-height:1.5;">Tikhonov Regularization is similar truncated SVD (explained below)</span></p>
<p>Any solution we find may work, but the predictions will be strongly biased towards the training data.</p>
<p><a href="https://www.quora.com/Is-it-true-that-a-fitted-regression-line-using-a-sample-of-data-gives-imperfect-predictions-for-future-observations-due-to-only-sampling-variability" target="_blank">But it is not only just sampling variability that can lead to poor predictions</a>.</p>
<p>In between these 2 limits is an seemingly harmless case, however, this is really a</p>
<h5>Danger Zone:  <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%5Capprox+N_%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}&#92;approx N_{I} " title="N_{f}&#92;approx N_{I} " class="latex" /></h5>
<p style="text-align:left;"><img data-attachment-id="8364" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/danger-will-robinson/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/danger-will-robinson.png" data-orig-size="400,368" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="danger-will-robinson" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/danger-will-robinson.png?w=107&#038;h=98" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/danger-will-robinson.png?w=400" class="wp-image-8364 alignleft" src="https://charlesmartin14.files.wordpress.com/2015/12/danger-will-robinson.png?w=107&#038;h=98" alt="danger-will-robinson" width="107" height="98" srcset="https://charlesmartin14.files.wordpress.com/2015/12/danger-will-robinson.png?w=107&amp;h=98 107w, https://charlesmartin14.files.wordpress.com/2015/12/danger-will-robinson.png?w=214&amp;h=196 214w, https://charlesmartin14.files.wordpress.com/2015/12/danger-will-robinson.png?w=150&amp;h=138 150w" sizes="(max-width: 107px) 100vw, 107px" />This is a <strong><span style="color:#008000;"><em>rare case</em></span> </strong>where the number of features = the number of instances.</p>
<p style="text-align:left;">This does come up in practical problems, such as classifying a large number of small text phrases.  (Something I have been working on with one of my larger clients)</p>
<p style="text-align:left;"><a href="https://charlesmartin14.wordpress.com/?attachment_id=8292#main" rel=" rel=&quot;attachment wp-att-8292&quot;"><img data-attachment-id="8292" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/phrase-cloud/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/phrase-cloud.jpeg" data-orig-size="334,151" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phrase-cloud" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/phrase-cloud.jpeg?w=300&#038;h=136" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/phrase-cloud.jpeg?w=334" class="aligncenter size-medium wp-image-8292" src="https://charlesmartin14.files.wordpress.com/2015/12/phrase-cloud.jpeg?w=300&#038;h=136" alt="phrase-cloud" width="300" height="136" srcset="https://charlesmartin14.files.wordpress.com/2015/12/phrase-cloud.jpeg?w=300&amp;h=136 300w, https://charlesmartin14.files.wordpress.com/2015/12/phrase-cloud.jpeg?w=150&amp;h=68 150w, https://charlesmartin14.files.wordpress.com/2015/12/phrase-cloud.jpeg 334w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p style="text-align:center;"><span style="color:#003366;"><strong><span style="color:#008080;">The number of phrases ~ the number of unique words</span>.</strong></span></p>
<p>This is a dangerous case &#8230; not only because it seems so simple &#8230; but because the general theory breaks down.  Fortunately, it is only pathologicial in</p>
<h4>The limit of zero regularization</h4>
<p>We examine how these different regimes behave for small values of <img src="https://s0.wp.com/latex.php?latex=%5Clambda+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda " title="&#92;lambda " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cunderset%7B%5Clambda%5Crightarrow+0%7D%7B%5Clim%7D%5C%3B%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D-%5Clambda%5Cmathbf%7BI%7D%29%5E%7B-1%7D%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7By%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;underset{&#92;lambda&#92;rightarrow 0}{&#92;lim}&#92;;(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})^{-1}&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " title="&#92;underset{&#92;lambda&#92;rightarrow 0}{&#92;lim}&#92;;(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})^{-1}&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{y} " class="latex" /></p>
<p style="text-align:left;">Let&#8217;s formalize and consider the how the predicted accuracy behaves.</p>
<h5>The Setup:  An Analytical Formulation</h5>
<p>We analyze our regression under Gaussian noise <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cxi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;xi} " title="&#92;mathbf{&#92;xi} " class="latex" />.  Let</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=y%3D%5Cmathbf%7BX%7D%5Cmathbf%7Bw%7D%2A%2B%5Cmathbf%7B%5Cxi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="y=&#92;mathbf{X}&#92;mathbf{w}*+&#92;mathbf{&#92;xi} " title="y=&#92;mathbf{X}&#92;mathbf{w}*+&#92;mathbf{&#92;xi} " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cxi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;xi} " title="&#92;mathbf{&#92;xi} " class="latex" /> is a Gaussian random variable with unit mean and variance <img src="https://s0.wp.com/latex.php?latex=%5Csigma%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;sigma^{2} " title="&#92;sigma^{2} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cxi%7D%3DN%280%2C%5Csigma%5E%7B2%7D%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;xi}=N(0,&#92;sigma^{2}) " title="&#92;mathbf{&#92;xi}=N(0,&#92;sigma^{2}) " class="latex" /></p>
<p>This simple model lets us analyze predicted Accuracy analytically.</p>
<p>Let</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7Bw%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;hat{w}} " title="&#92;mathbf{&#92;hat{w}} " class="latex" /> be the optimal <em><span style="color:#800080;"><strong>Estimate</strong></span></em>, and</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%5E%7B%2A%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{w^{*}} " title="&#92;mathbf{w^{*}} " class="latex" /> be the <span style="color:#333399;"><em><strong>Ground Truth</strong></em></span>, and</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cbar%7Bw%7D%7D%3D%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5B%5Cmathbf%7B%5Chat%7Bw%7D%7D%5D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;bar{w}}=&#92;mathbb{E}_{&#92;xi}[&#92;mathbf{&#92;hat{w}}] " title="&#92;mathbf{&#92;bar{w}}=&#92;mathbb{E}_{&#92;xi}[&#92;mathbf{&#92;hat{w}}] " class="latex" /> be expected (mean).</p>
<p>We would like define, and the decompose, the Generalization Accuracy into Bias and Variance contributions.</p>
<p>Second, to derive the <span style="color:#333399;"><em><strong>Generalization Error</strong></em></span>, we need to work out how the <span style="color:#800080;"><em><strong>estimator</strong></em></span> behaves as a random variable. Frequently, in Machine Learning research, one examines the worst case scenario.  Alternatively, we use the average case.</p>
<p>Define the <span style="color:#800080;"><strong><em>Estimation Error</em></strong></span></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Err%28%5Cmathbf%7B%5Chat%7Bw%7D%7D%29%3D%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Err(&#92;mathbf{&#92;hat{w}})=&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{w^{*}}&#92;Vert^{2} " title="Err(&#92;mathbf{&#92;hat{w}})=&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{w^{*}}&#92;Vert^{2} " class="latex" /></p>
<p>Notice that by <span style="color:#333399;"><em><strong>Generalization Error</strong></em></span>, we usually we want to know how our model performs on a hold set or test point <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{x} " title="&#92;mathbf{x} " class="latex" />:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Cxi%7D%7D%5B%28%5Cmathbf%7Bx%5E%7B%5Cdagger%7D%7D%28%5Chat%7B%5Cmathbf%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%29%29%5E%7B2%7D%5D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;mathbf{&#92;xi}}[(&#92;mathbf{x^{&#92;dagger}}(&#92;hat{&#92;mathbf{w}}-&#92;mathbf{w^{*}}))^{2}] " title="&#92;mathbb{E}_{&#92;mathbf{&#92;xi}}[(&#92;mathbf{x^{&#92;dagger}}(&#92;hat{&#92;mathbf{w}}-&#92;mathbf{w^{*}}))^{2}] " class="latex" /></p>
<p>In the Appendix, we work out the generalization bounds for the worst case and the average case.  From here on, however, when we refer to <span style="color:#333399;"><em><strong>Generalization Error</strong></em></span>, we mean the average case <span style="color:#800080;"><strong><em>Estimation Error</em></strong><span style="color:#000000;"> (above).</span></span></p>
<h5>Bias-Variance Tradeoff</h5>
<p>For Ridge Regression, the mean estimator is</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Cbar%7Bw%7D%7D%3D%28%5Cmathbf%7BX%5E%7B%5Cdagger%7DX%7D-%5Clambda%5Cmathbf%7BI%7D%29%5E%7B-1%7D%5Cmathbf%7BX%5E%7B%5Cdagger%7DX%7D%5Cmathbf%7Bw%5E%7B%2A%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;bar{w}}=(&#92;mathbf{X^{&#92;dagger}X}-&#92;lambda&#92;mathbf{I})^{-1}&#92;mathbf{X^{&#92;dagger}X}&#92;mathbf{w^{*}} " title="&#92;mathbf{&#92;bar{w}}=(&#92;mathbf{X^{&#92;dagger}X}-&#92;lambda&#92;mathbf{I})^{-1}&#92;mathbf{X^{&#92;dagger}X}&#92;mathbf{w^{*}} " class="latex" /></p>
<p>and its variation is</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7B%5Cbar%7Bw%7D%7D%3D%28%5Cmathbf%7BX%5E%7B%5Cdagger%7DX%7D-%5Clambda%5Cmathbf%7BI%7D%29%5E%7B-1%7D%5Cmathbf%7BX%5E%7B%5Cdagger%7D%7D%5Cmathbf%7B%5Cxi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}=(&#92;mathbf{X^{&#92;dagger}X}-&#92;lambda&#92;mathbf{I})^{-1}&#92;mathbf{X^{&#92;dagger}}&#92;mathbf{&#92;xi} " title="&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}=(&#92;mathbf{X^{&#92;dagger}X}-&#92;lambda&#92;mathbf{I})^{-1}&#92;mathbf{X^{&#92;dagger}}&#92;mathbf{&#92;xi} " class="latex" /></p>
<p>We can now decompose the <span style="color:#800080;"><strong><em>Estimation Error</em></strong></span><span style="color:#000000;"> into 2 terms:</span></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cxi%7D%3D%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7B%5Cbar%7Bw%7D%7D%5CVert%5E%7B2%7D%2B%5CVert%5Cmathbf%7B%5Cbar%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%5E%7B2%7D%5C%3B%5C%3B&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;xi}=&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}+&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert^{2}&#92;;&#92;;" title="&#92;mathbb{E}_{&#92;xi}=&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}+&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert^{2}&#92;;&#92;;" class="latex" />,  where</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7B%5Cbar%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert " title="&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert " class="latex" /> is the <span style="color:#000080;"><strong>Bias</strong></span>, and</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7B%5Cbar%7Bw%7D%7D%5CVert%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2} " title="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2} " class="latex" />  is the <span style="color:#000080;"><strong>Variance</strong><span style="color:#000000;">,</span></span></p>
<p style="text-align:left;"><small></small>and examine each regime in the limit <img src="https://s0.wp.com/latex.php?latex=%5Clambda%5Crightarrow+0+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda&#92;rightarrow 0 " title="&#92;lambda&#92;rightarrow 0 " class="latex" /></p>
<p>The <span style="color:#000080;"><strong>Bias</strong></span> is just the error of the average estimator:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7B%5Cbar%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%3D%5Clambda%5E%7B2%7D%5CVert%28%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D-%5Clambda%5Cmathbf%7BI%7D%29%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=&#92;lambda^{2}&#92;Vert((&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})&#92;mathbf{w^{*}}&#92;Vert^{2} " title="&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=&#92;lambda^{2}&#92;Vert((&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}-&#92;lambda&#92;mathbf{I})&#92;mathbf{w^{*}}&#92;Vert^{2} " class="latex" /></p>
<p>and the <span style="color:#000080;"><strong>Variance</strong></span> is the trace of the Covariance of the estimator</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7B%5Cbar%7Bw%7D%7D%5CVert%5E%7B2%7D%3D%5Csigma%5E%7B2%7D%5C%3BTr%5Cbigg%28%5Cdfrac%7B%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D%7D%7B%28%5Cmathbf%7BX%7D%5E%7B%5Cdagger%7D%5Cmathbf%7BX%7D%2B%5Clambda%5Cmathbf%7BI%7D%29%5E%7B2%7D%7D%5Cbigg%29+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}=&#92;sigma^{2}&#92;;Tr&#92;bigg(&#92;dfrac{&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}}{(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}+&#92;lambda&#92;mathbf{I})^{2}}&#92;bigg) " title="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}=&#92;sigma^{2}&#92;;Tr&#92;bigg(&#92;dfrac{&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}}{(&#92;mathbf{X}^{&#92;dagger}&#92;mathbf{X}+&#92;lambda&#92;mathbf{I})^{2}}&#92;bigg) " class="latex" /></p>
<p style="text-align:left;">We can examine the limiting behavior of these statistics by looking at the leading terms in a series expansion for each. Consider the Singular Value Decomposition (SVD) of <img src="https://s0.wp.com/latex.php?latex=X+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="X " title="X " class="latex" /></p>
<p style="text-align:center;"> <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3D%5Cmathbf%7BUSV%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}=&#92;mathbf{USV} " title="&#92;mathbf{X}=&#92;mathbf{USV} " class="latex" />.</p>
<p style="text-align:left;">Let <img src="https://s0.wp.com/latex.php?latex=%5C%7Bs_%7B1%7D%2Cs_%7B2%7D%2C%5Ccdots%2Cs_%7Bm%7D%5C%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;{s_{1},s_{2},&#92;cdots,s_{m}&#92;} " title="&#92;{s_{1},s_{2},&#92;cdots,s_{m}&#92;} " class="latex" /> be the positive singular values, and <img src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cmathbf%7Bv%7D_%7Bi%7D%5C%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;{&#92;mathbf{v}_{i}&#92;} " title="&#92;{&#92;mathbf{v}_{i}&#92;} " class="latex" /> be the right singular vectors.</p>
<p style="text-align:left;">We can write the <span style="color:#000080;"><strong>Bias</strong></span> as</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7B%5Cbar%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%3D%5Csum%5Climits_%7B1%7D%5E%7BN_%7Bf%7D%7D%5Cbig%28%5Cdfrac%7B%5Clambda%5Cmathbf%7Bv%7D_%7Bi%7D%5E%7B%5Cdagger%7D%5Cmathbf%7Bw%7D%5E%7B%2A%7D%7D%7B%28s_%7Bi%7D%5E%7B2%7D%2B%5Clambda%29%7D%5Cbig%29%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=&#92;sum&#92;limits_{1}^{N_{f}}&#92;big(&#92;dfrac{&#92;lambda&#92;mathbf{v}_{i}^{&#92;dagger}&#92;mathbf{w}^{*}}{(s_{i}^{2}+&#92;lambda)}&#92;big)^{2} " title="&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=&#92;sum&#92;limits_{1}^{N_{f}}&#92;big(&#92;dfrac{&#92;lambda&#92;mathbf{v}_{i}^{&#92;dagger}&#92;mathbf{w}^{*}}{(s_{i}^{2}+&#92;lambda)}&#92;big)^{2} " class="latex" /></p>
<p style="text-align:left;">With no regularization, and the number of features exceeds the number of instances, there is a strong bias, determined by the &#8216;extra&#8217; singular values</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clim%5Climits_%7B%5Clambda%5Crightarrow+0%7D%5CVert%5Cmathbf%7B%5Cbar%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%3D%5Csum%5Climits_%7BN_%7Bi%7D%2B1%7D%5E%7BN_%7Bf%7D%7D%28%5Cmathbf%7Bv%7D_%7Bi%7D%5E%7B%5Cdagger%7D%5Cmathbf%7Bw%7D%5E%7B%2A%7D%29%5E%7B2%7D%5C%3B%5C%3Bwhen%5C%3B%5C%3BN_%7Bf%7D%5Cgg+N_%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lim&#92;limits_{&#92;lambda&#92;rightarrow 0}&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=&#92;sum&#92;limits_{N_{i}+1}^{N_{f}}(&#92;mathbf{v}_{i}^{&#92;dagger}&#92;mathbf{w}^{*})^{2}&#92;;&#92;;when&#92;;&#92;;N_{f}&#92;gg N_{I} " title="&#92;lim&#92;limits_{&#92;lambda&#92;rightarrow 0}&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=&#92;sum&#92;limits_{N_{i}+1}^{N_{f}}(&#92;mathbf{v}_{i}^{&#92;dagger}&#92;mathbf{w}^{*})^{2}&#92;;&#92;;when&#92;;&#92;;N_{f}&#92;gg N_{I} " class="latex" /></p>
<p style="text-align:left;">Otherwise, the Bias vanishes.</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clim%5Climits_%7B%5Clambda%5Crightarrow+0%7D%5CVert%5Cmathbf%7B%5Cbar%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%3D0%5C%3B%5C%3Bwhen%5C%3B%5C%3BN_%7Bf%7D%5Cle+N_%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lim&#92;limits_{&#92;lambda&#92;rightarrow 0}&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=0&#92;;&#92;;when&#92;;&#92;;N_{f}&#92;le N_{I} " title="&#92;lim&#92;limits_{&#92;lambda&#92;rightarrow 0}&#92;Vert&#92;mathbf{&#92;bar{w}}-&#92;mathbf{w^{*}}&#92;Vert=0&#92;;&#92;;when&#92;;&#92;;N_{f}&#92;le N_{I} " class="latex" /></p>
<p>So the Bias appears to only matter in the underdetermined case.</p>
<p>(Actually, this is misleading; in the overdetermined case, we can introduce a bias when tuning the regularization parameter too high)</p>
<p>In contrast, the <span style="color:#000080;"><strong>Variance</strong></span></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7B%5Cbar%7Bw%7D%7D%5CVert%5E%7B2%7D%3D%5Csigma%5E%7B2%7D%5Csum%5Climits_%7B1%7D%5E%7Bm%7D%5Cdfrac%7Bs_%7Bi%7D%5E%7B2%7D%7D%7B%28s%5E%7B2%7D_%7Bi%7D%2B%5Clambda%29%5E%7B2%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}=&#92;sigma^{2}&#92;sum&#92;limits_{1}^{m}&#92;dfrac{s_{i}^{2}}{(s^{2}_{i}+&#92;lambda)^{2}} " title="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}=&#92;sigma^{2}&#92;sum&#92;limits_{1}^{m}&#92;dfrac{s_{i}^{2}}{(s^{2}_{i}+&#92;lambda)^{2}} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clim%5Climits_%7B%5Clambda%5Crightarrow+0%7D%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7B%5Cbar%7Bw%7D%7D%5CVert%5E%7B2%7D%3D%5Csum%5Climits_%7B1%7D%5E%7Bm%7Ds_%7Bi%7D%5E%7B-2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lim&#92;limits_{&#92;lambda&#92;rightarrow 0}&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}=&#92;sum&#92;limits_{1}^{m}s_{i}^{-2} " title="&#92;lim&#92;limits_{&#92;lambda&#92;rightarrow 0}&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}=&#92;sum&#92;limits_{1}^{m}s_{i}^{-2} " class="latex" /></p>
<p>can diverge if the minimal singular value is small:</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D-%5Cmathbf%7B%5Cbar%7Bw%7D%7D%5CVert%5E%7B2%7D%5Crightarrow%5Cinfty+%5C%3B%5C%3Bwhen%5C%3B%5C%3B+s_%7B0%7D%5Csim+0+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}&#92;rightarrow&#92;infty &#92;;&#92;;when&#92;;&#92;; s_{0}&#92;sim 0 " title="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;mathbf{&#92;hat{w}}-&#92;mathbf{&#92;bar{w}}&#92;Vert^{2}&#92;rightarrow&#92;infty &#92;;&#92;;when&#92;;&#92;; s_{0}&#92;sim 0 " class="latex" /></p>
<p style="text-align:left;">That is,  if the singular values decrease too fast, the variance can explode.</p>
<p style="text-align:left;">And this can happen in the danger zone</p>
<p style="text-align:center;"><strong><span style="color:#339966;"><em>When <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%5Csim+N_%7Bi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}&#92;sim N_{i} " title="N_{f}&#92;sim N_{i} " class="latex" /> the variance can be infinite! </em></span></strong></p>
<p>Which also means the Central Limit Theorem (CLT) breaks down&#8230;at least how we normally use it.</p>
<p>Traditionally, the (classical) CLT says that the infinite sum of i.i.d. random variables <img src="https://s0.wp.com/latex.php?latex=%5C%7Bx_%7Bi%7D%5C%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;{x_{i}&#92;} " title="&#92;{x_{i}&#92;} " class="latex" /> converges to a Gaussian distribution&#8211;when the variance of the <img src="https://s0.wp.com/latex.php?latex=%7Bx_%7Bi%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="{x_{i}} " title="{x_{i}} " class="latex" /> is finite.  We can, however, generalize the CLT to show that, when the variance is infinite, the sum converges to a Levy or <img src="https://s0.wp.com/latex.php?latex=%5Calpha+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;alpha " title="&#92;alpha " class="latex" />-stable, power-law distribution.</p>
<p>These power-law distributions are quite interesting&#8211;and arise frequently in chemistry and physics during a phase transition. But first, let&#8217;s see the divergent behavior actually arise.</p>
<h4>When More is Less:  Singularities</h4>
<p>Ryota Tomioka [3] has worked some excellent examples using standard data sets</p>
<p><a href="https://charlesmartin14.wordpress.com/?attachment_id=8372#main" rel=" rel=&quot;attachment wp-att-8372&quot;"><img data-attachment-id="8372" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/screen-shot-2015-12-17-at-4-01-56-pm/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-01-56-pm.png" data-orig-size="1852,1276" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2015-12-17 at 4.01.56 PM" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-01-56-pm.png?w=300&#038;h=207" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-01-56-pm.png?w=840" class="aligncenter size-medium wp-image-8372" src="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-01-56-pm.png?w=300&#038;h=207" alt="Screen Shot 2015-12-17 at 4.01.56 PM" width="300" height="207" srcset="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-01-56-pm.png?w=300&amp;h=207 300w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-01-56-pm.png?w=600&amp;h=414 600w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-01-56-pm.png?w=150&amp;h=103 150w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p style="text-align:center;">[I will work up some ipython notebook examples soon]</p>
<p>The <a href="https://archive.ics.uci.edu/ml/datasets/Spambase" target="_blank">Spambase dataset </a>is a great example.  It has <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%3D57+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}=57 " title="N_{f}=57 " class="latex" /> features, and <img src="https://s0.wp.com/latex.php?latex=N_%7Bi%7D%3D4601+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{i}=4601 " title="N_{i}=4601 " class="latex" /> instances. We run a Ridge Regression, with <img src="https://s0.wp.com/latex.php?latex=N_%7Bi%7D%5Cin%280%2C4601%5D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{i}&#92;in(0,4601] " title="N_{i}&#92;in(0,4601] " class="latex" />, with <img src="https://s0.wp.com/latex.php?latex=%5Clambda%3D10%5E%7B-6%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda=10^{-6} " title="&#92;lambda=10^{-6} " class="latex" />.</p>
<p>As the data set size <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f} " title="N_{f} " class="latex" /> increases, the accuracy sharps to zero at <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%3DN_%7Bf%7D%3D57+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}=N_{f}=57 " title="N_{f}=N_{f}=57 " class="latex" />, and then increases sharply again, saturating at $latex N_{f}\sim 10^{3} $.</p>
<p>Many other datasets show this anomalous behavior at <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%3DN_%7Bf%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}=N_{f} " title="N_{f}=N_{f} " class="latex" />, as predicted by our analysis above.</p>
<h4>Ridge Regression vs Logistic Regression: Getting out of the Hole</h4>
<p>Where Ridge Regression fails, Logistic Regression bounces back.  Below we show some examples of RR vs LR in the <img src="https://s0.wp.com/latex.php?latex=N_%7Bf%7D%3DN_%7Bf%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="N_{f}=N_{f} " title="N_{f}=N_{f} " class="latex" /> danger zone</p>
<p>&nbsp;</p>
<p><a href="https://charlesmartin14.wordpress.com/?attachment_id=8374#main" rel=" rel=&quot;attachment wp-att-8374&quot;"><img data-attachment-id="8374" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/screen-shot-2015-12-17-at-4-40-13-pm/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-40-13-pm.png" data-orig-size="1898,1108" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2015-12-17 at 4.40.13 PM" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-40-13-pm.png?w=300&#038;h=175" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-40-13-pm.png?w=840" class="aligncenter size-medium wp-image-8374" src="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-40-13-pm.png?w=300&#038;h=175" alt="Screen Shot 2015-12-17 at 4.40.13 PM" width="300" height="175" srcset="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-40-13-pm.png?w=300&amp;h=175 300w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-40-13-pm.png?w=600&amp;h=350 600w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-17-at-4-40-13-pm.png?w=150&amp;h=88 150w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p>Logistic Regression still has problems, but it no where near as pathological as RR.</p>
<p>The reason for this is subtle.</p>
<ul>
<li>In Regression, the variance goes to infinity</li>
<li>In Classification, the norm <img src="https://s0.wp.com/latex.php?latex=%5CVert%5Cmathbf%7B%5Chat%7Bw%7D%7D%5CVert+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;Vert&#92;mathbf{&#92;hat{w}}&#92;Vert " title="&#92;Vert&#92;mathbf{&#92;hat{w}}&#92;Vert " class="latex" /> goes to infinity</li>
</ul>
<h4>Traditional ML Theory:  Isn&#8217;t the Variance Bounded?</h4>
<p>Traditional theory (very losely) bounds for quantities like the generalization error and (therefore) the variance.</p>
<p>We work out some of examples in the Appendix.</p>
<p>Clearly these bounds don&#8217;t work in all cases.  So what do they really tell us?  What&#8217;s the point?</p>
<p>These theories gives us some confidence that we can actually apply a learning algorithm&#8211;but only when the regularizer is not too small and the noise is not too large.</p>
<p>They essentially try to get us far away from the pathological phase transition that arises when the variance diverges.</p>
<h4>Cross Validation and Correlated Errors</h4>
<p><img class="aligncenter" src="https://i2.wp.com/cdn1.theodysseyonline.com/files/2015/09/13/635777203152249031-133095383_giphy.gif" alt="" width="151" height="142" /></p>
<p style="text-align:center;">In practice, we would never just cross our fingers set <img src="https://s0.wp.com/latex.php?latex=%5Clambda%3D0.00000001+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda=0.00000001 " title="&#92;lambda=0.00000001 " class="latex" />.</p>
<p style="text-align:left;">We pick a hold out set and run Generalized Cross Validation (GCV) . Yet this can fail when</p>
<ol>
<li style="text-align:left;"> the model errors <img src="https://s0.wp.com/latex.php?latex=%5Cxi+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;xi " title="&#92;xi " class="latex" /> are not i.i.d. but are strongly correlated</li>
<li style="text-align:left;"> we don&#8217;t know what accuracy metric to use?</li>
</ol>
<p style="text-align:left;">Hansen [5, 7] has discussed these issue in detail&#8230;and he provides an alternative method, the L-curve, which attempt to balance the size of the regularized solution and the residuals.</p>
<p style="text-align:left;"><a href="https://charlesmartin14.wordpress.com/?attachment_id=8647#main" rel=" rel=&quot;attachment wp-att-8647&quot;"><img data-attachment-id="8647" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/screen-shot-2015-12-27-at-11-16-54-pm/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-11-16-54-pm.png" data-orig-size="1904,1420" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2015-12-27 at 11.16.54 PM" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-11-16-54-pm.png?w=300&#038;h=224" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-11-16-54-pm.png?w=840" class="size-medium wp-image-8647 aligncenter" src="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-11-16-54-pm.png?w=300&#038;h=224" alt="Screen Shot 2015-12-27 at 11.16.54 PM" width="300" height="224" srcset="https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-11-16-54-pm.png?w=300&amp;h=224 300w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-11-16-54-pm.png?w=600&amp;h=448 600w, https://charlesmartin14.files.wordpress.com/2015/12/screen-shot-2015-12-27-at-11-16-54-pm.png?w=150&amp;h=112 150w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p style="text-align:left;">For example, we can sketch the relative errors <img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7B%5CVert+w-w%28%5Clambda%29%5CVert%7D%7B%5CVert+w%5CVert_%7B2%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;dfrac{&#92;Vert w-w(&#92;lambda)&#92;Vert}{&#92;Vert w&#92;Vert_{2}} " title="&#92;dfrac{&#92;Vert w-w(&#92;lambda)&#92;Vert}{&#92;Vert w&#92;Vert_{2}} " class="latex" /> for the L-curve and GVC method (see 7).  Above we see that the L-curve relative errors are more Gaussian than GCV.</p>
<p style="text-align:left;">In other words, while most packages provide <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics" target="_blank">a small choice of regression metrics</a>; as data scientists, the defaults like <img src="https://s0.wp.com/latex.php?latex=R%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="R^{2} " title="R^{2} " class="latex" /> may not be good enough.  Even using the 2-norm may not represent the errors well. As data scientists, we may need to develop our own metrics to get a good <img src="https://s0.wp.com/latex.php?latex=%5Clambda+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda " title="&#92;lambda " class="latex" /> for a regression.  (or maybe just use an SVM regression).</p>
<p style="text-align:left;">Hansen claims that,&#8221;experiments confirm that whenever GCV finds a good regularization parameter, the corresponding solution is located at the corner of the L-curve.&#8221; [7]</p>
<p style="text-align:left;">This means that the optimal regularized solution lives &#8216;just below&#8217; the phase transition, where the norm diverges.</p>
<h4>Phase Transitions in Regularized Regression</h4>
<p><a href="https://charlesmartin14.wordpress.com/?attachment_id=8224#main" rel=" rel=&quot;attachment wp-att-8224&quot;"><img data-attachment-id="8224" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/cliff/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/cliff.jpg" data-orig-size="473,355" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cliff" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/cliff.jpg?w=137&#038;h=103" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/cliff.jpg?w=473" class=" wp-image-8224 alignleft" src="https://charlesmartin14.files.wordpress.com/2015/12/cliff.jpg?w=137&#038;h=103" alt="cliff" width="137" height="103" srcset="https://charlesmartin14.files.wordpress.com/2015/12/cliff.jpg?w=137&amp;h=103 137w, https://charlesmartin14.files.wordpress.com/2015/12/cliff.jpg?w=274&amp;h=206 274w, https://charlesmartin14.files.wordpress.com/2015/12/cliff.jpg?w=150&amp;h=113 150w" sizes="(max-width: 137px) 100vw, 137px" /></a></p>
<p>What do we mean by a phase transition?</p>
<p>The simplest thing is to we plot a graph and look a steep cliff or sharp change.</p>
<p>Here, generalization accuracies drops off suddenly as <img src="https://s0.wp.com/latex.php?latex=%5Cdfrac%7BN_%7Bf%7D%7D%7BN_%7Bi%7D%7D%5Crightarrow+1%5C%3B%5C%3Band%5C%3B%5C%3B%5Calpha%5Crightarrow+0+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;dfrac{N_{f}}{N_{i}}&#92;rightarrow 1&#92;;&#92;;and&#92;;&#92;;&#92;alpha&#92;rightarrow 0 " title="&#92;dfrac{N_{f}}{N_{i}}&#92;rightarrow 1&#92;;&#92;;and&#92;;&#92;;&#92;alpha&#92;rightarrow 0 " class="latex" /></p>
<p>In a physical phase transition,the fluctuations (i.e. variances and norms) in the system approach infinity.</p>
<p style="text-align:center;">Imagine watching a pot boil</p>
<p style="text-align:center;"><img data-attachment-id="8663" data-permalink="https://calculatedcontent.com/2015/12/28/when-regularization-fails/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot/" data-orig-file="https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=217&#038;h=144" data-orig-size="1300,863" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="4418437-Glass-saucepan-on-the-gas-stove-close-up-Stock-Photo-water-boiling-pot" data-image-description="" data-medium-file="https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=217&#038;h=144?w=300" data-large-file="https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=217&#038;h=144?w=840" class="alignnone  wp-image-8663" src="https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=217&#038;h=144" alt="4418437-Glass-saucepan-on-the-gas-stove-close-up-Stock-Photo-water-boiling-pot.jpg" width="217" height="144" srcset="https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=217&amp;h=144 217w, https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=434&amp;h=288 434w, https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=150&amp;h=100 150w, https://charlesmartin14.files.wordpress.com/2015/12/4418437-glass-saucepan-on-the-gas-stove-close-up-stock-photo-water-boiling-pot.jpg?w=300&amp;h=199 300w" sizes="(max-width: 217px) 100vw, 217px" /></p>
<p>We see bubbles of all sizes, small and large.  The variation in bubble size is all over the map.  This is characteristic of a phase transition: i.e. water to gas.</p>
<p>When we cook, however, we frequently simmer our foods&#8211;we keep the water just below the boiling point.  This is as hot as the water can get before it changes to gas.</p>
<p>Likewise, it appears that in Ridge Regression, we frequently operate at a point just below the phase transition&#8211;where the solution norm explodes.  And this is quite interesting.  And, I suspect, may be important generally.</p>
<p>In chemical physics, we need special techniques to treat this regime, such as the Renormalization Group. Amazingly, <a href="https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/" target="_blank">Deep Learning / Restricted Boltzmann Machines  look very similar to the Variational Renormalization Group</a> method.</p>
<p>In our next post, we will examine this idea further, by looking at the phase diagram of the traditional Hopfield Neural Networ, and the idea of Replica Symmetry Breaking in the Statistical Mechanics of Generalization. Stay tuned !</p>
<h4>References</h4>
<p>1. Vapnik and Izmailov, <a href="http://www.jmlr.org/papers/volume16/vapnik15a/vapnik15a.pdf" target="_blank">V -Matrix Method of Solving Statistical Inference Problems,</a> JMLR 2015</p>
<p>2. Smale, <a href="http://ttic.uchicago.edu/~smale/papers/math_foundation_of_learning.pdf">On the Mathematical Foundations of Learning</a>, 2002</p>
<p>3. https://github.com/ryotat</p>
<p>4. <a href="http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/proj_learn1.pdf" rel="nofollow">http://ttic.uchicago.edu/~gregory/courses/LargeScaleLearning/lectures/proj_learn1.pdf</a></p>
<p>5.  <a href="https://www.sintef.no/globalassets/project/evitameeting/2005/lcurve.pdf" target="_blank">The L-curve and its use in the numerical treatment of inverse problems</a></p>
<p class="p1">6.  <a href="http://www.sce.carleton.ca/~boyle/publications/boyle2009-inv-presentation1.pdf" target="_blank"><span class="s1">Discrete Ill-Posed and </span><span class="s1">Rank­ Deficient Problems</span></a></p>
<p class="p1">7.  <a href="http://www.cs.umd.edu/~oleary/reprints/j37.pdf" target="_blank">The use of the L-curve in the regularization of discrete ill-posed problems</a>, 1993</p>
<h1 class="ArticleTitle">Appendix:  some math</h1>
<h4>Bias-Variance Decomposition</h4>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7By%7D%3D%5Cmathbf%7BX%7D%7Bw%7D%2B%5Cmathbf%7B%5Cxi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{y}=&#92;mathbf{X}{w}+&#92;mathbf{&#92;xi} " title="&#92;mathbf{y}=&#92;mathbf{X}{w}+&#92;mathbf{&#92;xi} " class="latex" />,</p>
<p style="text-align:center;">when <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Cmathbf%7B%5Cxi%7D%5D%3D0+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}[&#92;mathbf{&#92;xi}]=0 " title="&#92;mathbb{E}[&#92;mathbf{&#92;xi}]=0 " class="latex" />,</p>
<p style="text-align:center;">then <img src="https://s0.wp.com/latex.php?latex=Cov%28%5Cmathbf%7B%5Cxi%7D%29%3D%5Csigma%5E%7B2%7D%5Cmathbf%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Cov(&#92;mathbf{&#92;xi})=&#92;sigma^{2}&#92;mathbf{I} " title="Cov(&#92;mathbf{&#92;xi})=&#92;sigma^{2}&#92;mathbf{I} " class="latex" /></p>
<p>Let</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CSigma%7D%3D%5Cdfrac%7B1%7D%7BN_%7Bi%7D%7D%5Cmathbf%7BX%5E%7B%5Cdagger%7DX%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;Sigma}=&#92;dfrac{1}{N_{i}}&#92;mathbf{X^{&#92;dagger}X} " title="&#92;mathbf{&#92;Sigma}=&#92;dfrac{1}{N_{i}}&#92;mathbf{X^{&#92;dagger}X} " class="latex" /></p>
<p>be the data covariance matrix.  And , for convenience, let</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Clambda_%7Bn%7D%3D%5Cdfrac%7B%5Clambda%7D%7Bn%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;lambda_{n}=&#92;dfrac{&#92;lambda}{n} " title="&#92;lambda_{n}=&#92;dfrac{&#92;lambda}{n} " class="latex" />.</p>
<p>The Expected value of the optimal estimator, assuming Gaussian noise, is given using the Penrose Pseudo-Inverse</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cmathbf%7B%5Chat%7Bx%7D%7D%3D%28%5Cmathbf%7B%5CSigma%7D-%5Clambda_%7Bn%7D%5Cmathbf%7BI%7D%29%5E%7B-1%7D%5Cmathbf%7B%5CSigma%7D%5Cmathbf%7Bw%5E%7B%2A%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}&#92;mathbf{&#92;hat{x}}=(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1}&#92;mathbf{&#92;Sigma}&#92;mathbf{w^{*}} " title="&#92;mathbb{E}&#92;mathbf{&#92;hat{x}}=(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1}&#92;mathbf{&#92;Sigma}&#92;mathbf{w^{*}} " class="latex" /></p>
<p>The Covariance is</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=Cov%28%5Cmathbf%7B%5Chat%7Bx%7D%7D%29%3D%5Cdfrac%7B%5Csigma%5E%7B2%7D%7D%7Bn%7D%28%5Cmathbf%7B%5CSigma%7D-%5Clambda_%7Bn%7D%5Cmathbf%7BI%7D%29%5E%7B-1%7D%5Cmathbf%7B%5CSigma%7D%28%5Cmathbf%7B%5CSigma%7D-%5Clambda_%7Bn%7D%5Cmathbf%7BI%7D%29%5E%7B-1%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="Cov(&#92;mathbf{&#92;hat{x}})=&#92;dfrac{&#92;sigma^{2}}{n}(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1}&#92;mathbf{&#92;Sigma}(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1} " title="Cov(&#92;mathbf{&#92;hat{x}})=&#92;dfrac{&#92;sigma^{2}}{n}(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1}&#92;mathbf{&#92;Sigma}(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1} " class="latex" /></p>
<p>The regularized Covariance matrix arises so frequently that we will assign it a symbol</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda%7D%3D%28%5Cmathbf%7B%5CSigma%7D-%5Clambda_%7Bn%7D%5Cmathbf%7BI%7D%29%5E%7B-1%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{&#92;Sigma}_{&#92;lambda}=(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1} " title="&#92;mathbf{&#92;Sigma}_{&#92;lambda}=(&#92;mathbf{&#92;Sigma}-&#92;lambda_{n}&#92;mathbf{I})^{-1} " class="latex" /></p>
<p style="text-align:left;">We can now write down the mean and covariance</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cxi%7D%5CVert%5Chat%7Bw%7D-%5Cbar%7Bw%7D%5CVert%5E%7B2%7D%3DTr%28Cov%28%5Cmathbf%7B%5Chat%7Bw%7D%7D%29%29&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;hat{w}-&#92;bar{w}&#92;Vert^{2}=Tr(Cov(&#92;mathbf{&#92;hat{w}}))" title="&#92;mathbb{E}_{&#92;xi}&#92;Vert&#92;hat{w}-&#92;bar{w}&#92;Vert^{2}=Tr(Cov(&#92;mathbf{&#92;hat{w}}))" class="latex" /></p>
<p style="text-align:center;"><span style="color:#ff0000;"><strong>[more work needed here]</strong></span></p>
<h4 style="text-align:left;">Average and Worst Case Generalization Bounds</h4>
<p>Consider the Generalization Error for a test point <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bx%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{x} " title="&#92;mathbf{x} " class="latex" /></p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7B%5Cxi%7D%7D%5B%28%5Cmathbf%7Bx%5E%7B%5Cdagger%7D%7D%28%5Chat%7B%5Cmathbf%7Bw%7D%7D-%5Cmathbf%7Bw%5E%7B%2A%7D%7D%29%29%5E%7B2%7D%5D+%3D%5Clambda%5E%7B2%7D_%7Bn%7D%5B%5Cmathbf%7Bx%5E%7B%5Cdagger%7D%7D%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda_%7Bn%7D%7D%5E%7B-1%7D%5Cmathbf%7Bw%7D%5D%5E%7B2%7D%2B%5Cdfrac%7B%5Csigma%5E%7B2%7D_%7Bn%7D%7D%7Bn%7D%5Cmathbf%7Bx%5E%7B%5Cdagger%7D%7D%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda_%7Bn%7D%7D%5E%7B-1%7D%5Cmathbf%7B%5CSigma%7D%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda_%7Bn%7D%7D%5E%7B-1%7D%5Cmathbf%7Bw%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;mathbf{&#92;xi}}[(&#92;mathbf{x^{&#92;dagger}}(&#92;hat{&#92;mathbf{w}}-&#92;mathbf{w^{*}}))^{2}] =&#92;lambda^{2}_{n}[&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w}]^{2}+&#92;dfrac{&#92;sigma^{2}_{n}}{n}&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{&#92;Sigma}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w} " title="&#92;mathbb{E}_{&#92;mathbf{&#92;xi}}[(&#92;mathbf{x^{&#92;dagger}}(&#92;hat{&#92;mathbf{w}}-&#92;mathbf{w^{*}}))^{2}] =&#92;lambda^{2}_{n}[&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w}]^{2}+&#92;dfrac{&#92;sigma^{2}_{n}}{n}&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{&#92;Sigma}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w} " class="latex" /></p>
<p style="text-align:left;">We can now obtain some very simple bounds on the Generalization Accuracy (just  like a bona-a-fide ML researcher)</p>
<h5 style="text-align:left;">The worst case bounds</h5>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5B%5Cmathbf%7Bx%5E%7B%5Cdagger%7D%7D%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda_%7Bn%7D%7D%5E%7B-1%7D%5Cmathbf%7Bw%7D%5D%5E%7B2%7D%5Cle%5CVert%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda_%7Bn%7D%7D%5E%7B-1%7D%5Cmathbf%7Bx%7D%5CVert%5E%7B2%7D%5CVert%5Cmathbf%7Bw%5E%7B%2A%7D%7D%5CVert%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="[&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w}]^{2}&#92;le&#92;Vert&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{x}&#92;Vert^{2}&#92;Vert&#92;mathbf{w^{*}}&#92;Vert^{2} " title="[&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w}]^{2}&#92;le&#92;Vert&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{x}&#92;Vert^{2}&#92;Vert&#92;mathbf{w^{*}}&#92;Vert^{2} " class="latex" /></p>
<h5 style="text-align:left;">The average case bounds</h5>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bw%5E%7B%2A%7D%7D%7D%5B%5Cmathbf%7Bx%5E%7B%5Cdagger%7D%7D%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda_%7Bn%7D%7D%5E%7B-1%7D%5Cmathbf%7Bw%7D%5D%5E%7B2%7D%5Cle%5Calpha%5E%7B-1%7D%5CVert%5Cmathbf%7B%5CSigma%7D_%7B%5Clambda_%7Bn%7D%7D%5E%7B-1%7D%5Cmathbf%7Bx%7D%5CVert%5E%7B2%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;mathbf{w^{*}}}[&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w}]^{2}&#92;le&#92;alpha^{-1}&#92;Vert&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{x}&#92;Vert^{2} " title="&#92;mathbb{E}_{&#92;mathbf{w^{*}}}[&#92;mathbf{x^{&#92;dagger}}&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{w}]^{2}&#92;le&#92;alpha^{-1}&#92;Vert&#92;mathbf{&#92;Sigma}_{&#92;lambda_{n}}^{-1}&#92;mathbf{x}&#92;Vert^{2} " class="latex" />,</p>
<p style="text-align:left;">where we assume the &#8216;covariance*&#8217; is</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bw%5E%7B%2A%7D%7D%7D%5B%5Cmathbf%7Bw%5E%7B%2A%7Dw%5E%7B%2A%5Cdagger%7D%7D%5D%3D%5Calpha%5E%7B-1%7D%5Cmathbf%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbb{E}_{&#92;mathbf{w^{*}}}[&#92;mathbf{w^{*}w^{*&#92;dagger}}]=&#92;alpha^{-1}&#92;mathbf{I} " title="&#92;mathbb{E}_{&#92;mathbf{w^{*}}}[&#92;mathbf{w^{*}w^{*&#92;dagger}}]=&#92;alpha^{-1}&#92;mathbf{I} " class="latex" /></p>
<h4 style="text-align:left;">Discrete Picard Condition</h4>
<p>We can use the SVD approach to make stronger statements about our ability to solve the inversion problem</p>
<p style="text-align:center;"><span style="text-align:center;line-height:1.5;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BXw%7D%3D%5Cmathbf%7By%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{Xw}=&#92;mathbf{y} " title="&#92;mathbf{Xw}=&#92;mathbf{y} " class="latex" />.</span></p>
<p style="text-align:left;">I will sketch an idea called the &#8220;Discrete Picard Condition&#8221; and provide some intuition</p>
<p style="text-align:left;">Write <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X} " title="&#92;mathbf{X} " class="latex" /> as a sum over singular vectors</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%3D%5Cmathbf%7BU%5CSigma+V%5E%7B%5Cdagger%7D%7D%3D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BN%7D%5Cmathbf%7Bu_%7Bi%7D%7D%5Csigma_%7Bi%7D%5Cmathbf%7Bv_%7Bi%7D%5E%7B%5Cdagger%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}=&#92;mathbf{U&#92;Sigma V^{&#92;dagger}}=&#92;sum&#92;limits_{i=1}^{N}&#92;mathbf{u_{i}}&#92;sigma_{i}&#92;mathbf{v_{i}^{&#92;dagger}} " title="&#92;mathbf{X}=&#92;mathbf{U&#92;Sigma V^{&#92;dagger}}=&#92;sum&#92;limits_{i=1}^{N}&#92;mathbf{u_{i}}&#92;sigma_{i}&#92;mathbf{v_{i}^{&#92;dagger}} " class="latex" />.</p>
<p style="text-align:left;">Introduce an abstract PseudoInverse,  defined by</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%5E%7B-%5Cdagger%7D%5Cmathbf%7BX%7D%3D%5Cmathbf%7BI%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}^{-&#92;dagger}&#92;mathbf{X}=&#92;mathbf{I} " title="&#92;mathbf{X}^{-&#92;dagger}&#92;mathbf{X}=&#92;mathbf{I} " class="latex" /></p>
<p style="text-align:left;">Express the formal solution as</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%3D%5Cmathbf%7BX%7D%5E%7B-%5Cdagger%7D%5Cmathbf%7By%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{w}=&#92;mathbf{X}^{-&#92;dagger}&#92;mathbf{y} " title="&#92;mathbf{w}=&#92;mathbf{X}^{-&#92;dagger}&#92;mathbf{y} " class="latex" /></p>
<p style="text-align:left;">Imagine the SVD of the PseudoInverse is</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D%5E%7B-%5Cdagger%7D%3D%5Cmathbf%7BU%5E%7B%5Cdagger%7D%5CSigma%5E%7B-1%7DV%7D%3D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BN%7D%5Cmathbf%7Bu_%7Bi%7D%5E%7B%5Cdagger%7D%7D%5Csigma%5E%7B-1%7D_%7Bi%7D%5Cmathbf%7Bv_%7Bi%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{X}^{-&#92;dagger}=&#92;mathbf{U^{&#92;dagger}&#92;Sigma^{-1}V}=&#92;sum&#92;limits_{i=1}^{N}&#92;mathbf{u_{i}^{&#92;dagger}}&#92;sigma^{-1}_{i}&#92;mathbf{v_{i}} " title="&#92;mathbf{X}^{-&#92;dagger}=&#92;mathbf{U^{&#92;dagger}&#92;Sigma^{-1}V}=&#92;sum&#92;limits_{i=1}^{N}&#92;mathbf{u_{i}^{&#92;dagger}}&#92;sigma^{-1}_{i}&#92;mathbf{v_{i}} " class="latex" />.</p>
<p style="text-align:left;">and use it to obtain</p>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%3D%5Csum%5Climits_%7Bi%3D1%7D%5E%7BN%7D%5Cdfrac%7B%5Cmathbf%7Bu%5E%7B%5Cdagger%7D_%7Bi%7Dy%7D%7D%7B%5Csigma_%7Bi%7D%7D%5Cmathbf%7Bv_%7Bi%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{w}=&#92;sum&#92;limits_{i=1}^{N}&#92;dfrac{&#92;mathbf{u^{&#92;dagger}_{i}y}}{&#92;sigma_{i}}&#92;mathbf{v_{i}} " title="&#92;mathbf{w}=&#92;sum&#92;limits_{i=1}^{N}&#92;dfrac{&#92;mathbf{u^{&#92;dagger}_{i}y}}{&#92;sigma_{i}}&#92;mathbf{v_{i}} " class="latex" />.</p>
<p style="text-align:left;">For this expression to be meaningful, the SVD coefficients <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bu%5E%7B%5Cdagger%7D_%7Bi%7Db%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{u^{&#92;dagger}_{i}b} " title="&#92;mathbf{u^{&#92;dagger}_{i}b} " class="latex" /> must decay, on average, faster than the corresponding singular values <img src="https://s0.wp.com/latex.php?latex=%5Csigma%7Bi%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;sigma{i} " title="&#92;sigma{i} " class="latex" />.</p>
<p>Consequently, the only coefficients that carry any information are larger than the noise level.  The small coefficients have small singular values and are dominated by noise.    As the problem becomes more difficult to solve uniquely, the singular values decay more rapidly.</p>
<p>If we discard the <img src="https://s0.wp.com/latex.php?latex=k%2B1%2CN+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="k+1,N " title="k+1,N " class="latex" /> singular components,  we obtain a regularized, unsupervised solution called</p>
<h5>Truncated SVD</h5>
<p style="text-align:center;"><img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7Bw%7D%3D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bk%7D%5Cdfrac%7B%5Cmathbf%7Bu%5E%7B%5Cdagger%7D_%7Bi%7Dy%7D%7D%7B%5Csigma_%7Bi%7D%7D%5Cmathbf%7Bv_%7Bi%7D%7D+&#038;bg=dddddd&#038;fg=303030&#038;s=0" alt="&#92;mathbf{w}=&#92;sum&#92;limits_{i=1}^{k}&#92;dfrac{&#92;mathbf{u^{&#92;dagger}_{i}y}}{&#92;sigma_{i}}&#92;mathbf{v_{i}} " title="&#92;mathbf{w}=&#92;sum&#92;limits_{i=1}^{k}&#92;dfrac{&#92;mathbf{u^{&#92;dagger}_{i}y}}{&#92;sigma_{i}}&#92;mathbf{v_{i}} " class="latex" />.</p>
<p>which is similar in spirit to <a href="https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/">spectral clustering</a>.</p>
<p>&nbsp;</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/charlesmartin14.wordpress.com/8086/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/charlesmartin14.wordpress.com/8086/" /></a> <img alt="" border="0" src="https://pixel.wp.com/b.gif?host=calculatedcontent.com&#038;blog=32496692&#038;post=8086&#038;subd=charlesmartin14&#038;ref=&#038;feed=1" width="1" height="1" />
* WorkFlow 1
http://curtis.ml.cmu.edu/w/courses/index.php/Class_meeting_for_10-605_Workflows_For_Hadoop
*** Readings
Pig: none required. A nice on-line resource for PIG is the on-line version of the O'Reilly Book [[http://chimera.labs.oreilly.com/books/1234000001811/index.html][Programming Pig]].
*** Also discussed
Joachims, Thorsten, A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization. Proceedings of International Conference on Machine Learning (ICML), 1997.
Relevance Feedback in Information Retrieval, SMART Retrieval System Experiments in Automatic Document Processing, 1971, Prentice Hall Inc.
Schapire et al, Boosting and Rocchio applied to text filtering, SIGIR 98.
*** Things to Remember
    - The TFIDF representation for documents.
    - The Rocchio algorithm.
    - Why Rocchio is easy to parallelize.
    - Definition of a similarity join/soft join.
    - Why inverted indices make TFIDF representations useful for similarity joins
      - e.g., whether high-IDF words have shorter or longer indices, and more or less impact in a similarity measure
** Recap
  #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:44:17
  [[file:WorkFlow 1/screenshot_2017-07-12_16-44-17.png]]
  #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:44:27
  [[file:WorkFlow 1/screenshot_2017-07-12_16-44-27.png]]

** 如果 classifier 不能放进内存中怎么办？
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:44:36
   [[file:WorkFlow 1/screenshot_2017-07-12_16-44-36.png]]
   当你的模型，也就是你统计出的这个 hashtable 或是其他结构，无法 fit in memory 时该
   怎么办？
   you could do if you believe the test set is small you could use this
   pipeline to just load the part of the classifier you need so then you
   have to scan the classifier once and pull out the weights that are
   appropriate for that test set. And then classifier the test set.

   but this is _assume the test set is small_, how can we do it on large
   test set?

   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:44:50
   [[file:WorkFlow 1/screenshot_2017-07-12_16-44-50.png]]
   Map-Reduce 模型的难点是，如何把已知的算法转换成多个 Map-Reduce 步骤。
   So the hard part is basically using that as a primitive when you're coding.

   上页 ppt 中，Test data 是包含了:
   - 文章 id：id
   - 单词（文章 id, 单词在文章中出现位置）：w1,1  w2,1 ...

   上页 ppt 中，Test data 是包含了:
   - 事件：X=w1 and Y=sports
   - 计数：5245

   问题是如果这两个都太大了，没法直接放进内存中怎么办呢？
   So, what I'd like to do is I'd like to take all this information and _shuffle_
   it around appropriately. So I can classify in a streaming process.
   每遇到一篇文章，我就扫描 hashtable 中所有这篇文章需要的 events 并保存下来。但是这样做实在是
   _太慢了_.

*** 测试集和模型都太大了，改进步骤 1
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:45:00
    [[file:WorkFlow 1/screenshot_2017-07-12_16-45-00.png]]
    ---------------------------------------------------------------
    1. 重新整理 event counts, 不是以 event 作为 key, 而是以 word 作为 key
       来统计数目。
    2. 这样做可以得到（单词，所有含有这个单词的 event 列表）这样的键值对。
    ---------------------------------------------------------------
    具体怎么做呢？ 经典算法 stream and sort.
    1. stream: 对于每一个 event counts 列表中的键出现的单词都要 print "w C[Y=y]=n"
    2. sort: stream 完成之后就进行排序。这一步相当于 'reduce' 步骤
       this is just like the inverted index program that I showed last week
       we're just going to reduce instead of adding things up we're going to
       concatenate them in a list.

    一个 stream and sort 就是一个 map-reduce 步骤，
    我可以同时让很多个 stream and sort 步骤一起工作，
    如此就 _把没法放进内存的测试集和模型 mapreduce 化了_


*** 测试集和模型都太大了，改进步骤 2
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:45:14
    [[file:WorkFlow 1/screenshot_2017-07-12_16-45-14.png]]
    我们这里假设文章是可以 fit in memory 的，所以这篇文章需要的所有 (w:counts associated with w)
    表也可以 fit in memory.
    如果我们能把这个从步骤 1 中得到的(w:counts associated with w)表放进数据库中，那么一切都解决了
    但是数据库由于 seek 数据太慢，所以我们不使用数据库，那么下面该怎么办呢？

    所以我们接下来设计一个模式，他可以很好的适用于 stream and sort 算法和 map-reduce 模型。
    这个模式就是用来模拟这个表的

    I'm going to go through some article of Test data, and I'm going to
    issue requests for every event counter, after those requests get answered
    I'm going to do the classification step.

    So what is the [request and answer] process look like?
    well it's going to be another sort of [stream and sort] type operation.

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:45:47
    [[file:WorkFlow 1/screenshot_2017-07-12_16-45-47.png]]


    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:45:56
    [[file:WorkFlow 1/screenshot_2017-07-12_16-45-56.png]]
    [request and answer] 和 [stream and sort] 之间到底有什么关系呢？
    先回忆 stream and sort 的处理过程。

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:46:08
    [[file:WorkFlow 1/screenshot_2017-07-12_16-46-08.png]]
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:46:25
    [[file:WorkFlow 1/screenshot_2017-07-12_16-46-25.png]]
    These are all the counters you need for id1. So we just
    _print these strings_ (found crts to id1, aardvark ctrs to
    id1) out just like we _printed out our increment requests_
    for the downstream process.

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:46:51
    [[file:WorkFlow 1/screenshot_2017-07-12_16-46-51.png]]
    Compare to the previous ppt, I suggest we do it by ~ after
    every word.

    这个 ~ 标记是用来改变 unix sort 命令的执行目标仅仅考虑 ascii 码。
    或者可以用 ~% export LC_COLLATE=C~

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:47:04
    [[file:WorkFlow 1/screenshot_2017-07-12_16-47-04.png]]
    我这样做的目标就是把我的 request 与 record of all event counts for each
    word 结合起来 -- concanate. 得到一张新的表。

    这个表的是由 record of all event counts for each word 表中的 item 作为
    头，把 _所有文章_ 对该单词的 request 列在其下。这里的意思是把所有的 request
    都放在一起排序么？

    这个过程要做： _combine and sort_
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:47:16
    [[file:WorkFlow 1/screenshot_2017-07-12_16-47-16.png]]
    I'll see ~aardvark~ counter first, and I'll see all the requests
    involving ~aardvark~.

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:47:33
    [[file:WorkFlow 1/screenshot_2017-07-12_16-47-33.png]]
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:47:49
    [[file:WorkFlow 1/screenshot_2017-07-12_16-47-49.png]]
    经过这个步骤 [combine and sort] 之后得到的就是这里 output 的内容。

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:47:58
    [[file:WorkFlow 1/screenshot_2017-07-12_16-47-58.png]]

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:48:11
    [[file:WorkFlow 1/screenshot_2017-07-12_16-48-11.png]]

*** 这个程序的伪代码
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:48:19
    [[file:WorkFlow 1/screenshot_2017-07-12_16-48-19.png]]
    event counts 程序是作业 1 --- CountForNB
    then I sort them and stream through them, then convert them
    into this format(word, counts associated with word)

    以上這些是 step 1 --- _reformatting_ of the things that basically
    _one MapReduce process_

    ~cat -~
    means use the standard input as one of the files your concatenated.

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:48:31
    [[file:WorkFlow 1/screenshot_2017-07-12_16-48-31.png]]
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:48:45
    [[file:WorkFlow 1/screenshot_2017-07-12_16-48-45.png]]
    橙色表格是 ~java requestWrodCounts test.dat~ 的输出。
    蓝色表格是 ~words.dat~.
    绿色表格是 ~java answerWordCountRequests~ 的输入。
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:48:59
    [[file:WorkFlow 1/screenshot_2017-07-12_16-48-59.png]]
    紫色是 ~java answerWordCountRequest~ 的输出。
    红色是 ~test.dat~


    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:49:10
    [[file:WorkFlow 1/screenshot_2017-07-12_16-49-10.png]]
    绿色表格是 ~testNBUsingRequests~ 的输入。

    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:49:19
    [[file:WorkFlow 1/screenshot_2017-07-12_16-49-19.png]]
    OK, so we filled in the missing piece now we can do training
    and tests, we can do it you know in a hash table or we can do
    using basically zero memory, doing everything on disk only
    using sorts to organize our computations.

    which also means I can do it in hadoop with a low memory.

** Rocchio's Algorithm
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:50:13
   [[file:WorkFlow 1/screenshot_2017-07-12_16-50-13.png]]
   这里是之前讲解的 rocchio algo

   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:50:27
   [[file:WorkFlow 1/screenshot_2017-07-12_16-50-27.png]]
   这里想要引入一些新的符号，这样解释 rocchio algo 会更方便的将其应用在
   scale 的情况中。

   d - doc
   w - word

   v(d) is just the document vector for 'd' normalized.
   normalizing it by a Euclidean length.

   v(d) 点乘 v(y) 就表示这个 doc 与标签 y 的相关度。
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:50:58
   [[file:WorkFlow 1/screenshot_2017-07-12_16-50-57.png]]



   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:51:09
   [[file:WorkFlow 1/screenshot_2017-07-12_16-51-09.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:51:22
   [[file:WorkFlow 1/screenshot_2017-07-12_16-51-22.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:51:32
   [[file:WorkFlow 1/screenshot_2017-07-12_16-51-32.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:51:43
   [[file:WorkFlow 1/screenshot_2017-07-12_16-51-43.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:52:00
   [[file:WorkFlow 1/screenshot_2017-07-12_16-52-00.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:52:15
   [[file:WorkFlow 1/screenshot_2017-07-12_16-52-15.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:52:28
   [[file:WorkFlow 1/screenshot_2017-07-12_16-52-28.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:52:39
   [[file:WorkFlow 1/screenshot_2017-07-12_16-52-39.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:52:50
   [[file:WorkFlow 1/screenshot_2017-07-12_16-52-50.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:53:07
   [[file:WorkFlow 1/screenshot_2017-07-12_16-53-07.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:53:20
   [[file:WorkFlow 1/screenshot_2017-07-12_16-53-20.png]]
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:53:32
   [[file:WorkFlow 1/screenshot_2017-07-12_16-53-32.png]]

** Abstractions for stream and sort and Map-Reduce
   request and answer is a good _low-level_ opreation,
   and if you're writing a MapReduce pipeline.
   But when you're prototyping things it may not be the
   level you want to work at.

   So, let's introduce some sort of high-level things.

   So is there some special cases of this mapReduc process
   that we can _parameterize and reuse_.
   #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:54:19
   [[file:WorkFlow 1/screenshot_2017-07-12_16-54-19.png]]

*** map and filter
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:54:32
    [[file:WorkFlow 1/screenshot_2017-07-12_16-54-32.png]]
    - Transform it and output the result
      if we're _streaming through a table_ we can obviously take
      some operation on every _row_ of the table, so we could
      go through a table and _transform that row_ and output the
      result.
      use some _map_ operation

    - Decide if you keep it
      And we'll throw out stop words from my table.
      eg ("the", 1) -> deleted
      use some _filter_ operation

*** tokenizing and flatten
    #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:54:43
    [[file:WorkFlow 1/screenshot_2017-07-12_16-54-43.png]]
    什么是 tokenizing?
    I love you ---> ["I", "love", "you"]

    什么是 flatten?
    就是把通过 tokenizing 得到的所有 list, 都 _拼接_ 在一起。

    本页 ppt 右边的绿色方块就是 flatten 两句话的结果。

    _但是需要注意必须符合格式：每行一个单词_

*** 举例说明上面這些操作出现在朴素贝叶斯算法中

**** map sort and reduce
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:54:54
     [[file:WorkFlow 1/screenshot_2017-07-12_16-54-54.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:55:05
     [[file:WorkFlow 1/screenshot_2017-07-12_16-55-05.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:55:16
     [[file:WorkFlow 1/screenshot_2017-07-12_16-55-16.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:55:29
     [[file:WorkFlow 1/screenshot_2017-07-12_16-55-29.png]]
     (f(row), row) 很有意思就对应之前讲解的，reformatting the table
     (event, envent-num) -> (word, event)
     这里就是
     (f((event, envent-num)), event) = (word, event)

     所以总体上就三個步骤：
     1. map/streaming
     2. sort
     3. reduce and aggregate

**** join two tables
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:55:40
     [[file:WorkFlow 1/screenshot_2017-07-12_16-55-40.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:55:56
     [[file:WorkFlow 1/screenshot_2017-07-12_16-55-56.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:56:07
     [[file:WorkFlow 1/screenshot_2017-07-12_16-56-07.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:56:19
     [[file:WorkFlow 1/screenshot_2017-07-12_16-56-19.png]]
     (w, request) + (w, counters) = (w, counters, request)

     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:56:30
     [[file:WorkFlow 1/screenshot_2017-07-12_16-56-30.png]]

**** Abstract implementation: TF-IDF
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:56:41
     [[file:WorkFlow 1/screenshot_2017-07-12_16-56-41.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:56:55
     [[file:WorkFlow 1/screenshot_2017-07-12_16-56-55.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:57:06
     [[file:WorkFlow 1/screenshot_2017-07-12_16-57-06.png]]

**** Two ways to join
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:57:15
     [[file:WorkFlow 1/screenshot_2017-07-12_16-57-15.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:57:27
     [[file:WorkFlow 1/screenshot_2017-07-12_16-57-27.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:57:43
     [[file:WorkFlow 1/screenshot_2017-07-12_16-57-43.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:57:56
     [[file:WorkFlow 1/screenshot_2017-07-12_16-57-56.png]]
     #+DOWNLOADED: /tmp/screenshot.png @ 2017-07-12 16:58:06
     [[file:WorkFlow 1/screenshot_2017-07-12_16-58-06.png]]

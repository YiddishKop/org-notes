* lec-14: WordEmbedding
:Reference:
[[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::2][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf]]
:END:

这里是不能使用 auto-encoder 的

[[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::5][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:5]]

机器可以根据菜英文和马英九都具有类似的语义结构，来推断出马英九和菜英文
大概是类似的 ===> 通过 context 来推断两个单词的 similarity

[[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::6][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:6]]

有两种思想可以挖掘这种 context：
1. count based
   两个单词常常一起出现 co-occur ，那么就认为他们具有较高的相似性
   Glove Vector:
   http://nlp.stanford.edu/projects/glove/

   还是通过 inner-product 来计算 similarity：
   希望 内积与出现次数 越接近越好

2. prediction based(重点探讨)
   [[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::7][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:7]]

   given 前一个word，这里每一个w代表一个word。
   predict which is next word.

   input: 是某个单词的 one-hot encoding
   outpu: 是所有单词是 next word 的概率

   把 input layer 的下一层作为 word vector，为什么这么做呢？
   因为不同单词的 one-hot encoding 会产生不同的 next-layer neurons
   -> z向量， 如果你把z向量的每一维都看作一个坐标轴，就可以画出某个单词
   在z这个坐标系统中的向量坐标，所有单词都可以画在里面，就可以看出谁离
   的更近。

   [[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::7][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:8]]

   why prediction based method work

   他是如何 exploit context 的？
   菜英文 宣誓就职
    wi-1  wi
   马英九 宣誓就职
    wi-1  wi
   所以我们希望，当 input-layer 时，不管是菜英文or马英九最后 ouputlayer
   都是 ‘宣誓就职’ 的概率最高。那么在之前的 hidden layer 就肯定会有所
   体现: 每一层转换都是在拉近两个向量的距离，因此通过第一层就应该表现出这
   种趋势。

   但是这依然很难通过一个词汇就预测出下一个词汇，输入的信息不足以产生这种
   判断
   [[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::9][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:9]]

   基本的思想就是：1个单词不够，就2～n个单词，一般是需要10个单词。才能够
   learn 出比较合理的模型。

   如何一次输入10个单词呢？一般是进行·「向量叠加」，把10个向量按照层层叠
   起来就可以了。但是我们使用 one-hot encoding ，如果词典中单词量是1w
   那么10个单词的 one-hot encoding 叠加在一起的向量维度就是10w。

   我们希望在叠加起来的每一个 word 的 One-hot encoding 之间共享weight

   为什么要这么做呢？
   同一个word，因为在10个word序列的位置不同，如果不 share weight
   那么最后同一个word就会得到不同的映射z。

   [[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::10][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:10]]

   基本的公式推导：
   1. word i-1 -> xi-1; word i-2 -> xi-2; combine to project to z
   2. z = W1xi-2 + W2xi-1
   3. W1 W2 both |Z|×|V| matrix
   4. make W1 = W2 = W  --> z = W(xi-2 + xi-1)

   这种 share weight 的做法在 CNN 中也出现过，该怎么做呢？
   How to make wi equal to wj?
   1. give the same initialization
   2. update wi : wi = wi - ita*(partial(wi)+partial(wj)
   3. update wj : wj = wj - ita*(partial(wi)+partial(wj)
   这样 初始值一样 更新步进一样 所以两者会一直保持相等。


   如何训练这个 NN 呢？
   完全 unsupervised, 写个脚本上网去爬就好了，unsupervised最好的地方
   就在于不需要给出 label，这省去了很多的麻烦。

   那我最后的结果跟谁比较呢，跟下一个单词比较
   爬下的文本：潮水退了就知道谁没穿裤子
   我们选择 2-gram（就是用前两个单词预测下一个单词）

   input 2-gram: enconding(潮水 + 退了)
   expected output: encoding(就)
   learning output: xx

   minimizing cross-entropy(learning - expected)

   input 2-gram: enconding(退了+就)
   expected output: encoding(知道)
   learning output: xx

   input 2-gram: enconding(就+知道)
   expected output: encoding(谁)
   learning output: xx

   [[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::13][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:13]]

   刚才讲的都是 prediction-based 的 base model, 还有很多 various architectures

   1. 变形1：用 wi-1 + wi+1 --predict--> wi
   2. 变形2：用 wi --predict--> wi-1 + wi+1

   word embedding 有很多好的特性
   [[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::14][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:14]]

   1. 同一个动词的三个时态，国家跟首都，把他们放在一起，是有某种固定的形态的。
   2. 某种「·属种」关系映射到的地方彼此靠近。
   3. 可以做某些推理题目。（利用向量相近的方法）。
   4. 可以做类似翻译的效果。
   5. 图片分类，word embedding 可以很好的处理新增的他没有看过的类型的图片
      比如原来的种类只有 auto, horse, dog, 传统的图片分类很难处理新来的猫
      但是 word embedding 可以做的非常好, 一个新的类型的图片进来就正常做
      project，他确实就会出现在猫的周围


   刚才讲的都是 word embedding,那怎么做 document embedding?
   [[docview:/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec%20(v3).pdf::20][/media/yiddi/9C6C7BA46C7B783A/YIDDISHKOP/ML/TaiDa-ML_LiHongYi/course-lec-note/lec14-word2vec (v3).pdf:20]]

   1. document -> Bag of word -> NN(auto-encoder) Sementic Ebedding
   2. 只有 bag of word 是完全不够的，因为没考虑单词的顺序对于语义的影响
